#+title: Numerical methods
#+subtitle: on the Cahn-Hilliard Equation
#+BIBLIOGRAPHY: ~/org/resources/bibliography/refs.bib
#+options: toc:nil
#+BIND: org-latex-title-command ""
#+BIND: org-latex-default-figure-position "H"
#+latex_class: mimosis
# #+latex_header: \include{~/.doom.d/OrgConfig/noteHeader.tex}
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
#+PROPERTY: header-args:julia :output-dir images :eval never :noweb no-export
#+PROPERTY: header-args:julia-vterm :output-dir images :exports results :noweb no-export :eval yes :session jl :cache yes
# #+INFOJS_OPT: view:info toc:nil
#+latex_header:\renewcommand{\floatpagefraction}{.9}%
#+latex_header:\usepackage[level]{datetime}


* Title-page :ignore:
#+begin_export latex
\makeatletter
\begin{titlepage}
    \centering
\includegraphics[width=1\textwidth]{logo/logo.png}
\par
	\vspace{1.5cm}
	{\scshape\huge Bachelor's Thesis \par}
	\vspace{1.5cm}
	{\Huge\bfseries  \@title \par}
	\vspace{2cm}
	{\LARGE \@author \par}
	{\Large Matriculation Number: 3545737 \par}
	\vspace{1.5cm}
	{\large Examiner: Prof Rohde I believe \par}
	{\large Advisor: Hasel \par}
	\vspace{1.5cm}
	{\large Institute of Applied Analysis and Numerical Simulation\par}



	\vfill

% Bottom of the page
	{\large Completed 01.01.2022 \par}
\end{titlepage}
\makeatother

#+end_export



#+begin_abstract
This Thesis gives a short overview and derivation for the Cahn-Hilliard Equation. It uses a discretization by the authors [cite:@SHIN20117441] as baseline, and expands upon this dicretisation with an elliptical relaxation approach. It introduces evaluation metrics regarding stability in time, space and during sub-iteration. And compares the elliptical approach against the baseline. Furthermore, it shows a qualitative success of the elliptical solver, however it also highlights challenges in numerical stability.
#+end_abstract
#+TOC: headlines 3


* Introduction
The Cahn-Hilliard equation is a well known fourth order PDE used in multiphase flow. It is used to couple different phases with a diffuse interface, as compared to a sharp interface, approach. Therefore, it has a smooth transition between phases.
The CH equation serves the same purpose, as the second order Allen-Cahn equation. However, the Allen-Cahn equation is not mass conservative. Hence, the Cahn-Hilliard equation is used if mass conservation  is required.
In this thesis we implement numerical solvers for the Cahn-Hilliard equation in the Julia programming language.
We begin by giving an overview and a derivation for the analytical CH equation in chapter [[The Cahn-Hilliard equation]]. We then show mass conservation and a decrease in total energy for it.
The chapter [[Baseline multi-grid solver]] introduces our discretization and a fitting numerical method. We explain the necessary functions, describe the relevant steps of our numerical implementation, and give their implementation. Additionally, in chapter [[Baseline multi-grid solver]] we introduce the initial conditions we used in this thesis.
After introducing a numerical method, in chapter [[Numerical experiments]], we evaluate this method's stability,discrete mass conservation and discrete energy decrease that we have shown continuously for the analytical CH equation.
Our thesis introduces a analytical relaxation approach to the classical CH equation, where instead of solving a fourth order PDE [fn:1], we solver a second order relaxed PDE and an additional elliptical PDE. In the chapter [[Relaxed problem]] we introduce this approach, and then derive a numerical solver using the method described in chapter [[Baseline multi-grid solver]]. Hereupon we derive and implement the necessary functions for the discretized relaxed equation, and  we introduce a simple solver for the elliptical PDE.
Subsequently, in chapter [[Relaxed experiments]], we evaluate our relaxed method against the baseline with the same measures, as introduced in chapter [[Numerical experiments]].

We began writing this thesis with a reproducible research philosophy in mind. Hence, we provide the explanation you  are reading, and the implementation in the same file. The original aim was to have the mathematical formulas and their implementation interleaved in a way, that leaves no room for interpretation. While we fall short of this goal, we still provide all relevant code in the relevant sections and the appendix. All shown code is therefore the code that is run on our machine. Since not all parts of the code are relevant for understanding, unimportant sections are implemented elsewhere. Didacticly they aer replaced with a comment of form =<<unumportant-code-section>>=. Their implementation can be found in ~Thesis_jl.org~ in a code block of the same name.
We did experiment with additional tools such as [[https:orgmode.org][org-mode]] that allow for scientific note-taking and literate programming.
This file is available on our github repository at [[https://github.com/ProceduralTree/CahnHilliardJulia.git]]
as ~Thesis_jl.org~.
* The Cahn-Hilliard equation
The Cahn-Hilliard(CH) equation is a partial differential equation (PDE) that governs the dynamics of a two-phase fluid[cite:@Wu_2022]. The form of the CH equation used in this thesis in the domain \( \Omega \times (0, T) \,, \Omega \subset \mathbb{R}^d \,, d \in \mathbb{N}  \,, T>0 \).
#+name: eq:CH
\begin{equation}
\begin{aligned}
\partial_{t}\phi(x,t) &=  \nabla \cdot(M(\phi)\nabla\mu), \\
\mu &= - \varepsilon^2 \Delta\phi  + W'(\phi),
\end{aligned}
\end{equation}
where the variables \( \phi , \mu : \Omega \times (0,T) \to \mathbb{R}^d \) are phase-field variable and chemical potential,
\(\varepsilon\) is a positive constant correlated with interface thickness, \( W(\phi) \) is a double well potential and \(M(\phi) > 0\) is a mobility coefficient [cite:@Wu_2022].
 \( \phi\) is defined in an interval \(I=[-1,1] \) and  represent the different phases.
\begin{align*}
\phi &=
\begin{cases}
1 &\,, \phi \in \text{phase 1} \\
-1 &\,, \phi \in\text{phase 2}
\end{cases}
\end{align*}

 In this thesis we assume \(M(\phi) \equiv 1 \), simplifying the CH equation.

The advantages of the CH approach, as compared to traditional boundary coupling, are for example: "explicit tracking of the interface" [cite:@Wu_2022], as well as "evolution of complex geometries and topological changes [...] in a natural way" [cite:@Wu_2022].
In practice, it enables linear interpolation between different formulas on different phases.
** Physical derivation of the CH equation [[eqref:eq:CH]]
*** The free energy
The authors in [cite:@Wu_2022] define the CH equation using the *Ginzburg-Landau* free energy equation:
#+name: eq:energy
\begin{align}
E^{\text{bulk}}[\phi] &= \int_{\Omega} \frac{\varepsilon^2}{2} |\nabla \phi |^2 + W(\phi) \, dx ,
\end{align}
where \(W(\phi) \) denotes the Helmholtz free energy density of mixing [cite:@Wu_2022] that we approximate it in further calculations with \(W(\phi) = \frac{(1-\phi ^2)^2}{4}\) as in [cite:@SHIN20117441] shown in Fig. [[fig:double-well]].
#+name: fig:double-well
#+begin_src julia-vterm :results file graphics :file double-well.svg
using Plots
using LaTeXStrings
W(x) = 1/4 * (1- x^ 2)^2
p = plot(W , xlims=(-2,2) , label=:none)
savefig(p, "images/double-well.svg")
#+end_src

#+caption: Double well potential \( W(\phi) \)
#+RESULTS[990bafb41c1855db23a8eb8b6bc4129e91d73342]: fig:double-well
[[file:images/double-well.svg]]




The chemical potential, \( \mu \), then follows as the variational derivation of the free energy [[eqref:eq:energy]].
#+name: eq:chemical-potential
\begin{align}
 \mu &= \frac{\delta E_{bulk}(\phi)}{\delta \phi} = -\varepsilon^2 \Delta \phi + W'(\phi)
\end{align}

*** Derivation of the CH equation from mass balance
The paper [cite:@Wu_2022] states that the observable phase separation is driven by a diffusion resulting from the gradient in chemical potential \( \mu \). The emergent conservative dynamics motivate the following diffusion equation
#+name: eq:massbal
\begin{equation}
    \partial_t \phi + \nabla \cdot \mathbf{J} = 0,
\end{equation}
where \( \mathbf{J} = -\nabla \mu \) represents mass-flux.
We follow the authors [cite:@Wu_2022] in deriving the CH equation by combining Eq.eqref:eq:chemical-potential and Eq.[[eqref:eq:massbal]].
\begin{equation}
\begin{aligned}
\implies \partial_t \phi   &=- \nabla \cdot \mathbf{J} = \Delta\mu , \\
\mu &=  -\varepsilon^2 \Delta \phi + W'(\phi) \,,
\end{aligned}
\end{equation}
Furthermore the CH equation is mass conservative under homogeneous Neumann boundary conditions, defined as:
#+name: eq:boundary-conditions
\begin{equation}
\begin{aligned}
\mathbf{J} \cdot n &= 0 & \text{on} \, \partial\Omega &\times (0,T),\\
\partial_n\phi &= 0 & \text{on} \, \partial\Omega &\times (0,T),
\end{aligned}
\end{equation}
where \( n  \) is the outward normal on \( \partial \Omega \).
To show the conservation of mass we analyze the change in total mass in the domain \( \Omega \) over time.
#+name: eq:mass-conservation
\begin{equation}
\begin{aligned}
\frac{d}{dt}\int_{\Omega}\phi \ d \mathbf{x} &=\int_{\Omega}\frac{\partial \phi}{\partial t} \ d\mathbf{x} \\
&= - \int_{\Omega} \nabla \cdot \mathbf{J} \ d\mathbf{x}\\
&=  \int_{\partial\Omega}  \mathbf{J} \cdot n  \ d\mathbf{s} \\
&= 0 & \forall t\in(0,T)\,,
\end{aligned}
\end{equation}

In order to show thermodynamic consistency of the CH equation, we take the time derivation of the free energy functional Eq.[[eqref:eq:energy]].
\begin{align*}
\frac{d}{dt}E^{bulk}[\phi(t)] &= \int_{\Omega} ( \varepsilon^2 \nabla \phi \cdot \nabla \partial_t \phi + W'(\phi) \partial_t \phi) \ d x \\
&=\int_{\Omega} (\varepsilon^2\nabla\phi + W'(\phi))\partial_t\phi \ dx\\
&=\int_{\Omega} \mu \partial_t \phi \ dx\\
&= \int_{\Omega} \mu \cdot \Delta\mu \ dx \\
&= -\int_{\Omega} \nabla\mu \cdot \nabla\mu \ dx + \int_{\partial\Omega} \mu \nabla\phi_t \cdot n \ dS \\
&\stackrel{\partial_n\phi = 0}{=} - \int_{ \Omega } |\nabla \mu|^2 \ d x, & \forall t \in (0,T)
\end{align*}

* Baseline multi-grid solver
** The discretization of the CH equation:
As baseline for numerical experiments we use a two-grid method based on the finite difference method defined in [cite:@SHIN20117441].
Our discretization follows the one taken by the authors in [cite:@SHIN20117441].
We discretize our domain \( \Omega \) to be a Cartesian-grid \( \Omega_d \) on a square with side-length \( N\cdot h \), where N is the number of grid-points in one direction, and \( h \) is the distance between grid-points. In all our initial data \( h \) is \( 3\cdot10^{-3}\) and \( N=64 \). However, for stability tests we change \( h \) and \( N \).
\begin{equation}
\Omega_d = \left\{ i,j \mid i,j \in \mathbb{N} \,, i,j \in [2,N+1] \right\}
\end{equation}
where \( \Omega_{d} \) is the discrete version or our domain as shown in [[fig:discrete-domain]].
#+name: fig:discrete-domain
#+begin_src julia-vterm :results file graphics :file domain.svg
using Plots
using LaTeXStrings
pgfplotsx()
Idx = CartesianIndex(1,1)
M = zeros(66,66)
M[2:end-1 , 2:end-1] = ones(64,64)
p= heatmap(M, title=L"\Omega_d" , clim=(-1,1),
            gridlinewidth=2 , axis_equal_image=true , extra_kwargs=:subplot , xlims=(1 ,66) , ylims=(1,66))

savefig(p,"images/domain.svg")
#+end_src

#+caption: Discrete Domain used for most of the experiments in this Thesis
#+RESULTS[46038739234db0a64b145e68000e9b1ea9d30425]: fig:discrete-domain
[[file:images/domain.svg]]


We discretize the phase-field ,\( \phi \), and chemical potential ,\( \mu \), into grid-wise functions \(\phi_{ij}, \mu_{ij} \)
\begin{equation}
\begin{aligned}
\phi_{ij}^n: \Omega_d \times \left\{ 0, \dots  \right\} &\to \mathbb{R}\\
\mu_{ij}^n: \Omega_d \times \left\{ 0, \dots \right\} &\to \mathbb{R}
\end{aligned}
\end{equation}
Here \( n \) denotes the nth time-step, and \( (i,j) \) are Cartesian indices on the discrete domain \( \Omega_d \).
The authors in [cite:@SHIN20117441] then use the characteristic function \( G \) of the  domain \( \Omega \) to enforce no-flux boundary conditions [[eqref:eq:boundary-conditions]].

\begin{align*}
G(x,y) &=
\begin{cases}
1, & (x,y) \in  \Omega \\
0, & (x,y) \not\in  \Omega
\end{cases}
\end{align*}
We implement the discrete version of \( G \) on \( \Omega_d \) as follows:
\begin{align*}
G_{ij} &=
\begin{cases}
1, & i,j \in [2,N+1]  \\
0, & \text{else}
\end{cases}
\end{align*}
The definition of \( G_{ij} \) with \( i,j \in [2,N+1] \) enables us to evaluate \( G_{ij} \) of-grid.
#+begin_src julia :tangle src/utils.jl :eval never :exports none
"""
Boundry indicator function

Returns
---------------
1 if index i,j is in bounds(without padding) and 0 else
"""
#+end_src
#+begin_src julia :tangle src/utils.jl :eval never
function G(i, j, len, width)
    if 2 <= i <= len + 1 && 2 <= j <= width + 1
        return 1.0
    else
        return 0.0
    end
end
#+end_src

We then define the discrete derivatives \( D_x\phi_{ij}, \ D_y\phi_{ij} \) using centered differences:
\begin{align}
D_x\phi^{n+1,m}_{i+\frac{1}{2} j} &= \frac{\phi^{n+1,m}_{i+1j} - \phi^{n+1,m}_{ij}}{h} & D_y\phi^{n+1,m}_{ij+\frac{1}{2}} &= \frac{\phi^{n+1,m}_{ij+1} - \phi^{n+1,m}_{ij}}{h}
\end{align}
We define \( D_x\mu_{ij}^{n+\frac{1}{2},m} , D_y\mu_{ij}^{n+\frac{1}{2},m} \) in the same way.
Next we define the discrete gradient \( \nabla_d \phi^{n+1,m}_{ij}\), as well as a modified Laplacian \( \nabla_d \cdot (G_{ij} \nabla_d \phi^{n+1,m}_{ij} )\):



#+name: eq:discretization
\begin{equation}
\begin{aligned}
\nabla_d \phi^{n+1,m}_{ij} &= \left(D_x \phi^{n+1,m}_{i+1j} , \ D_y \phi^{n+1,m}_{ij+1}\right) \,,\\
 \nabla_d \cdot (G_{ij} \nabla_d \phi^{n+1,m}_{ij}) &= \frac{G_{i+\frac{1}{2}j}D_x \phi^{n+1,m}_{i+\frac{1}{2}j} -  G_{i-\frac{1}{2}}D_x \phi^{n+1,m}_{i-\frac{1}{2}j} + D_y \phi^{n+1,m}_{ij+\frac{1}{2}} - D_y \phi^{n+1,m}_{ij-\frac{1}{2}}}{h} \\
  &= \frac{ G_{i+\frac{1}{2}j} \phi^{n + 1,m}_{i+1j} +  G_{i-\frac{1}{2}j} \phi^{n +,m}_{i-1j} + G_{ij+\frac{1}{2}}  \phi^{n +,m}_{ij+1} + G_{ij-\frac{1}{2}} \phi^{n +,m}_{ij-1}    }{h^2}\\
& \, - \frac{\left(   G_{i+\frac{1}{2}j} + G_{i-\frac{1}{2}j} + G_{ij+\frac{1}{2}} + G_{ij-\frac{1}{2}}  \cdot \phi_{ij} \right)}{h^2} \,,
\end{aligned}
\end{equation}
The discretization for \(  \nabla_d\mu_{ij}^{n+\frac{1}{2},m} ,  \nabla_d \cdot (G_{ij} \nabla_d \mu^{n+\frac{1}{2},m}_{ij}) \) are done the same as for \( \phi_{ij}^{n+1} \)
 We define \(   \nabla_d \cdot (G_{ij} \nabla_d \phi_{ij} )\) instead of a discrete Laplacian \( \Delta_d \) to ensure a discrete version of boundary conditions [[eqref:eq:boundary-conditions]].
 The authors in [cite:@SHIN20117441] show this to be the case by expanding \( \nabla_d \cdot (G_{ij} \nabla_d\phi_{ij}) \).
Notably, when one point lies outside the domain, e.g. \( G_{i + \frac{1}{2}} = 0 \)  then the corresponding discrete gradient \( \frac{\phi_{i+1}^{n+1} - \phi_i}{h}  \) is weighted by 0. This corresponds the discrete version of \( \partial_n\phi = 0 \).
The authors in [cite:@SHIN20117441]

To simplify the notation for discretized derivatives we use the following abbreviations:
- \(  \Sigma_G \phi_{ij} = G_{i+\frac{1}{2}j} \phi^{n + 1,m}_{i+1j} +  G_{i-\frac{1}{2}j} \phi^{n +1,m}_{i-1j} + G_{ij+\frac{1}{2}}  \phi^{n +1,m}_{ij+1} + G_{ij-\frac{1}{2}} \phi^{n +1,m}_{ij-1}  \)
- \(  \Sigma_{Gij} = G_{i+\frac{1}{2}j} + G_{i-\frac{1}{2}j} + G_{ij+\frac{1}{2}} + G_{ij-\frac{1}{2}}  \)
Code:
#+begin_src julia :tangle src/utils.jl :eval never
function neighbours_in_domain(i, j, G, len, width)
    (
        G(i + 0.5, j, len, width)
        + G(i - 0.5, j, len, width)
        + G(i, j + 0.5, len, width)
        + G(i, j - 0.5, len, width)
    )

end
function discrete_G_weigted_neigbour_sum(i, j, arr, G, len, width)
    (
        G(i + 0.5, j, len, width) * arr[i+1, j]
        + G(i - 0.5, j, len, width) * arr[i-1, j]
        + G(i, j + 0.5, len, width) * arr[i, j+1]
        + G(i, j - 0.5, len, width) * arr[i, j-1]
    )
end
#+end_src

We can then write the modified Laplacian \( \nabla_d (G \nabla_d\phi_{ij}^{n+1}) \) as:
\begin{align*}
\nabla_{d} \cdot(G \nabla_d\phi_{ij}^{n+1}) &= \frac{\Sigma_G\phi_{ij}^{n+1} - \Sigma_{Gij}\cdot \phi_{ij}^{n+1}}{h^2}
\end{align*}
We use this modified Laplacian to deal with boundary conditions. Our abbreviations simplify separating implicit and explicit terms in the discretization.
** Initial data
For testing we use initial phase-fields defined by the following equations:

\begin{equation}
\begin{aligned}
\phi_{ij} &=
\begin{cases}
1 &\,, \|(i,j) - (\frac{N}{2} , \frac{N}{2})\|_p < \frac{N}{3}\\
-1 &\,,else
\end{cases}
&
\text{where    }  p \in \{2,\infty\}
\\
\phi_{ij} &=
\begin{cases}
1 &\,,  i < \frac{N}{2} \\
-1 &\,,else
\end{cases}
\\
\phi_{ij} &=
\begin{cases}
1 &\,, \|(i,j) - (\frac{N}{2} , 2)\|_2 < \frac{N}{3} \\
-1 &\,,else
\end{cases}
\\
\phi_{ij} &=
\begin{cases}
1 &\,, \| (i,j) - q_k \|_p < \frac{N}{5}  \\
-1 &\,,else
\end{cases}
& p \in \{1,2, \infty\} , q_k \in Q
\end{aligned}
\end{equation}
where \( q_k \) are random points inside my domain. Those we generate those using the following RNG setup in Julia


#+name: fig:testinput
#+begin_src julia-vterm :results file graphics  :file testdata.svg
<<init>>
<<setup-diverse-testgrids>>
plots =[  heatmap(t[1].phase ,  legend=:none , aspectratio=:equal , grid=false , showaxis=false , size=(600,600))
for t in tests[1:2:end]]
#plots = [heatmap(t[1].phase , size=(600,600), axis=:none , aspect_ratio=:equal) for t in tests]
p = plot(plots... , layout=(1,4) , size=(2400,600))
savefig(p,"images/testdata.svg")
#+end_src

#+caption: Examples of different phase-fields used as the initial condition in this work.
#+RESULTS[022c521f7b12b7f61d32b1d70b05629b2f49e747]: fig:testinput
[[file:images/testdata.svg]]

** Numerical ansatz
The authors in [cite:@SHIN20117441] then define the discrete CH equation adapted for the domain as:
#+name: eq:discrete-cahn-hilliard
\begin{equation}
\begin{aligned}
\frac{\phi_{ij}^{n+1} - \phi_{ij}^n}{\Delta t}  &=  \nabla _d \cdot (G_{ij} \nabla_d \mu_{ij}^{n+\frac{1}{2}} )  \,, \\
 \mu_{ij}^{n+\frac{1}{2}} &= 2\phi_{ij}^{n+1} - \varepsilon^2  \nabla_d \cdot  (G_{ij} \nabla _d \phi_{ij}^{n+1} ) + W'(\phi_{ij}^n) - 2\phi _{ij}^n \,,
\end{aligned}
\end{equation}
and derive a numerical scheme from this implicit equation.
** The discrete system
The authors in [cite:@SHIN20117441] derive their method by separating [[eqref:eq:discrete-cahn-hilliard]] into implicit and linear terms, and explicit non-linear terms. We write the implicit terms in form of a function \( L: \RR^2 \to \RR^2  \) and the explicit terms in \( (\zeta^n_{ij} , \psi^n_{ij})^T \).
\begin{align*}
L
\begin{pmatrix}
\phi^{n+1}_{ij} \\
\mu^{n+\frac{1}{2}}_{ij}
\end{pmatrix}
&=
\begin{pmatrix}
\frac{\phi^{n+1}_{ij}}{\Delta t} - \nabla _d \cdot  ( G_{ij} \nabla _d \mu^{n+\frac{1}{2}}_{ij} ) \\
\varepsilon^2 \nabla _d \cdot  (G \nabla_d \phi_{ij}^{n+1}) - 2\phi_{ij}^{n+1} + \mu_{ij}^{n+\frac{1}{2}}
\end{pmatrix}
\end{align*}
This operator follows from [[eqref:eq:discrete-cahn-hilliard]] by separating implicit and explicit terms \( L \) and   \( (\zeta^n_{ij} , \psi^n_{ij})^T \), respectively.
\begin{align*}
\begin{pmatrix}
\zeta^n_{ij}
 \\
\psi^n_{ij}
\end{pmatrix}
&=
\begin{pmatrix}
\frac{\phi_{ij}^{n}}{\Delta t}\\
W'(\phi_{ij}^n) - 2\phi_{ij}^n
\end{pmatrix}
\end{align*}
Due to being explicit, we know everything needed to calculate \( (\zeta^n_{ij} , \psi^n_{ij})^T \) at the beginning of each time step. We compute those values once and store them in the solver.

Furthermore, as it is needed later on, we derive its Jacobian with respect to the current grid point \( (\phi^{n+1}_{ij} , \mu^{n+\frac{1}{2}}_{ij})^{T} \):

\begin{align*}
DL\begin{pmatrix}
\phi_{ij} \\
\mu_{ij}
\end{pmatrix} &= \begin{pmatrix}
\frac{1}{\Delta t} & \frac{1}{h^2}\Sigma_{Gij}  \\
-\frac{\varepsilon^2}{h^2}\Sigma_{Gij} - 2 & 1
\end{pmatrix}
\end{align*}
Implementation details can be found in the Appendix under  [[*baseline][baseline]].
** SMOOTH operator
The authors [cite:@SHIN20117441]derived Gauss-Seidel Smoothing from:
#+name: eq:smooth
\begin{align}
L
\begin{pmatrix}
\phi^{n+1}_{ij} \\
\mu^{n+\frac{1}{2}}_{ij}
\end{pmatrix}
&=
\begin{pmatrix}
\zeta^n_{ij} \\
\psi^n_{ij}
\end{pmatrix}
.
\end{align}
 SMOOTH consists of point-wise Gauss-Seidel relaxation, by solving Eq.[[eqref:eq:smooth]] for all \( i,j \) with the initial guess for \( \zeta^n_{ij} , \psi^n_{ij} \). Since \( L \) is linear we can write Eq.[[eqref:eq:smooth]] as
 #+name: eq:explicit-smooth
 \begin{equation}
\begin{aligned}
\begin{pmatrix}
  \zeta_{ij}^n\\
\psi_{ij}^n
\end{pmatrix}
&=
DL\begin{pmatrix}
\phi_{ij}^{n+1} \\
\mu_{ij}^{n+\frac{1}{2}}
\end{pmatrix}
\cdot
\begin{pmatrix}
\phi^{n+1}_{ij} \\
\mu^{n+\frac{1}{2}}_{ij}
\end{pmatrix}
+
\begin{pmatrix}
 - \frac{1}{h^2} \Sigma_{Gij}\mu_{ij}^{n+\frac{1}{2}} \\
+ \frac{\varepsilon^2}{h^2} \Sigma_{Gij}\phi_{ij}^{n+1} \\
\end{pmatrix}
,\\
\begin{pmatrix}
  \zeta_{ij}^n\\
\psi_{ij}^n
\end{pmatrix}
-
\begin{pmatrix}
 - \frac{1}{h^2} \Sigma_{Gij}\mu_{ij}^{n+\frac{1}{2}} \\
+ \frac{\varepsilon^2}{h^2} \Sigma_{Gij}\phi_{ij}^{n+1} \\
\end{pmatrix}
&=
DL\begin{pmatrix}
\phi_{ij}^{n+1} \\
\mu_{ij}^{n+\frac{1}{2}}
\end{pmatrix}
\cdot
\begin{pmatrix}
\phi^{n+1}_{ij} \\
\mu^{n+\frac{1}{2}}_{ij}
\end{pmatrix}
\,,
\end{aligned}
\end{equation}
where
- \(  \Sigma_G \phi_{ij}^{n+1} = G_{i+\frac{1}{2}j} \phi^{n + 1,m}_{i+1j} +  G_{i-\frac{1}{2}j} \phi^{n + 1,m}_{i-1j} + G_{ij+\frac{1}{2}}  \phi^{n + 1,m}_{ij+1} + G_{ij-\frac{1}{2}} \phi^{n + 1,m}_{ij-1}  \),
- \(  \Sigma_G \mu_{ij} = G_{i+\frac{1}{2}j} \mu^{n + \frac{1}{2},m}_{i+1j} +  G_{i-\frac{1}{2}j} \mu^{n + \frac{1}{2},m}_{i-1j} + G_{ij+\frac{1}{2}}  \mu^{n + \frac{1}{2},m}_{ij+1} + G_{ij-\frac{1}{2}} \mu^{n + \frac{1}{2},m}_{ij-1}  \),
In order to compute \( \left(   \phi_{ij}^{n+1} , \mu^{n+\frac{1}{2}}_{ij}  \right) \) we have to evaluate those grid-wise functions on at neighboring indices \( k,l \) e.g. \( k=i+1 , l=j-1 \).
Since values for \( \phi_{kl}^{n+1,m} , \mu_{kl}^{n+\frac{1}{2},m} \) are unknown, if \( k > i , l > j \), the authors in [cite:@SHIN20117441] and we use initial approximations, and the values of the current smooth iteration else. As initial approximation we use the values of \(  \phi_{kl}^{n+1,m} , \mu_{kl}^{n+\frac{1}{2},m}  \) from the last smoothing iteration.
The equation Eq.[[eqref:eq:explicit-smooth]] is of form \(b = Ax\)
We then and solve Eq.[[eqref:eq:explicit-smooth]] for \( \left( \phi_{ij}^{n+1} , \mu^{n+\frac{1}{2}}_{ij}  \right)  \).
#+name: calculate-left-hand-side-b
#+begin_src julia :eval never :exports none
bordernumber = neighbours_in_domain(i, j, G, solver.len, solver.width)

b = [(
            solver.xi[i, j]
            +
            discrete_G_weigted_neigbour_sum(
                i, j, solver.potential, G, solver.len, solver.width
            ) / solver.h^2
        ), (
            solver.psi[i, j]
            -
            (solver.epsilon^2 / solver.h^2) * discrete_G_weigted_neigbour_sum(
                i, j, solver.phase, G, solver.len, solver.width
            ))]


#+end_src
#+name:SMOOTH
#+begin_src julia :tangle src/multisolver.jl :eval never :noweb no-export
function SMOOTH!(
    solver::T,
    iterations,
    adaptive
) where T <: Union{multi_solver, adapted_multi_solver , gradient_boundary_solver}
    for k = 1:iterations
        # old_phase = copy(solver.phase)
        for I in CartesianIndices(solver.phase)[2:end-1, 2:end-1]
            i, j = I.I

            <<calculate-left-hand-side-b>>

            res = dL(solver, i,j ) \ b
            solver.phase[i, j] = res[1]
            solver.potential[i, j] = res[2]
        end
    end
end
#+end_src
We denote the approximations for \( \left( \phi_{ij}^{n+1} , \mu^{n+\frac{1}{2}}_{ij}  \right)  \) after smoothing, as  \( \left( \bar{\phi}_{ij}^{n+1} , \bar{\mu}^{n+\frac{1}{2}}_{ij}  \right)  \)
In Fig.[[fig:smoothing-examples]] we show 4 of the 7 initial data after one 200 iterations of smoothing. It is apparent that the sharp interface from the initial Data has diffused.
#+name: fig:smoothing-examples
#+begin_src julia-vterm :results file graphics  :file smooth.svg
<<input>>
<<setup-diverse-testgrids>>
plots= []
for t in tests
set_xi_and_psi!(t[1])
SMOOTH!(t[1], 200, true);
end
plots =[  heatmap(t[1].phase ,  legend=:none , aspectratio=:equal , grid=false , showaxis=false , size=(600,600))
          for t in tests[1:2:end]]
p = plot(plots... , layout=(1,4) , size=(2400,600))
savefig(p,"images/smooth.svg")

#+end_src

#+caption: Inputs from [[Initial data]] after SMOOTH.
#+RESULTS[fdb9207550b6615253fa672f5417f153b861be3b]: fig:smoothing-examples
[[file:images/smooth.svg]]

** Multigrid method
The numerical method proposed in [cite:@SHIN20117441] consists of a V-cycle multi-grid method derived from previously stated operators. Specifically we use a two-grid implementation consisting of.
#+begin_src julia :eval never :exports code
for j in 1:timesteps

    set_xi_and_psi!(solvers[1])

    for i = 1:subiterations

        v_cycle!(solvers, 1)
    end
end
#+end_src
where the V-cycle is a
1. A Gauss-Seidel relaxation for smoothing, as described in Chapter [[SMOOTH operator]].
2. calculate the residual error  \( \left(d_{ij,H}^{n+1,m} , r_{ij,H}^{n+1,m} \right) = L\left( \phi_{ij}^{n+1} , \mu^{n+\frac{1}{2}}_{ij}  \right) - (\zeta^n_{ij} , \psi^n_{ij}  )  \).
3. restriction and between grids \(  h \to H  \).
4. a Newton iteration to solve \( L(\hat{\phi}_{ij,H}^{n+1,m}, \hat{\mu}_{ij,H}^{n+\frac{1}{2},m})_H = L(\bar{\phi}_{ij,H}^{n+1,m} , \bar{\mu}_{ij,H}^{n+\frac{1}{2},m}) + (d_{ij,H}^{n+1,m} , r_{ij,H}^{n+1,m}) \).
We solve for \( \left( \hat{\phi}_{ij,H}^{n+1,m}, \hat{\mu}_{ij,H}^{n+\frac{1}{2},m} \right) \) using the same iteration as in Chapter [[SMOOTH operator]] however we replace \( (\zeta_{ij}^{n} , \psi_{ij}^n) \) with  \(  L(\bar{\phi}_{ij,H}^{n+1,m} , \bar{\mu}_{ij,H}^{n+\frac{1}{2},m}) + (d_{ij,H}^{n+1,m} , r_{ij,H}^{n+1,m}) \).  In the iteration, where \( \bar{\phi}_{ij,H}^{n+1,m} , \bar{\mu}_{ij,H}^{n+\frac{1}{2},m} \) are the values after the smooth restricted to the coarser grid and \( d_{ij,H}^{n+1,m} , r_{ij,H}^{n+1,m} \) is the residual from the smooth  iteration on the fine grid restricted onto the coarse grid.
5. prolongation from \( H\to h \)
6. post smoothing

#+name: restrict-to-coarse-grid
#+begin_src julia :eval never :exports none
restrict_solver!(grid[level], grid[level+1])
solver = grid[level+1]
solution = deepcopy(solver)

d_large = restrict(d, G)
r_large = restrict(r, G)


u_large = zeros(size(d_large))
v_large = zeros(size(d_large))

#+end_src
#+name: prolong-to-fine-grid
#+begin_src julia :eval never :exports none
u_large = solver.phase .- solution.phase
v_large = solver.potential .- solution.potential

solver = grid[level]

solver.phase .+= prolong(u_large , G)
solver.potential .+= prolong(v_large, G)

#+end_src
The V-cycle of a two-grid method using pre- and post-smoothing is then stated by:
#+begin_src julia :tangle src/multisolver.jl :eval never :noweb no-export
function v_cycle!(grid::Array{T}, level) where T <: solver
    solver = grid[level]
    #pre SMOOTHing:
    SMOOTH!(solver, 400, false)

    d = zeros(size(solver.phase))
    r = zeros(size(solver.phase))

    # calculate error between L and expected values
    for I in CartesianIndices(solver.phase)[2:end-1, 2:end-1]
        d[I], r[I] = [solver.xi[I], solver.psi[I]] .- L(solver, I.I..., solver.phase[I], solver.potential[I])
    end

    <<restrict-to-coarse-grid>>

    #Newton Iteration for solving smallgrid
    for i = 1:300
        for I in CartesianIndices(solver.phase)[2:end-1, 2:end-1]

            diffrence = L(solution, I.I..., solution.phase[I], solution.potential[I])
                        .- [d_large[I], r_large[I]]
                        .- L(solver, I.I..., solver.phase[I], solver.potential[I])

            local ret = dL(solution, I.I...) \ diffrence

            u_large[I] = ret[1]
            v_large[I] = ret[2]
        end
        solution.phase .-= u_large
        solution.potential .-= v_large
    end

    <<prolong-to-fine-grid>>

    SMOOTH!(solver, 800, false)
end
#+end_src


After a few iterations, V-cycle exhibits the following behavior:

#+name: fig:solver-iteration
#+begin_src julia-vterm :results file graphics  :file iteration.gif :noweb no-export :async t :exports results :output-dir images  :tangle src/plot.jl :session jl :eval never-export
<<init>>
using JLD2
using DataFrames
results = jldopen("experiments/iteration.jld2")["result"]
anim = @animate for res in eachrow(results)
    heatmap(res.solver.phase , title="phase field" , legend=:none , aspectratio=:equal , showaxis=false , grid=false , size=(400 ,400))
end
gif(anim , "images/iteration.gif" , fps = 10)
#+end_src

#+caption: A few time steps of the solver for different initial conditions as shown in [[Initial data]]
#+RESULTS: fig:solver-iteration
[[file:images/iteration.gif]]

* Numerical experiments
In the previous Chapter we discretized the CH equation based on the multigrid method described by the authors in [cite:@SHIN20117441] and we obtained a numerical scheme for \( \phi , \mu \). In this chapter we analyse the change in mass, change in total energy \( E^{bulk} \), stability in time , space and during sub-iterations.

** Energy evaluations
As discrete energy measure we use:
#+name: eq:discrete-energy
\begin{equation}
\begin{aligned}
E^{\text{bulk}}_d(\phi_{ij}) &= \sum_{i,j \in \Omega} \frac{\varepsilon^2}{2} |G\nabla_d \phi_{ij} |^2 + W\left(\phi_{ij}\right)  \\
&= \sum_{i,j \in \Omega} \frac{\varepsilon^2}{2} G_{i+\frac{1}{2}j}(D_x\phi_{i+\frac{1}{2}j}) ^2 + G_{ij+\frac{1}{2}}(D_y\phi_{ij+\frac{1}{2}})^2  + W\left(\phi_{ij}\right)  .\\
\end{aligned}
\end{equation}
Since the continous total energy Eq.[[eqref:eq:energy]] decreases over time, we expect it's discrete couterpart to exhibit the same behaviour. Them numerical implementation for the bulk energy can be found in the Appendix [[bulk energy and mass balance]].
In Fig.[[fig:energy-balance]] we observe the discrete total energy going down with increasing number of time-steps, as we expect from a  CH based solver. Visually we observe the energy decrease as reduced surface curvature.
#+name: fig:energy-balance
#+begin_src julia-vterm :results file graphics :file energy_balance.svg
<<init>>
using JLD2
using DataFrames
i0 = 1*64 +1
results = jldopen("experiments/iteration.jld2")["result"]
energy = bulk_energy.(results[i0:i0+63,:].solver)
p1 = plot(1:64 ,
          energy ,
          title=L"Discrete Helmholtz Energy $E_d^{bulk}$",
          xlabel="timesteps" ,
          ylabel="energy"  ,
          label=false)
p2 = heatmap(results.solver[i0].phase ,
             title="initial condition" ,
             legend=:none ,
             aspectratio=:equal ,
             showaxis=false ,
             grid=false)
p3 = heatmap(results.solver[i0+63].phase ,
             title="after 64 time-steps" ,
             aspectratio=:equal ,
             legend=:none ,
             showaxis=false ,
             grid=false)
p = plot(p2,p3,p1 , layout=layout3x1 , size=size3x1  )

savefig(p , "images/energy_balance.svg")
#+end_src

#+caption: Behaviour of energy \( E_{bulk} \) over time for one initial condition \( \phi_0 \).
#+RESULTS: fig:energy-balance
[[file:images/energy_balance.svg]]

** Numerical mass conservation
The analytical CH equation in Eq.[[eqref:eq:CH]]  is mass conservative as shown in Eq.[[eqref:eq:mass-conservation]].
Instead of a physical mass we use the average of \(\phi\) over the domain \(\Omega\).
This yields a balance between both phases.  Since our implementation uses no-flow boundary conditions the balance between /phase 1/ and /phase 2/ stays the same. We therefore calculate a balanace
\begin{align*}
b &= \frac{\sum_{i,j \in \Omega} \phi_{ij}}{N^2}
\end{align*}
such that \( b = 1 \) means there is only phase 1, \( \phi \equiv 1 \), and \( b = -1 \) means there is only phase 2, \( \phi \equiv -1 \).
Ideally this value stays constant over time for numerical mass conservation.
In practice we observe slight fluctuations in Figure [[fig:mass-balance]]. Those however are close to machine precision and can therefore be ignored. The numerical impolementation is  in appendix [[bulk energy and mass balance]].

#+name: fig:mass-balance
#+begin_src julia-vterm :results file graphics :file mass_balance.svg :output-dir images :noweb no-export :session jl
<<init>>
using JLD2
using DataFrames
using Measures
pgfplotsx()
i0 = 64 * 1 + 1
results = jldopen("experiments/iteration.jld2")["result"]
energy = [ massbal(s.phase) for s in results[i0:i0+63,:].solver]
p1 = plot(1:64 ,
          energy .- energy[1],
          xlabel= "time-steps" ,
          ylabel = "error" ,
          title = "phase change",
          label=false)
p2 = heatmap(results.solver[i0].phase ,
             title="initial condition" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(results.solver[i0+63].phase ,
             title="after 64 time-steps" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false ,
             legend=:none)

p = plot(p2,
         p3 ,
         p1 ,
         layout= layout3x1 ,
         size=size3x1)

savefig(p , "images/mass_balance.svg")
#+end_src

        #+caption: Behaviour of phase change over time for one initial condition \( \phi_0 \).
#+RESULTS: fig:mass-balance
[[file:images/mass_balance.svg]]

** Stability of a multi-grid sub-iteration
We expect our solver to stay stable when increasing the number of multigrid sub-iterations. To validate this assumption we compare the phase-field of the current sub-iteration \( \phi^{n+1,m}_{ij} \) with the phse-field of the previous sub-iteration \( \phi_{ij}^{n+1,m-1} \).
\begin{equation}
\| \phi^{n+1,m-1} - \phi^{n+1,m} \|_{Fr}= \sqrt{ \sum_{i,j \in \Omega_d} \left|   \phi^{n+1,m-1}_{ij} - \phi^{n+1,m}_{ij} \right|^2 }
\end{equation}
 As sub-iterations increase , \( m\to\infty \),  we expect the difference between both phase-fields to go to zero \( \|\phi^{n+1,m} - \phi^{n+1,m-1}\|_{Fr} \to 0 \). We observe this behaviour in Figure [[fig:convergence]]
#+name: fig:convergence
#+begin_src julia-vterm :results file graphics :file convergence.svg
<<init>>
<<setup-diverse-testgrids>>
using DataFrames
using JLD2
using LaTeXStrings
i0 = 4
df = jldopen("experiments/subiteration.jld2")["result"]
gd = groupby(df , :iteration)
res = combine(gd  , :cycle => ((x)-> [norm(x[i].phase - x[i-1].phase) for i in 2:size(x,1)]))

gres =  groupby(res , :iteration)[1]
p1= res.cycle_function[i0*64:(i0+1)*64-2] |>
    (x)-> plot(x ,
               yscale=:log10 ,
               title="Behaviour" ,
               xlabel="sub-iterations" ,
               ylabel= "diffrence" ,
               label= L" \|\phi^{n+1,m} - \phi^{n+1,m-1}\|_{Fr} ")
p2 = heatmap(df.cycle[i0].phase ,
             title="initial condition" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(df.cycle[i0].phase .-df.cycle[i0+62].phase ,
             title="after 64 subiteration" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false )

p = plot(p2,
         p3 ,
         p1 ,
         layout= layout3x1 ,
         size=(1600 , 1600))
savefig(p , "images/convergence.svg")
#+end_src

#+caption: Stability of the original CH solver for increasing sub-iterations
#+RESULTS: fig:convergence
[[file:images/convergence.svg]]

in practise we observe the behaviour we expect, where an increasing number of sub-iterations leads to decreasing change compared to the previous sub-iteration.

#+begin_src julia-vterm :results file graphics :file subiteration.svg :output-dir images :noweb no-export :session jl :exports none
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
df = jldopen("experiments/subiteration.jld2")["result"]
gd = groupby(df , :iteration)
p1 = heatmap(gd[1].cycle[1].phase , aspectratio=:equal , title= "one subiteration" , showaxis=false  )
p2 = heatmap(gd[1].cycle[64].phase , aspectratio=:equal , title = "64 sub-iterations" , showaxis=false)
p = plot(p1,p2)
savefig(p , "images/subiteration.svg")
#+end_src

#+RESULTS:
[[file:images/subiteration.svg]]

** Stability in time
We expect our numerical error to decrease when calculating with smaller time steps. To test this, we  successively subdivide the original time interval \( [0,T] \) in finer parts. We fix \( \Delta t \cdot n = T \) for \( T=10^{-2} \) and test different values of \( n \). In Figure [[fig:stability-in-time]] we compare the phase-field \( \phi^{n}_{ij} \) and \( \phi^{n-1}_{ij}  \) at \( T=10^{-2} \). and observe the decrease we expect.
#+name: fig:stability-in-time
#+begin_src julia-vterm :results file graphics :file time-stability.svg
<<init>>
using DataFrames
using JLD2
using LaTeXStrings

df = jldopen("experiments/time.jld2")["result"]
gd = groupby(df , :iteration)

sd =  combine(x->(;phase=x[end,:].phase) , gd)
change = [norm(sd[!, "phase"][i] .- sd[! , "phase"][i-1]) for i=2:size(sd , 1)]

p1 = plot(change ,
         ylabel = "difference to previous number time-steps" ,
         xlabel = L"number of time-steps to $t = 10^{-2}s$" ,
         label=L"\|\phi_{ij}^{n+1} - \phi_{ij}^n \|_{Fr}" ,
         title= L"behavior of the original CH solver at $t=10^{-2}s$")
p2 = heatmap(gd[32].phase[end],
             title=L"$t=10^{-2} \,, n=32$" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(gd[end].phase[end],
             title=L"$t=10^{-2} \,, n=64$" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false ,
             legend=:none)

p = plot(p2,
         p3 ,
         p1 ,
         layout= layout3x1 ,
         size=size3x1)
savefig(p , "images/time-stability.svg")
#+end_src

#+Caption: Behavior of the baseline solver while solving the time interval \( T = \left[ 0 , 10^{-2} \right] \) with increasing number of time-steps.
#+RESULTS[c9dd717190350aef1329f2cbdb26859677756133]: fig:stability-in-time
[[file:images/time-stability.svg]]

** Stability in space
We expect our methods to be stable under different grid-sizes \( h \) and grid-points \( N \). Therefore we expect the difference after one time-step between eg. a \( 512 \times 512 \) grid and a \( 1024 \times 1024 \) grid to be smaller than the difference between a \( 64 \times 64 \) grid and a \( 128 \times 128 \) grid. In order to keep the problem the same , we fix \( Nh = 10^{-3} \cdot 1024 \) and test for \( N \in \left\{ 1024 , 512 , 256 , 128 , 64 , 32 \right\} \)
#+name: fig:stability-in-space
#+begin_src julia-vterm :results file graphics :file space-stability.svg
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
df = jldopen("experiments/space_refinement.jld2")["result"]
gd = groupby(df , :iteration)
change = [norm(df[!, "phase"][i] .- restrict(df[! , "phase"][i-16] , G))/*(size(df[!,"phase"][i])...) for i=17:16:size(df , 1)]
p1 = plot([L"1024^2 \to 512^2" , L"512^2 \to 256^2" , L"256^2\to128^2" , L"128^2\to64^2" , L"64^2 \to32^2"],
         change ,
         ylabel = "difference" ,
         yscale=:log10,
         xlabel = "change in number of gridpoints" ,
         label=L"\Delta \phi" ,
         xscale=:log2 ,
         seriestype=:scatter ,
         xaxis=:flip ,
         legend=:topright)

p2 = heatmap(gd[16].phase[begin],
             title=L"1024 \times 1024" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(gd[16].phase[4],
             title=L"128 \times 128" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false ,
             legend=:none)

p = plot(p2,
         p3 ,
         p1 ,
         layout= layout3x1 ,
         size=size3x1)
savefig(p , "images/space-stability.svg")
#+end_src

#+Caption: Behavior of the baseline solver while solving on successively finer grids
#+RESULTS[d65d9dfebf929c604fd273a9c1ffcfd955bb13e0]: fig:stability-in-space
[[file:images/space-stability.svg]]

* Relaxed problem
In effort to decrease the order of complexity, from fourth order derivative to second order, we propose an elliptical relaxation approach, where the relaxation variable \( c \) is the solution of the following elliptical PDE:
#+name: eq:elliptical-equation
\begin{align}
- \Delta c^\alpha  + \alpha c^a &= \alpha \phi ^\alpha,
\end{align}
where \( \alpha \) is a relaxation parameter. We expect to approach the original solution of the CH equation Eq.[[eqref:eq:CH]] as  \( \alpha \to \infty \).
This results in the following relaxation for the classical CH equation Eq.[[eqref:eq:CH]]:
#+name: eq:relaxed-cahn-hilliard
\begin{equation}
\begin{aligned}
\partial_t \phi^\alpha  &= \Delta \mu \,,\\
\mu &= \varepsilon ^2 \alpha(c^\alpha - \phi^\alpha) + W'(\phi) .
\end{aligned}
\end{equation}
It requires solving the elliptical PDE each time-step to calculate \(c\).

As ansatz for the numerical solver we propose:
#+name: eq:discrete-relaxed-cahn-hilliard
\begin{equation}
\begin{aligned}
\frac{\phi_{ij}^{n+1,\alpha} - \phi_{ij}^{n,\alpha}}{\Delta t}  &=  \nabla _d \cdot (G_{ij} \nabla_d \mu_{ij}^{n+\frac{1}{2},\alpha} )  \,,\\
 \mu_{ij}^{n+\frac{1}{2},\alpha} &= 2\phi_{ij}^{n+1,\alpha} - \varepsilon^2 a(c_{ij}^{n+1,\alpha} - \phi_{ij}^{n+1,\alpha})  + W'(\phi_{ij}^{n,\alpha}) - 2\phi _{ij}^{n,\alpha} \,.
\end{aligned}
\end{equation}
This approach is inspired by Eq.[[eqref:eq:discrete-cahn-hilliard]] and adapted to the relaxed CH equation in Eq.[[eqref:eq:discrete-relaxed-cahn-hilliard]].
We then apply the multi-grid method proposed in [[Baseline multi-grid solver]] to the relaxed problem by replacing the differential operators with their discrete counterparts, as defined in Eq.[[eqref:eq:discretization]],
and expand them.
** Elliptical PDE
In order to solve the relaxed CH equation we solve the following PDE in each  time step:
\begin{align*}
- \nabla \cdot  (G \nabla c^\alpha) + \alpha c^\alpha  = \alpha \phi ^\alpha \,.
\end{align*}
Similarly to the first solver we solve this PDE  with a finite difference scheme using the same discretization as before.
*** Discretization
The discretization of the PDE expands the differential operators in the same way and proposes an equivalent scheme for solving the elliptical equation Eq.[[eqref:eq:elliptical-equation]].
\begin{align*}
- \nabla_d \cdot  (G_{ij} \nabla_d c_{ij}^\alpha) + \alpha  c_{ij}^\alpha &= \alpha \phi_{ij}^\alpha
\end{align*}
\( \implies \)
\begin{align*}
- (\frac{1}{h}(G_{i+\frac{1}{2}j} \nabla c^\alpha_{i+\frac{1}{2}j} + G_{ij+\frac{1}{2}} \nabla c^\alpha_{ij+\frac{1}{2}}) &  \\
- (G_{i-\frac{1}{2}j} \nabla c^\alpha_{i-\frac{1}{2}j} + G_{ij-\frac{1}{2}} \nabla c^\alpha_{ij-\frac{1}{2}})) + \alpha  c_{ij}^\alpha   &= \alpha  \phi_{ij}^\alpha
\end{align*}
\( \implies \)
\begin{align*}
- \frac{1}{h^2} ( G_{i+\frac{1}{2}j}(c_{i+1j}^\alpha - c_{ij}^\alpha) & \\
+G_{ij+\frac{1}{2}}(c_{ij+1}^\alpha - c_{ij}^\alpha) & \\
+G_{i-\frac{1}{2}j}(c_{i-1j}^\alpha - c_{ij}^\alpha)& \\
+G_{ij-\frac{1}{2}}(c_{ij-1}^\alpha - c_{ij}^\alpha)) + \alpha  c_{ij}^\alpha &=\alpha  \phi_{ij}^\alpha
\end{align*}


As before we abbreviate \(  \Sigma_G c^\alpha_{ij} = G_{i+\frac{1}{2}j} c^\alpha_{i+1j} +  G_{i-\frac{1}{2}j} c^\alpha_{i-1j} + G_{ij+\frac{1}{2}}  c^\alpha_{ij+1} + G_{ij-\frac{1}{2}} c^\alpha_{ij-1}  \) and \(  \Sigma_{Gij} = G_{i+\frac{1}{2}j} + G_{i-\frac{1}{2}j} + G_{ij+\frac{1}{2}} + G_{ij-\frac{1}{2}}  \). Then the discrete elliptical PDE can be stated as:
#+name: eq:discrete_elyps
\begin{align}
-\frac{ \Sigma_G c^\alpha_{ij}}{h^2} + \frac{\Sigma_G}{h^2} c^\alpha_{ij} + \alpha c^\alpha_{ij} &= \alpha\phi^\alpha_{ij} \,.
\end{align}
Solving Eq.[[eqref:eq:discrete_elyps]] for \(c_{ij}^\alpha \) then results in.
\begin{align*}
\left( \frac{\Sigma_{Gij}}{h^2} + \alpha \right)c_{ij}^{\alpha} = \alpha\phi^{\alpha}_{ij} + \frac{\Sigma_G c_{ij}^{\alpha}}{h^2}\\
c_{ij}^{\alpha} = \frac{\alpha\phi^{\alpha}_{ij} + \frac{\Sigma_G c_{ij}^{\alpha}}{h^2}}{\frac{\Sigma_{G}}{h^2} + \alpha}\\
c_{ij}^{\alpha} = \frac{\alpha h^2 \phi^{\alpha}_{ij}}{\Sigma_{Gij} + \alpha h^2} + \frac{\Sigma_G c_{ij}^{\alpha}}{\Sigma_{Gij} + \alpha h^{2}}
\end{align*}
and can be translated to code as follows
#+begin_src julia :eval never :tangle src/elypssolver.jl :exports none
using ProgressBars

"""
    elyps_solver(c,
    phase,
    len,
        width,
    alpha,
    h,
    n
)

TBW
"""
#+end_src
#+name: elyps_solver
#+begin_src julia :eval never :tangle src/elypssolver.jl
function elyps_solver!(solver::T, n) where T  <: Union{relaxed_multi_solver , adapted_relaxed_multi_solver}
    for k in 1:n
        for i = 2:(solver.len+1)
            for j = 2:(solver.width+1)
                bordernumber = neighbours_in_domain(i, j,G, solver.len, solver.width)
                solver.c[i, j] =
                    (
                        solver.alpha * solver.phase[i, j] +
                        discrete_G_weigted_neigbour_sum(i, j, solver.c, G, solver.len, solver.width) / solver.h^2
                    ) / (bordernumber / solver.h^2 + solver.alpha)

            end
        end
    end
end
#+end_src
** Relaxed system
We reformulate the discretization in Eq.[[eqref:eq:discrete-relaxed-cahn-hilliard]] in terms of the relaxed function \(L\) as follows:
\begin{align*}
L_r
\begin{pmatrix}
\phi ^{n+1,\alpha}_{ij} \\
\mu^{n+\frac{1}{2},\alpha}_{ij}
\end{pmatrix}
&=
\begin{pmatrix}
\frac{\phi^{n+1,m,\alpha}_{ij}}{\Delta t} - \nabla _d \cdot (G_{ji} \nabla _d \mu^{n + \frac{1}{2},m,\alpha}_{ji}) \\
\varepsilon ^2 \alpha (c^\alpha_{ij} - \phi^{n+1,m,\alpha}_{ij}) - 2\phi ^{n+1,m,\alpha}_{ij} -\mu^{n + \frac{1}{2},m,\alpha}_{ji}
\end{pmatrix}
\end{align*}

and its Jacobian:
\begin{align*}
DL_r\begin{pmatrix}
\phi^{n+1,\alpha, m}_{ij} \\
\mu^{n+\frac{1}{2},m,\alpha}_{ij}
\end{pmatrix} &= \begin{pmatrix}
\frac{1}{\Delta t} & \frac{1}{h^2}\Sigma_{G}  \\
- \varepsilon^2 \alpha  - 2 & 1
\end{pmatrix}
\end{align*}

** The relaxed multigrid method
As the difference between both methods is abstracted away in the operators, the relaxed V-cycle the replaces the original operators with their relaxed counterparts. Due to julias multiple dispatch features this changes nothing in the implementation Therefore we reuse the original V-cycle in the [[Multigrid method]].
In the executions for each time step, we add the elliptic solver in the subiteration.
#+begin_src julia :eval never :exports code
for j in 1:timesteps

    set_xi_and_psi!(solvers[1])

    for i = 1:subiterations

        elyps_solver!(solvers[1] , 1000)
        v_cycle!(solvers, 1)
    end
end
#+end_src

#+name: fig:relaxed-anim
#+begin_src julia-vterm :results file graphics :file relaxed-anim.gif
<<init>>
using JLD2
using DataFrames
using Measures

gr()

results = jldopen("experiments/relaxed-iteration4.jld2")["result"]
anim = @animate for s in results.solver
    heatmap(s.phase)
    end
gif(anim , "images/relaxed-anim.gif", fps=10)
#+end_src

#+RESULTS: fig:relaxed-anim
[[file:images/relaxed-anim.gif]]

** SMOOTH operator
The relaxed solver uses the same approach as the original solver, where we solve \( L_r(\phi^{n+1,m,\alpha}_{ij}, \mu^{n+\frac{1}{2},m,\alpha}_{ij}) = (\zeta_{ij}^n , \psi_{ij}^n)^T \) for each grid-point \( \phi_{ij}^{n+1,m,\alpha} \). Notably \((\zeta_{ij}^n , \psi_{ij}^n)^T  \) is the same as in the original part. As in the original smoothing, evalations of \( \mu^{n+\frac{1}{2},m,\alpha}_{kl} \) for \( k,l > i,j \) are replaced with their values from the previous SMOOTH iteration.

Correspondingly the SMOOTH operation expands to:
#+name: eq:discrete-relaxed-smooth
\begin{equation}
\begin{aligned}
  -\frac{\Sigma_{Gij}}{h^2}\overline{\mu^{n + \frac{1}{2},m,\alpha}_{ji}} &= \frac{\phi ^{n+1,m,\alpha}_{ij}}{\Delta t} - \zeta^{n,\alpha}_{ij} - \frac{\Sigma_G\mu_{ij}}{h^2} \,,\\
 \varepsilon ^2 \alpha \overline{\phi ^{n+1,m,\alpha}_{ij}} + 2 \phi ^{n+1,m,\alpha}_{ij} &= \varepsilon ^2 \alpha c^{n,\alpha}_{ij}  -\overline{\mu^{n + \frac{1}{2},m,\alpha}_{ji}}  - \psi_{ij}^{n,\alpha} \,,
\end{aligned}
\end{equation}
where
- \(  \Sigma_G \mu_{ij} = G_{i+\frac{1}{2}j} \mu^{n + \frac{1}{2},m}_{i+1j} +  G_{i-\frac{1}{2}j} \mu^{n + \frac{1}{2},m}_{i-1j} + G_{ij+\frac{1}{2}}  \mu^{n + \frac{1}{2},m}_{ij+1} + G_{ij-\frac{1}{2}} \mu^{n + \frac{1}{2},m}_{ij-1}  \),
We then solve directly for the smoothed variables, \( \overline{\mu_{ij}^{n+1,m,\alpha}} \) and \( \overline{\phi_{ij}^{n+1,m,\alpha}} \). This was not done in the original paper [cite:@SHIN20117441] because the required system of linear equations in the paper [cite:@SHIN20117441]  was solved numerically.
\begin{align*}
\varepsilon^2 \alpha(\phi_{ij}^{n+1,m,\alpha}) + 2\phi_{ij}^{n+1,m,\alpha} &= \varepsilon^2 \alpha c^\alpha - \frac{h^2}{\Sigma_G} (\frac{\phi_{ij}^{n+1,m,\alpha}}{\Delta t} - \zeta^n_{ij} - \frac{1}{h^2} \Sigma_G \mu_{ij}) - \psi_{ij}
\end{align*}
\( \implies \)
\begin{align*}
\varepsilon^2\alpha (\phi_{ij}^{n+1,m,\alpha}) + 2\phi_{ij}^{n+1,m,\alpha} + \frac{h^2}{\Sigma_{Gij}}\frac{\phi_{ij}^{n+1,m,\alpha}}{\Delta t} &= \varepsilon^2 \alpha c^\alpha - \frac{h^2}{\Sigma_G} (- \zeta^n_{ij} - \frac{1}{h^2} \Sigma_G \mu_{ij}) - \psi_{ij}
\end{align*}
\( \implies \)
\begin{align*}
(\varepsilon^2 \alpha + 2 + \frac{h^2}{\Sigma_G \Delta t}) \phi_{ij}^{n+1,m,\alpha} &= \varepsilon^2 \alpha c^\alpha - \frac{h^2}{\Sigma_G}(- \zeta^n_{ij} - \frac{\Sigma_G \mu_{ij}}{h^2} ) -\psi_{ij}
\end{align*}
\( \implies \)
\begin{align*}
 \phi_{ij}^{n+1,m,\alpha} &= \left(\varepsilon^2 \alpha c^\alpha - \frac{h^2}{\Sigma_G}(- \zeta^n_{ij} - \frac{\Sigma_G \mu_{ij}}{h^2} ) -\psi_{ij}\right)\left(\varepsilon^2 \alpha + 2 + \frac{h^2}{\Sigma_G \Delta t}\right)^{-1}
\end{align*}
#+name: solve-for-phi
#+begin_src julia :eval never :exports none
bordernumber = neighbours_in_domain(i, j, G, solver.len, solver.width)

solver.phase[I] = (solver.epsilon^2 * solver.alpha * solver.c[I] - solver.h^2 / bordernumber * ( -solver.xi[I]  - discrete_G_weigted_neigbour_sum(i,j,solver.potential , G , solver.len , solver.width) / solver.h^2 ) - solver.psi[I]) / (solver.epsilon^2 * solver.alpha  + 2 + solver.h^2 / (bordernumber*solver.dt))
#+end_src
#+name: update-the-potential
#+begin_src julia :eval never :exports none
            solver.potential[I] = (solver.phase[I]/solver.dt - solver.xi[I] - discrete_G_weigted_neigbour_sum(i,j, solver.potential , G , solver.len , solver.width)/solver.h^2) * (-solver.h^2/bordernumber)
#+end_src
#+name: SMOOTH_relaxed
#+begin_src julia :eval never :tangle src/multi_relaxed.jl :noweb no-export
function SMOOTH!(
    solver::T,
    iterations,
    adaptive
) where T <: Union{relaxed_multi_solver , adapted_relaxed_multi_solver}
    for k = 1:iterations
        # old_phase = copy(solver.phase)
        for I in CartesianIndices(solver.phase)[2:end-1, 2:end-1]
            i, j = I.I
            <<solve-for-phi>>
            <<update-potential>>
        end

        #if adaptive && LinearAlgebra.norm(old_phase - solver.phase) < 1e-10
            ##println("SMOOTH terminated at $(k) succesfully")
            #break
        #end
    end
end
#+end_src

#+name: fig:relaxed-smooth-eval
#+begin_src julia-vterm :results file graphics :file smooth_relaxed.svg
<<init>>
plots = []
eps = 0.13
#M = testdata(64, div(64,3), 64/5 , 2)
for alpha in [1e3 , 1e4 , 1e5 , 1e6 , 32500]
local testgrd = testgrid(relaxed_multi_solver,M, 2 ; alpha=alpha , epsilon=eps)
set_xi_and_psi!(testgrd[1])
elyps_solver!(testgrd[1] , 2000)
SMOOTH!(testgrd[1], 1000, false);
push!(plots , heatmap(testgrd[1].phase, aspect_ratio=:equal, title=L"$\alpha = %$alpha$" , xlim=(2,testgrd[1].len) , ylim=(2,testgrd[1].width) , showaxis=false , legend=false));
    end

original = testgrid(multi_solver,M, 2)
set_xi_and_psi!(original[1])
SMOOTH!(original[1], 1000, false);
push!(plots , heatmap(original[1].phase, aspect_ratio=:equal, title="original" , xlim=(2,original[1].len) , ylim=(2,original[1].width) , showaxis=false , legend=false));
p = plot(plots...)
savefig(p,"images/smooth_relaxed.svg")
#+end_src

#+caption: Effect of the relaxed SMOOTH operator, and additional solving of the elliptical problem, for different values of alpha
#+RESULTS: fig:relaxed-smooth-eval
[[file:images/smooth_relaxed.svg]]

Furthermore, experimentation shows that alpha alone is insufficient to get a relaxed method consistent with the original solver, since alpha had an effect similar to epsilon, where it changed the boundary thickness in the phase-field \( \phi \). Therefore epsilon and alpha cannot be chosen independently. Hence we use a simple MCMC optimizer for \( \alpha,\varepsilon \) in order to give the relaxed solver the best chance we can.
Monte Carlo Optimizer For \( \varepsilon , \alpha \).
* Relaxed experiments
We expect the relaxed solver to behave the same as the baseline method for all test cases that we have introduced in Chapter [[Numerical experiments]]. Therefore we run the same experiments for our relaxed solver.
** Relaxed energy evaluations
we do evaluate our relaxed method using the discrete Helmoltz energy defined in Eq.[[eqref:eq:discrete-energy]]. On the same initial data, and with the same values for \( \varepsilon , h , dt \) as in the Chapter.[[Energy evaluations]]. In Figure.[[fig:relaxed-energy-balance]] we then observe the energy decay we expected. Our relaxed approach closely follows the baseline, although it consistently decayed slightly faster. This is within our expectations.
#+name: fig:relaxed-energy-balance
#+begin_src julia-vterm :results file graphics :file relaxed-energy-balance.svg
<<init>>
using JLD2
using DataFrames
i0 = 1*64 +1
original_results = jldopen("experiments/iteration.jld2")["result"]
relaxed_results = jldopen("experiments/relaxed-iteration.jld2")["result"]
original_energy = bulk_energy.(original_results[i0:i0+63,:].solver)
relaxed_energy = bulk_energy.(relaxed_results[i0:i0+63,:].solver)
p1 = plot(1:64 ,
          original_energy ,
          title=L"Discrete Helmholtz Energy $E_d^{bulk}$",
          xlabel="timesteps" ,
          ylabel="energy"  ,
          label="original")
p1 = plot!(p1,
           1:64 ,
           relaxed_energy ,
           title=L"Discrete Helmholtz Energy $E_d^{bulk}$",
           xlabel="timesteps" ,
           ylabel="energy"  ,
           label="relaxed")
p2 = heatmap(relaxed_results.solver[i0].phase ,
             title="initial condition" ,
             legend=:none ,
             aspectratio=:equal ,
             showaxis=false ,
             grid=false)
p3 = heatmap(relaxed_results.solver[i0+63].phase ,
             title="after 64 time-steps" ,
             aspectratio=:equal ,
             legend=:none ,
             showaxis=false ,
             grid=false)
p = plot(p2,p3,p1 , layout=layout3x1 , size=(1600 ,1600))
savefig(p , "images/relaxed-energy-balance.svg")
#+end_src

#+caption: Energy decay of the relaxed solver compared to the original solver.
#+RESULTS[06f7ce276ee26e0f3adfda9d2fb591ad7786b44f]: fig:relaxed-energy-balance
[[file:images/relaxed-energy-balance.svg]]


We observe the discrete Helmoltz energy decrease is the same manner as with the original solver.
** Relaxed numerical mass balance
since both the CH equation Eq.[[eqref:eq:CH]] and the baseline solver from Fig.[[fig:mass-balance]] are mass conservative, the relaxed solver should be as well, to be competitive with the baseline approach. Our relaxed solver shows  mass loss around 2% as seen in Fig.[[fig:relaxed-mass-balance]]. This is nowhere near the machine precision, we reached in Fig.[[fig:mass-balance]]. However it is still tolerable.
#+name: fig:relaxed-mass-balance
#+begin_src julia-vterm :results file graphics :file relaxed-mass-balance.svg
<<init>>
using JLD2
using DataFrames
using Measures
i0 = 64 * 0+1
results = jldopen("experiments/relaxed-iteration.jld2")["result"]
energy = [ massbal(s.phase) .- massbal(results.solver[i0].phase) for s in results[i0:i0+63,:].solver]
p1 = plot(1:64 , energy, xlabel= "time-steps" , ylabel = "error"  , label =false)
p2 = heatmap(results.solver[i0].phase , title="initial condition" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
p3 = heatmap(results.solver[i0+63].phase , title="after 64 time-steps" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
p = plot(p2,p3 , p1 , layout=layout3x1 , size=(1600 ,1600))
savefig(p , "images/relaxed-mass-balance.svg")
#+end_src

#+caption: Mass los in the relaxed solver
#+RESULTS[178f08c5de1dc17908fc5f5b85bbb77b27d17b9f]: fig:relaxed-mass-balance
[[file:images/relaxed-mass-balance.svg]]

** Stability of a relaxed multigrid sub-iteration
We also compare the subiteration behaviour of the relaxed solver to the original we therefore plot \( \|\phi_{ij}^{n+1,m} - \phi_{ij}^{n+2,m-1} \|_{Fr} \) against \( \| \phi_{ij}^{n+1,m,\alpha} - \phi_{ij}^{n+1,m-1,\alpha} \| \) for \( m \in \{2, \dots , 64\} \). Here we observe instablility at about 60 sub-iterations in Fig.[[fig:relaxed-convergence]]. We are uncertain, as to why.
#+name: fig:relaxed-convergence
#+begin_src julia-vterm :results file graphics :file relaxed-convergence.svg
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
n=1024

i0 = 1
df = jldopen("experiments/subiteration.jld2")["result"]
df = groupby(df , :experiment)[i0]
gd = groupby(df , :iteration)
original_res = combine(gd  , :cycle => ((x)-> [norm(x[i].phase - x[i-1].phase) for i in 2:size(x,1)]))

original_res =  groupby(original_res , :iteration)[1].cycle_function


df = jldopen("experiments/relaxed-subiteration.jld2")["result"]
df = groupby(df , :experiment)[i0]
gd = groupby(df , :iteration)
relaxed_res = combine(gd  , :cycle => ((x)-> [norm(x[i].phase - x[i-1].phase) for i in 2:size(x,1)]))

relaxed_res =  groupby(relaxed_res , :iteration)[1].cycle_function
p=plot([original_res, relaxed_res],label= ["original"  "relaxed"])
savefig(p , "images/relaxed-convergence.svg")
#+end_src

#+RESULTS: fig:relaxed-convergence
[[file:images/relaxed-convergence.svg]]

** Relaxed stability in time
we test the behaviour under refinement in time by succesivly subdividing the original time interval \( [0,T] \) in finer parts. We use the same meassure as in Chaper.[[Stability in time]] and directly compare. We observe simmilar behaviour to the original solver in Fig.[[fig:relaxed-stability-in-time]]. The relaxed solver has consisten lower difference than the original solver. This might suggest a more consistent method over time. However since the sub-iteration showed problematic behaviour, this micht also be a side-effect of this.
#+name: fig:relaxed-stability-in-time
#+begin_src julia-vterm :results file graphics :file relaxed-time-stability.svg
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
df = jldopen("experiments/relaxed-time.jld2")["result"]
dfo = jldopen("experiments/time.jld2")["result"]
gdo = groupby(dfo,:iteration)
dfo = DataFrame([ last(x) for x in gdo])
change = [norm(df[!, "phase"][i] .- df[! , "phase"][i-1]) for i=2:size(df , 1)]
change0 = [norm(dfo[!, "phase"][i] .- dfo[! , "phase"][i-1]) for i=2:size(dfo , 1)]
p = plot(change , ylabel = "difference" , xlabel = "number of timesteps" , label="relaxed" )
p = plot(p , change0 , ylabel = "difference" , xlabel = "number of timesteps" , label="original")
savefig(p , "images/relaxed-time-stability.svg")
#+end_src

#+Caption: Behavior of the relaxed and baseline solvers while solving the time interval \( t \in \left[ 0 , 10^{-2} \right] \) with increasing number of time-steps.
#+RESULTS: fig:relaxed-stability-in-time
[[file:images/relaxed-time-stability.svg]]

** Relaxed stability in space
we test convergence in space by succesivly subdividing our grid into finer meshes


#+name: fig:relaxed-stability-in-space
#+begin_src julia-vterm :results file graphics :file relaxed-space-stability.svg
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
df = jldopen("experiments/relaxed_space_refinement.jld2")["result"]
change = [norm(df[!, "phase"][i] .- restrict(df[! , "phase"][i-16] , G))/*(size(df[!,"phase"][i])...) for i=17:16:size(df , 1)]
p = plot([L"1024^2 \to 512^2" , L"512^2 \to 256^2" , L"256^2\to128^2" , L"128^2\to64^2" , L"64^2 \to32^2"],change , ylabel = "difference" , yscale=:log10, xlabel = "change in number of gridpoints" , label=L"\Delta \phi" , xscale=:log2 , seriestype=:scatter , xaxis=:flip , legend=:topright)
savefig(p , "images/relaxed-space-stability.svg")
#+end_src

#+RESULTS: fig:relaxed-stability-in-space
[[file:images/relaxed-space-stability.svg]]

* Comparison
In the previous chapter we have shown stability compared to the original solver. However we have not yet show a direct comparison between both methods. Since the relaxed solver is dependant on the relaxation variable \( \alpha \) We are interested in finding an optimal value for it.
Furthemore to see the effect \( \alpha \) has on our solver, we evaluate both solvers after one time-step , and then calculate the difference between \( \phi_{ij}^{n+1} \) and \( \phi_{ij}^{n+1,\alpha} \), for various values of \( \alpha \).
Should the relaxed solver approach the original, we would expect
\begin{equation}
||\phi_{ij}^{n+1} - \phi_{ij}^{n+1,\alpha}||_{Fr} \to 0
\end{equation}
In Fig.[[fig:alpha-error]] we observe the following behaviour where in all cases the difference to the original solver is apparent. Furthermore we observe a optimal value of \( \alpha \) at approximately \( 7.5 * 10^5 \) we explain this with our observations done for the Smoothing operator, where for small and large values of \( \alpha \) the relaxed approach ironically results in restricted behaviour. Empirical this is to be expected as. for large values of alpha the elliptical equation approaches \( \phi \)  and for small values the elliptical solver from chapter [[Elliptical PDE]] does not converge. 
#+name: fig:alpha-error
#+begin_src julia-vterm :results graphics file :file alpha-error.svg
using JLD2
using DataFrames
using Measures
<<init>>

pgfplotsx()
results = jldopen("experiments/alpha.jld2")["result"]
p=plot(results.alpha  , results.error ./64^2, label=false , xlabel=L"alpha $\alpha$" , ylabel="difference" )
savefig(p, "images/alpha-error.svg")
#+end_src

#+caption: Difference between the original solver \( \phi^1_{ij} \) and the relaxed solver \( \phi^{1,\alpha}_{ij} \)
#+RESULTS[cd5058dea24342ec78fa89393da40207fc1a5fab]: fig:alpha-error
[[file:images/alpha-error.svg]]
#+begin_src julia-vterm :results file graphics :file relaxed-comp.gif
<<init>>
using JLD2
using DataFrames
using Measures
using LaTeXStrings

gr()

results = jldopen("experiments/iteration.jld2")["result"]
results1 = jldopen("experiments/relaxed-iteration.jld2")["result"]
results2 = jldopen("experiments/relaxed-iteration-nophi.jld2")["result"]
results3 = jldopen("experiments/relaxed-iteration-nosubiter.jld2")["result"]
titles =  ["original" , "subiter elliptical" , L"without $2\phi$" , L"without $2\phi$ and subiter"]

anim = @animate for iter in zip(results.solver,results1.solver ,results2.solver , results3.solver)
    plots = []
    for (phase , title) in zip(iter ,titles)
        push!(plots , heatmap(phase.phase , title=title , legend=:none , aspectratio=:equal , grid=false , showaxis=false))
        plot(plots...)
        end
    end
gif(anim , "images/relaxed-comp.gif", fps=10)
#+end_src
although we can observe slight differences between the original solver and the relaxed approach they are barely noticeable by eye. Therefore we run our solver for a fixed value of \( \alpha=7700 \) , as this was one of the best values from Fig.[[fig:alpha-error]], We then  show the numerical difference between \( \phi_{ij}^n \) and \( \phi_{ij}^{n,\alpha} \) in Fig.[[fig:relaxed-original-comparison]]. We observe a a small difference between both methods, especially in areas with high curvature and inclusions of small segments of one phase in the other.
#+name: relaxed-comparison
#+begin_src julia-vterm :results file graphics :file relaxed-comparison.gif
<<init>>
using JLD2
using DataFrames
using Measures
using LaTeXStrings

i = 1*64 +1
gr()

original_results = jldopen("experiments/iteration.jld2")["result"]
relaxed_results = jldopen("experiments/relaxed-iteration.jld2")["result"]

difference = [norm(original.phase./2 .- relaxed.phase./2) /64^2 for (original, relaxed) in zip(original_results.solver[i:i+63],relaxed_results.solver[i:i+63])
]
anim = @animate for (original, relaxed) in zip(original_results.solver[i:i+63],relaxed_results.solver[i:i+63])
        p1 = plot(1:size(difference,1) , difference , xlabel= "time-steps" , ylabel = "error"  , title="diffrence" , label=false)
        p2 = heatmap(original.phase , title="original" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
        p3 = heatmap(relaxed.phase , title="relaxed" , aspectratio=:equal , grid=false , showaxis=false , legend=:none)
        plot(p1,p2,p3 , layout=(1,3) , size=(2000 ,700) , bottom_margin=20Plots.mm , left_margin=20Plots.mm)
        end
gif(anim , "images/relaxed-comparison.gif", fps=10)
#+end_src

#+RESULTS:
[[file:images/relaxed-comparison.gif]]
#+name: fig:relaxed-original-comparison
#+begin_src julia-vterm :results file graphics :file relaxed-comparison.svg
<<init>>
using JLD2
using DataFrames
using Measures
using LaTeXStrings

i = 0*64 +1
pgfplotsx()
original_results = jldopen("experiments/iteration.jld2")["result"]
relaxed_results = jldopen("experiments/relaxed-iteration.jld2")["result"]

difference = [norm(original.phase .- relaxed.phase) /64^2 for (original, relaxed) in zip(original_results.solver[i:i+63],relaxed_results.solver[i:i+63])
]
original, relaxed =   original_results.solver[i+63],relaxed_results.solver[i+63]

p1 = plot(1:size(difference,
                 1) ,
          difference ,
          xlabel= "time-steps" ,
          ylabel = "error"  ,
          title="diffrence" ,
          label=false)

p2 = heatmap(original.phase ,
             title=L"original at $n=64$" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(relaxed.phase ,
             title=L"relaxed at $n=64$" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false ,
             legend=:none)
p=plot(p1,
       p2,
       p3 ,
       layout=Layout3x1 ,
       size=size3x1 )
savefig(p , "images/relaxed-comparison.svg")
#+end_src

#+caption: Comparison between the original and the relaxed CH solvers.
#+RESULTS[2f8aad027135327452252bb081a2dd8f7b63d60e]: fig:relaxed-original-comparison
[[file:images/relaxed-comparison.svg]]


In addition to the experiments in Fig.[[fig:alpha-error]] we have experimented with a Monte Carlo Optimizer to optimize \( \alpha \) in conjunction with \( \varepsilon \), to best approximate the baseline solver after one time-step. This resulted in a optimal \( \varepsilon \) found that was very close to the actual \( \varepsilon \) used. (9e-3 compared to 8e-3). This gives us confidence that the relaxed method solves the same problem, as the baseline. Optimal values for \( \alpha \) varied , however stayed fairly large around \( 10^5 \to 10^{6} \).

#+begin_src julia :tangle src/optim.jl :noweb yes
using Distributions
using DataFrames
using JLD2
<<init>>

function test_values(alpha_distribution::Distribution , epsilon_distribution::Distribution , M)
    alpha = rand(alpha_distribution)
    eps = max(rand(epsilon_distribution)  ,1e-10)
    relaxed_solver = testgrid(relaxed_multi_solver, M, 2; alpha=alpha, epsilon=eps)
    set_xi_and_psi!(relaxed_solver[1])
    #SMOOTH!(relaxed_solver[1], 100, false)
    for j=1:64
    elyps_solver!(relaxed_solver[1], 2000)
    v_cycle!(relaxed_solver , 1)
    end
    error = norm(relaxed_solver[1].phase .- original_solver[1].phase) / *(size(relaxed_solver[1].phase)...)
    return (;alpha=alpha , epsilon=eps , error=error)
end

original_solver = testgrid(multi_solver, M, 2)
set_xi_and_psi!(original_solver[1])
for j=1:64
v_cycle!(original_solver , 1)
end
#SMOOTH!(original_solver[1], 100, false);
eps = 3e-3
#M = testdata(64, div(64,3), 64/5 , 2)
alpha0 = 10000
epsilon0 = 1e-2
best_alpha = alpha0 / 10
best_epsilon = epsilon0 / 10
best_error  = Inf
results = DataFrame()
for n=1:1000
    searchradius = 1
    alpha_distribution = Normal(best_alpha , searchradius * alpha0)
    epsilon_distribution = Normal(best_epsilon , searchradius * epsilon0)
    result = test_values(alpha_distribution , epsilon_distribution , M)
    if result.error < best_error
        global best_error = result.error
        global best_alpha = result.alpha
        global best_epsilon = result.epsilon
        println(result)
    end
push!(results , result)
end
jldsave("experiments/alpha-epsilon.jld2"; result=results)
println("Best alpha: $best_alpha , Best epsilon: $best_epsilon")
#+end_src
* Conclusion
In this thesis we have presented a simple introduction to the CH equation and have shown two numerical solvers for it.
We have presented a baseline method implemented from the authors [cite:@SHIN20117441], and have Shown how to derive it from their initial approach.
We have done the derivations in a way, that enables a simple adaptation to a modified version of the discrete CH equation Eq.[[eqref:eq:discrete-cahn-hilliard]], as introduced in [cite:@SHIN20117441].
We have introduced measures to evaluate both solvers in space , time and mass conservation as well as their sub-iteration behaviour.
We have shown the baseline to be mass conservative, in a numerical sense, and we have shown it to be stable in all tested measures.
We have shown our relaxed solver to approach the baseline, however we have also highlighted instability with subiterations, and mass-loss.
We intentionally didn't evaluate runtime since numerical experiments have shown both solvers to be dependant on the amount of sub-iterations, hyperparameters such as \( \varepsilon \) as well as the number off smoothing iterations.
It would therefore be unfair to evaluate one solver on a set of parameters tweaked for the other.
As example for this dilemma we recall runs where the relaxed solver was around 10x faster than the baseline with the same parameters.
The baseline solver was able to run with 10x less smoothing iterations than the relaxed one.
A fair comparison would hence require to find the optimal number of smoothing for each solver.


For the sake of completeness we include runtime benchmarks Of both methods. Those should be taken with a pinch of salt because of the reasons above. Both examples are run with the same parameters and  the results are in the Appendix.

** Outlook
This thesis leaves a lot of room for further research. We have already mentioned runtime evaluations, which require more optimizations, and additional experiments to test the number of smoothing iterations. Here it would be beneficial if both solvers are made adaptive, to ensure fair evaluations.
Furthermore, we initially considered a machine learning approach to replace the elliptical system. We didn't follow this idea mostly due to time constraints, as we had already collected trainings data during our numerical experiments. Our choice of programming language would have been of benefit here, as it would enable more advanced technices, such as integrating the numerical solver in the trainings loop since julia offers automatic diccerentiation of arbitrary functions, and therefore enables back-propagation (gradient descent) through the entire solver. Interessting would alo have been different discretizations of the relaxed CH equation, and different method for solving it, such as a finite volume or finite element method. Those bring the chalange of beeing harder to compare to our baseline.
* Appendix
** Operator implementation
#+begin_src julia :tangle src/utils.jl :eval never
function set_xi_and_psi!(solver::T) where T <: Union{multi_solver , relaxed_multi_solver}
    xi_init(x) = x / solver.dt
    psi_init(x) = solver.W_prime(x) - 2 * x
    solver.xi[2:end-1, 2:end-1] = xi_init.(solver.phase[2:end-1,2:end-1])
    solver.psi[2:end-1, 2:end-1] = psi_init.(solver.phase[2:end-1,2:end-1])
    return nothing
end
#+end_src
*** baseline
#+begin_src julia :tangle src/multisolver.jl :eval never
function L(solver::multi_solver,i,j , phi , mu)
    xi = solver.phase[i, j] / solver.dt -
         (discrete_G_weigted_neigbour_sum(i, j, solver.potential, G, solver.len, solver.width)
          -
          neighbours_in_domain(i, j, G, solver.len, solver.width) * mu )/solver.h^2
    psi = solver.epsilon^2/solver.h^2 *
          (discrete_G_weigted_neigbour_sum(i, j, solver.phase, G, solver.len, solver.width)
           -
           neighbours_in_domain(i, j, G, solver.len, solver.width) * phi) - 2 * phi + mu
    return [xi, psi]
end
#+end_src
#+begin_src julia :tangle src/multisolver.jl :eval never
function dL(solver::multi_solver , i , j)
    return [ (1/solver.dt) (1/solver.h^2*neighbours_in_domain(i,j,G,solver.len , solver.width));
             (-1*solver.epsilon^2/solver.h^2 * neighbours_in_domain(i,j,G,solver.len , solver.width) - 2) 1]
    end
#+end_src
*** relaxed
#+begin_src julia :tangle src/multi_relaxed.jl :eval never
function L(solver::relaxed_multi_solver,i,j , phi , mu)
    xi = solver.phase[i, j] / solver.dt -
         (discrete_G_weigted_neigbour_sum(i, j, solver.potential, G, solver.len, solver.width)
          -
          neighbours_in_domain(i, j, G, solver.len, solver.width) * mu )/solver.h^2
    psi = solver.epsilon^2 * solver.alpha*(solver.c[i,j] - phi) - solver.potential[i,j] - 2 * solver.phase[i,j]
    return [xi, psi]
end
#+end_src
#+begin_src julia :tangle src/multi_relaxed.jl :eval never
function dL(solver::relaxed_multi_solver , i , j)
    return [ (1/solver.dt) (1/solver.h^2*neighbours_in_domain(i,j,G,solver.len , solver.width));
             (-1*solver.epsilon^2 * solver.alpha  - 2) 1]
    end
#+end_src
** rng generation
for random point generation we use the folowing Function and seed.
#+begin_src julia-vterm :session jl :results table :exports both
using Random
rng = MersenneTwister(42)
gridsize = 64
radius = gridsize /5
blobs = gridsize ÷ 5
rngpoints = rand(rng,1:gridsize, 2, blobs)
#+end_src

#+RESULTS:
: 2×12 Matrix{Int64}:
:  48  40  20   1  63  49   8  60  26  58  26  11
:  17  13  56  52  15   9  30  14  40   9  40  25


the random testdata is then generated as follows
#+name: testdata
#+begin_src julia :eval never :tangle src/utils.jl :exports none
using Random
function testdata(gridsize , blobs , radius ,norm;rng=MersenneTwister(42))
rngpoints = rand(rng,1:gridsize, 2, blobs)
M = zeros(gridsize,gridsize) .- 1
for p in axes(rngpoints , 2)
    point = rngpoints[:, p]
    for I in CartesianIndices(M)
        if (LinearAlgebra.norm(point .- I.I  , norm) < radius)
            M[I] = 1
        end
    end
end
M
end
#+end_src
** Experiments :noexport:
*** iteration
#+begin_src julia :results output  :noweb yes :eval never :tangle experiments/src/iteration.jl
using JLD2
using DataFrames
using Random
<<init>>
<<setup-diverse-testgrids>>
function iter(g::Vector{T} , n) where T<: solver
    out = []
    for j in 1:64
    set_xi_and_psi!(g[1])
    for i = 1:64
        v_cycle!(g, 1)
    end
    push!(out, (solver=deepcopy(g[1]), iteration=j , experiment=n))
    end
    return out
end


tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/iteration.jld2"; result)
#+end_src

#+RESULTS:

#+name: fig:behaviour
#+begin_src julia-vterm :results graphics file :file behaviour.gif :chache :session jl :noweb no-export :output-dir images :exports none :noweb no-export
<<init>>
using JLD2
using DataFrames
results = jldopen("experiments/iteration.jld2")["result"]
n  = size(results.solver , 1)
pbar = ProgressBar(total = 10 * n)
energy = zeros(0)
massbalance = zeros(0)

anim = @animate for res in eachrow(results)
    push!(energy , bulk_energy(res.solver))
    push!(massbalance , massbal(res.solver.phase))

    p0 = heatmap(res.solver.phase , clim =(-1,1) , framestyle=:none , legend=true, lims=(1, size(res.solver.phase , 1)) , aspect_ratio=:equal, title  = "phasefield" )
   p1 = heatmap(res.solver.potential , framestyle=:none , legend=true, lims=(1,size(res.solver.phase , 1)), aspect_ratio=:equal, title  = "potential" )

    current_range = (res.experiment -1)*64 +1

    p3 = plot( 1:res.iteration, (massbalance .-massbalance[current_range])[current_range:current_range+res.iteration-1] , xlim=(1,64),  title = "Mass change")
    p2 = plot(1:res.iteration , energy[current_range:current_range+res.iteration-1], xlim=(1,64),  title = "Bulk energy")
    plot(p0,p1,p2,p3)
end
gif(anim , "images/behaviour.gif" , fps = 10)
#+end_src

#+caption: Behaviour of bulk energy \( E_{bulk} \) and amount of fluid changing phase, for different initial conditions
#+RESULTS: fig:behaviour
[[file:images/behaviour.gif]]

*** subiteration
#+begin_src julia :results output :noweb yes :tangle experiments/src/subiteration.jl
using DataFrames
using JLD2
using ProgressMeter
<<init>>
<<setup-diverse-testgrids>>
n = 4
m = 64

function iter(g::Vector{T} , n , k , prg::Progress) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        v_cycle!(g, 1)
        push!(out, (cycle=deepcopy(g[1]), iteration=j , subiteration=i , experiment=k))
        next!(prg)
    end
    end
    return out
end


tasks = []
prg=Progress(size(tests ,1)*n*m , showspeed=true , )
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i] , n , i , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/subiteration.jld2"; result)
#+end_src
*** time
#+begin_src julia :results output :noweb yes :exports none :tangle experiments/src/time.jl
using DataFrames
using JLD2
<<init>>
SIZE  =64
M = testdata(SIZE, SIZE ÷ 5, SIZE /5 , 2)
tests = [testgrid(multi_solver , M , 2 , dt = t ) for t in 1e-2./(1:64)]

function iter(g::Vector{T} , n) where T<: solver
    out = []
    for i = 1:n
    set_xi_and_psi!(g[1])
    for i = 1:64
        v_cycle!(g, 1)
    end
    push!(out, (phase=copy(g[1].phase), iteration=n))
    end
    return out
end


tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/time.jld2"; result)
#+end_src
*** space
#+begin_src julia :results output :noweb yes :exports none :tangle experiments/src/space.jl
using DataFrames
using JLD2
<<init>>

M = testdata(2^10 , 2^5 , 2^7 , 2 )
grids = testgrid(multi_solver  , M , 7)
# inits
for i=2:size(grids,1)
    restrict_solver!(grids[i-1] , grids[i])
end
tests = [[grids[i-1] , grids[i]] for i=2:size(grids,1)]


function iter(g::Vector{T} , n) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:64
        v_cycle!(g, 1)
    end
    push!(out, (phase=copy(g[1].phase), iteration=j))
    end
    return out
end


tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], 16)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/space_refinement.jld2"; result)
#+end_src
** Relaxed experiments :noexport:
*** Iteration
#+begin_src julia    :noweb no-export :tangle experiments/src/relaxed-iteration.jl :async
using JLD2
using DataFrames
using ProgressMeter
using Random
<<init>>
<<setup-diverse-testgrids>>

#tests = [testgrid(relaxed_multi_solver, M , 2;alpha=82000 , epsilon=0.009) for M in initial_data]
tests = [testgrid(relaxed_multi_solver, M , 2) for M in initial_data]

n = 64
m = 64


function iter(g::Vector{relaxed_multi_solver} , n , prg::Progress)
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        elyps_solver!(g[1] , 1000)
        v_cycle!(g, 1)
        next!(prg)
    end
    push!(out, (solver=deepcopy(g[1]), iteration=j , experiment=n))
    end
    return out
end

prg=Progress(size(tests ,1)*n*m , showspeed=true , )
tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/relaxed-iteration.jld2"; result)
#+end_src

*** Subiteration
#+begin_src julia :tangle experiments/src/relaxed-subiteration.jl :noweb yes
using DataFrames
using JLD2
using ProgressMeter
<<init>>
<<setup-diverse-testgrids>>
#tests = [testgrid(relaxed_multi_solver, M , 2;alpha=32428.2 , epsilon=0.163398) for M in initial_data]
tests = [testgrid(relaxed_multi_solver, M , 2) for M in initial_data]
n = 4
m = 1024

function iter(g::Vector{T} , n ,k , prg::Progress) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        elyps_solver!(g[1] , 1000)
        v_cycle!(g, 1)
        push!(out, (cycle=deepcopy(g[1]), iteration=j , subiteration=i , experiment=k))
        next!(prg)
    end
    end
    return out
end


tasks = []
prg=Progress(size(tests ,1)*n*m , showspeed=true , )
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i] , n , i , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/relaxed-subiteration.jld2"; result)
#+end_src

*** Time
#+begin_src julia :results output :noweb yes :exports none :tangle experiments/src/relaxed-tiem.jl
using DataFrames
using JLD2
<<init>>
tests = [testgrid(relaxed_multi_solver , M , 2 , dt = t ) for t in 1e-2./(1:64)]

function iter(g::Vector{T} , n) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:64
        elyps_solver!(g[1] , 1000)
        v_cycle!(g, 1)
    end
    end
    push!(out, (phase=copy(g[1].phase), iteration=n))
    return out
end


tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/relaxed-time.jld2"; result)
#+end_src
*** Space
#+begin_src julia :results output :noweb yes :exports none :tangle experiments/src/space.jl
using DataFrames
using JLD2
<<init>>

M = testdata(2^10 , 2^5 , 2^7 , 2 )
grids = testgrid(relaxed_multi_solver  , M , 7)
# inits
for i=2:size(grids,1)
    restrict_solver!(grids[i-1] , grids[i])
end
tests = [[grids[i-1] , grids[i]] for i=2:size(grids,1)]


function iter(g::Vector{T} , n) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    elyps_solver!(solver , 1000)
    for i = 1:64
        v_cycle!(g, 1)
    end
    push!(out, (phase=copy(g[1].phase), iteration=j))
    end
    return out
end


tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], 16)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/relaxed_space_refinement.jld2"; result)
#+end_src
*** alpha

#+begin_src julia :noweb no-export :eval never :tangle experiments/src/alpha.jl :exports results
<<init>>
using JLD2
using Distributed
using ProgressBars
using DataFrames

original_grid = testgrid(multi_solver, M, 2)
alphas = 0:1e4:2e6

function alpha_error(alpha::Number , solution::Array )
    test_solver  = testgrid(relaxed_multi_solver, M, 2, alpha=alpha)
    set_xi_and_psi!(test_solver[1])
    for j in 1:64
        elyps_solver!(test_solver[1], 1000)
        v_cycle!(test_solver , 1)
    end
return [(;alpha=alpha , error=norm(test_solver[1].phase - solution))]
end
set_xi_and_psi!(original_grid[1])
for j in 1:64
    v_cycle!(original_grid, 1)
end
print("finished original v_cycle")
tasks = []
for alpha in alphas
    t = Threads.@spawn alpha_error(alpha , original_grid[1].phase)
    push!(tasks , (alpha=alpha , task = t))
end
result = DataFrame()
for task in ProgressBar(tasks)
    append!(result , fetch(task.task) )
    end
jldsave("experiments/alpha.jld2"; result)
#+end_src
** bulk energy and mass balance
#+begin_src julia :tangle src/utils.jl :eval never
function bulk_energy(solver::T) where T <: Union{multi_solver , relaxed_multi_solver}
    energy = 0
    dx = CartesianIndex(1,0)
    dy = CartesianIndex(0,1)
    W(x) = 1/4 * (1-x^2)^2
    for I in CartesianIndices(solver.phase)[2:end-1,2:end-1]
        i,j = I.I
        energy += solver.epsilon^2 / 2 * G(i+ 0.5,j ,solver.len, solver.width) * (solver.phase[I+dx] - solver.phase[I])^2 + G(i,j+0.5,solver.len ,solver.width) * (solver.phase[I+dy] - solver.phase[I])^2 + W(solver.phase[I])
        end
   return energy
end
#+end_src

#+begin_src julia :tangle src/utils.jl
function massbal(arr)
    num_cells= *((size(arr).-2)...)
    return sum(arr[2:end-1, 2:end-1])/num_cells
    end
#+end_src

** runtime
#+begin_src julia-vterm :results value :eval never
using BenchmarkTools
<<init>>
<<setup-diverse-testgrids>>

relaxed_grid= testgrid(relaxed_multi_solver, M , 2)
baseline_grid = testgrid(multi_solver, M , 2)
function test(s::Array{multi_solver})
    set_xi_and_psi!(s[1])
    for i=1:1
        v_cycle!(s , 1)
    end
    end

function test(s::Array{relaxed_multi_solver})
    set_xi_and_psi!(s[1])
    for i=1:1
        elyps_solver!(s[1] , 1000)
        v_cycle!(s , 1)
    end
    end
# b1 = @benchmark test(baseline_grid)
#b2 = @benchmark test(relaxed_grid)
#b2
#+end_src

#+RESULTS[8eeb500671b68950e0d0d2ce8c4d70a92b0b2b9b]:
: Executing... 1a392143


#+begin_center
BenchmarkTools.Trial: 1 sample with 1 evaluation.
 Single result which took 8.938 s (3.95% GC) to evaluate,
 with a memory estimate of 3.36 GiB, over 63995963 allocations.
#+end_center

#+begin_center
BenchmarkTools.Trial: 5 samples with 1 evaluation.
 Range (min … max):  1.030 s …    1.304 s  ┊ GC (min … max): 3.26% … 2.74%
 Time  (median):     1.068 s               ┊ GC (median):    3.14%
 Time  (mean ± σ):   1.128 s ± 111.612 ms  ┊ GC (mean ± σ):  2.87% ± 0.34%

  █             ██                                            █                                                           █
  █▁▁▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁
  1.03 s         Histogram: frequency by time          1.3 s <

 Memory estimate: 293.88 MiB, allocs estimate: 5013565.
#+end_center
* Utility functions :noexport:
#+name: imports
#+begin_src julia :session jl :results silent :exports none
using Plots
using LinearAlgebra
#+end_src


#+begin_src julia :tangle src/utils.jl :eval never
###############################################################################
#                  Common Utility Functions For Multi Solvers                 #
###############################################################################
"""
restricts an array on the small grid to an array in the large grid asserts size arr=2^n + 2 and returns ret=2^(n-1) + 2

Returns
---------------------------
large grid array + padding
"""
function restrict(arr, G)
    shape = (size(arr) .- 2) .÷ 2
    ret = zeros(shape .+ 2)
    for I in CartesianIndices(ret)[2:end-1, 2:end-1]
        i, j = I.I
        g = [
            G(2 * i - 1, 2 * j - 1, (size(arr) .- 2)...),
            G(2 * i - 1, 2 * j, (size(arr) .- 2)...),
            G(2 * i, 2 * j - 1, (size(arr) .- 2)...),
            G(2 * i, 2 * j, (size(arr) .- 2)...)
        ]
        if sum(g) == 0
            ret[I] = 0
        else
            ret[I] = (
                1 / sum(g)
                ,*
                dot(g,
                    [
                        arr[2*i-1, 2*j-1],
                        arr[2*i-1, 2*j],
                        arr[2*i, 2*j-1],
                        arr[2*i, 2*j]
                    ]
                )
            )
        end
    end
    return ret
end

"""
    prolong(arr , G)

interpolates int a smaller grid by a factor of 2

"""
function prolong(arr, G)
    inner_shape = (size(arr) .- 2) .* 2
    ret = zeros(inner_shape .+ 2)
    ONE = oneunit(CartesianIndices(arr)[1])
    for I in CartesianIndices(arr)[2:end-1, 2:end-1]
        Ind = 2 * (I - ONE) + ONE
        for J in (Ind-ONE):Ind
            ret[J] = G(J.I..., inner_shape...) * arr[I]
        end
    end
    return ret
end
"""
    restrict!(smallgrid_solver::multi_solver , largegrid_solver::multi_solver)::multi_solver

------------
Requires
----------
smallgrid solver and largegid solvers to be multiple of 2 from each other bar padding eg. (66x66)->(34x34)

------------
Returns
------------
    nothing. mutatest largegid in place to represent the smallgrid

"""
function restrict_solver!(smallgrid_solver::T, largegrid_solver::T) where {T<:solver}
    copy!(largegrid_solver.phase, restrict(smallgrid_solver.phase, G))
    copy!(largegrid_solver.potential, restrict(smallgrid_solver.potential, G))
    return nothing
end
#+end_src
#+begin_src julia :tangle src/solvers.jl :eval never
abstract type solver end
struct multi_solver <: solver
    phase::Matrix{Float64}
    potential::Matrix{Float64}
    xi::Matrix{Float64}
    psi::Matrix{Float64}
    epsilon::Float64
    h::Float64
    dt::Float64
    W_prime::Function
    len::Int
    width::Int

end
struct relaxed_multi_solver <: solver
    phase::Matrix{Float64}
    potential::Matrix{Float64}
    xi::Matrix{Float64}
    psi::Matrix{Float64}
    c::Matrix{Float64}
    epsilon::Float64
    h::Float64
    dt::Float64
    W_prime::Function
    len::Int
    width::Int
    alpha::Float64

end
#+end_src
#+begin_src julia :tangle src/testgrids.jl :eval never
function W_prime(x)
    return -x * (1 - x^2)
end
function testgrid(::Type{multi_solver},M, len; dt = 1e-3 ,  epsilon=8e-3 , h0=3e-3)
    grid = Array{multi_solver}(undef, len)
    phase = zeros(size(M) .+ 2)
    phase[2:end-1, 2:end-1] = M


    for i = 1:len
        dims = size(M) .÷ 2^(i-1) .+ 2
        grid[i] = multi_solver(zeros(dims),
            zeros(dims),
            zeros(dims),
            zeros(dims),
            epsilon, h0 * 2^i, dt,
            W_prime,
            (dims .- 2)...)

    end
    copyto!(grid[1].phase, phase)
    return grid

end

function testgrid(::Type{relaxed_multi_solver},M, len ; alpha=1e6 , dt=1e-3, epsilon=8e-3 , h0=3e-3)
    grid = Array{relaxed_multi_solver}(undef, len)
    phase = zeros(size(M) .+ 2)
    phase[2:end-1, 2:end-1] = M

    for i = 1:len
        dims = size(M) .÷ 2^(i-1) .+ 2
        grid[i] = relaxed_multi_solver(zeros(dims),
            zeros(dims),
            zeros(dims),
            zeros(dims),
            zeros(dims),
            epsilon, h0 * 2^i, dt,
            W_prime,
            (dims .- 2)... ,
            alpha)

    end
    copyto!(grid[1].phase, phase)
    return grid
end


#+end_src

#+name: init
#+begin_src julia :eval never
include(pwd() * "/src/solvers.jl")
include(pwd() * "/src/adapted_solvers.jl")
include(pwd() * "/src/utils.jl")
include(pwd() * "/src/multisolver.jl")
include(pwd() * "/src/multi_relaxed.jl")
include(pwd() * "/src/testgrids.jl")
include(pwd() * "/src/elypssolver.jl")
using Plots
using LaTeXStrings
using LinearAlgebra
using Printf
using ProgressBars
default(fontfamily="computer modern" , titlefontsize=32 , guidefontsize=32 , tickfontsize = 22 )
pgfplotsx()
layout2x2 = grid(2,2)
layout3x1 = @layout [ b  c ; a]
size3x1 = (1600,1600)
SIZE = 64
M = testdata(SIZE, SIZE ÷ 5, SIZE /5 , 2)

#+end_src
#+name: setup-grid
#+begin_src julia :eval never :noweb yes
<<init>>
testgrd = testgrid(multi_solver,M, 2)
test_solver = testgrd[1]
#+end_src


#+name: setup-relaxed-grid
#+begin_src julia :eval never :noweb yes
<<init>>
testgrd = testgrid(relaxed_multi_solver,M, 2)
println("Hi")
solver = testgrd[1]
#+end_src

#+name: setup-comparison
#+begin_src julia :noweb yes
<<init>>
using Plots
using LinearAlgebra
using ProgressBars
using JLD2
M = jldopen("data/test-phasefield.jld2")["M"]

relaxed_grid1 = testgrid(relaxed_multi_solver, M, 2 ,alpha=1e3)
relaxed_grid2 = testgrid(relaxed_multi_solver, M, 2 , alpha=1e4)
relaxed_grid3 = testgrid(relaxed_multi_solver, M, 2 , alpha=1e5)
original_grid = testgrid(multi_solver, M, 2)

#+end_src

#+name: setup-diverse-testgrids
#+begin_src julia :noweb yes
incirc(M) = filter(x -> norm(x.I .- (size(M, 1) / 2, size(M, 2) / 2)) < min(size(M)...) / 3, CartesianIndices(M))
insquare(M) = filter(x -> norm(x.I .- (size(M, 1) / 2, size(M, 2) / 2), Inf) < min(size(M)...) / 4, CartesianIndices(M))
side(M) = filter(x -> x.I[2] < size(M, 2) ÷ 2, CartesianIndices(M))
halfcirc(M) = filter(x -> norm(x.I .- (1, size(M, 2) / 2), 2) < min(size(M)...) / 3, CartesianIndices(M))

function get_special_input(fn, size)
    M = fill(-1, size , size )
    M[fn(M)] .= 1
    return M
end
SIZE  =64
t1= [testdata(SIZE, SIZE ÷ 5, SIZE /5 , j) for j in [1,2, Inf]]
t2 = [get_special_input(fn,SIZE) for  fn in [halfcirc , incirc, side , insquare]]
initial_data = [t1 ; t2]
tests = [testgrid(multi_solver, M , 2) for M in initial_data]

#+end_src








* References :ignore:
#+PRINT_BIBLIOGRAPHY:
#  LocalWords:  Discretization

* Footnotes
[fn:1] This solver uses a two dimensional version with 2 second order terms instead of the full fourth order  equation.

[fn:2] Julia provides iteration utilities over n dimensional matricies. Therefore it would technically be possible to write dimension agnostic algorithms. While we used some of this functionality, we did not implement a full n-dimensional algorithm, and only provide a 2D implementation
# Local Variables:
# mode: org
# org-export-allow-bind-keywords: t
# End:
