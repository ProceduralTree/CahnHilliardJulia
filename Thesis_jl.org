#+title: A Multi-grid method
#+subtitle: on the Cahn-Hilliard equation  and its relaxed variation.
#+BIBLIOGRAPHY: ~/org/resources/bibliography/refs.bib
#+options: toc:nil
#+BIND: org-latex-title-command ""
#+BIND: org-latex-default-figure-position "H"
#+latex_class: mimosis
  #+latex_header: \include{~/.doom.d/OrgConfig/noteHeader.tex}
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
#+PROPERTY: header-args:julia :output-dir images :eval never :noweb no-export
#+PROPERTY: header-args:julia-vterm :output-dir images :exports results :noweb no-export :eval yes :session jl :cache no :eval never
# #+INFOJS_OPT: view:info toc:nil
#+latex_header: \renewcommand{\floatpagefraction}{.9}%
#+latex_header: \usepackage[level]{datetime}
* Title-page :ignore:
#+begin_export latex
\frontmatter
\makeatletter
\begin{titlepage}
    \centering
\includegraphics[width=1\textwidth]{logo/logo.png}
\par
	\vspace{1.5cm}
	{\scshape\huge Bachelor's Thesis \par}
	\vspace{1.5cm}
	{\Huge\bfseries  \@title \par}
	\vspace{2cm}
	{\LARGE \@author \par}
	{\Large Matriculation Number: 3545737 \par}
	\vspace{1.5cm}
	{\large Supervisor: Prof. Dr. Christian Rohde\par}
	\vspace{1.5cm}
	{\large Institute of Applied Analysis and Numerical Simulation\par}



	\vfill

% Bottom of the page
	{\large Completed 21.08.2024 \par}
\end{titlepage}
\makeatother

#+end_export



#+begin_abstract
This thesis gives a short overview on the classical Cahn-Hilliard equation, based on the authors [cite:@Wu_2022], and introduces an elliptical relaxation approach. It uses the discretization, and solver by the authors [cite:@SHIN20117441] as baseline, and uses those to derive discretization and solver for the elliptical relaxation. We proof mass conservation for the Cahn-Hilliard equation, the relaxed equation, and a discrete version for both discretizations. However this thesis does not show discrete mass conservation for the solvers. We introduce evaluation metrics regarding stability and global energy, and shows that both solvers are to be stable and decay in energy. Furthermore, we show a qualitative success of the elliptical solver, however it also highlights the relaxed solvers failure to conserve mass. Additionally this thesis discusses the selection of a relaxation hyper-parameter \( \alpha \).
#+end_abstract
#+TOC: headlines 3
#+begin_export latex
\mainmatter
#+end_export
* Introduction
The Cahn-Hilliard (CH) equation is a well known fourth order partial differential equation (PDE) used in multi-phase flow. It is used to couple different phases in a fluid with a diffuse-interface. It does this using a continuous transition between two phases.
The CH equation, beeing fourth order, is difficult to solve numerically. However it is still used since it is able to guarantee conservation of mass.
In this thesis we implement numerical solvers for the Cahn-Hilliard equation in the Julia programming language.
We begin by giving an overview and a derivation for the analytical CH equation in Chapter [[The Cahn-Hilliard equation]]. We then show mass conservation and a decrease of total energy in time.
The Chapter [[Discretization of the CH equation]] introduces our finite difference discretization. We explain the necessary functions, and give their implementation. Additionally we introduce the initial conditions we used in this thesis.
 Chapter [[Numerical solver]] describes the relevant steps of our numerical implementation and the two-grid method we used.
In Chapter [[Numerical experiments]] we evaluate this method's stability,discrete mass conservation and discrete energy decrease that we have shown continuously for the analytical CH equation.
Our thesis introduces a elliptical relaxation approach to the classical CH equation, where instead of solving a fourth order PDE [fn:1], we solver a second order relaxed PDE and an additional elliptical PDE. In the chapter [[Relaxed Cahn-Hilliard equation]] we introduce this approach, and then derive a numerical solver using the method described in chapter [[Two-grid method]]. Hereupon we derive and implement the necessary functions for the discretized relaxed equation, and  we introduce a simple solver for the elliptical PDE.
Subsequently, in chapter [[Numerical experiments for the relaxed system]], we evaluate our relaxed method against the baseline with the same measures, as introduced in chapter [[Numerical experiments]].

We began writing this thesis with a reproducible research philosophy in mind. To do so we provide a single source of truth, i.e. a file (~Thesis_jl.org~) that includes explanations and mathematical formulae, accompanied with their numerical implementation, the implementation for our plots as well as the code for our numerical experiments. For convenience we also provide our Thesis in PDF format with some of the irrelevant code removed. Furthermore we provide our implementation and experiments in separate source directories for ease of execution. Those where directly exported from ~Thesis_jl.org~.  All shown code is therefore the code that is run on our machine. Since not all parts of the code are relevant for understanding, unimportant sections are implemented elsewhere. Didactically they are replaced with a comment of form =<<unimportant-code-section>>=. Their implementation can be found in ~Thesis_jl.org~ in a code block of the same name.
We did experiment with additional tools such as [[https:orgmode.org][org-mode]] that allow for scientific note-taking and literate programming. Sadly this format relies heavily on the Emacs text editor and requires some setup to get working. Therefore, we fall short of making the entire Thesis as accessible as possible.
~Thesis_jl.org~ is available on our github repository at [[https://github.com/ProceduralTree/CahnHilliardJulia.git]].
The code to our numerical experiments we provide in the appendix as well as under ~experiments/src/~ in the repository. We provide the code for the plots next to them in ~Thesis_jl.org~. The code for the numerical solvers themselves is in both the PDF and the ORG Document, and gets exported into the ~src/~ directory of the github repository.
* The Cahn-Hilliard equation
The Cahn-Hilliard (CH) equation is a partial differential equation (PDE) that describes the dynamics of a two-phase fluid [cite:@Wu_2022]. For this thesis we consider the following formulation of the CH equation  in a given domain \( \Omega \times (0, T) \,, \Omega \subset \mathbb{R}^d \,, d \in \mathbb{N}  \,, T>0 \),
#+name: eq:CH
\begin{equation}
\begin{aligned}
\partial_{t}\phi(x,t) &=  \nabla \cdot(M(\phi)\nabla\mu), \\
\mu &= - \varepsilon^2 \Delta\phi  + W'(\phi),
\end{aligned}
\end{equation}
where respectively, the variables \( \phi , \mu : \Omega \times (0,T) \to \mathbb{R}^d \) are phase-field variable and chemical potential,
\(\varepsilon\) is a positive constant correlated with interface thickness, \( W(\phi) \) is a double well potential and \(M(\phi) > 0\) is a mobility coefficient [cite:@Wu_2022].
 \( \phi\) is defined in an interval \(I=[-1,1] \) and  represent the different phases.
\begin{align*}
\phi &=
\begin{cases}
1 &\,, \phi \in \text{phase 1} \\
-1 &\,, \phi \in\text{phase 2}
\end{cases}
\end{align*}

 In this thesis we assume \(M(\phi) \equiv 1 \), simplifying the CH equation.

The advantages of the CH approach, as compared to traditional boundary coupling, are for example: "explicit tracking of the interface" [cite:@Wu_2022], as well as "evolution of complex geometries and topological changes [...] in a natural way" [cite:@Wu_2022].
** Physical derivation of the CH equation [[eqref:eq:CH]]
*** The free energy
The authors in [cite:@Wu_2022] define the CH equation using the *Ginzburg-Landau* free energy equation:
#+name: eq:energy
\begin{align}
E^{\text{bulk}}[\phi] &= \int_{\Omega} \frac{\varepsilon^2}{2} |\nabla \phi |^2 + W(\phi) \, dx ,
\end{align}
where \(W(\phi) \) denotes the Helmholtz free energy density of mixing [cite:@Wu_2022] that we approximate it in further calculations with \(W(\phi) = \frac{(1-\phi ^2)^2}{4}\) as in [cite:@SHIN20117441] shown in Fig. [[fig:double-well]].
#+name: fig:double-well
#+begin_src julia-vterm :results file graphics :file double-well.svg
using Plots
using LaTeXStrings
W(x) = 1/4 * (1- x^ 2)^2
p = plot(W , xlims=(-2,2) , label=:none)
savefig(p, "images/double-well.svg")
#+end_src

#+caption: Double well potential \( W(\phi) \)
#+RESULTS[990bafb41c1855db23a8eb8b6bc4129e91d73342]: fig:double-well
[[file:images/double-well.svg]]

The chemical potential, \( \mu \), then follows as the variational derivation of the free energy in [[eqref:eq:energy]].
#+name: eq:chemical-potential
\begin{align}
 \mu &= \frac{\delta E_{bulk}(\phi)}{\delta \phi} = -\varepsilon^2 \Delta \phi + W'(\phi)
\end{align}

*** Derivation of the CH equation from mass balance
The paper [cite:@Wu_2022] states that the observable phase separation is driven by a diffusion resulting from the gradient in chemical potential \( \mu \). The emergent conservative dynamics motivate the following diffusion equation
#+name: eq:massbal
\begin{equation}
    \partial_t \phi + \nabla \cdot \mathbf{J} = 0,
\end{equation}
where \( \mathbf{J} = -\nabla \mu \) represents mass-flux.
We follow the authors [cite:@Wu_2022] in deriving the CH equation by combining eqref:eq:chemical-potential and [[eqref:eq:massbal]].
\begin{equation}
\begin{aligned}
\implies \partial_t \phi   &=- \nabla \cdot \mathbf{J} = \Delta\mu , \\
\mu &=  -\varepsilon^2 \Delta \phi + W'(\phi) \,,
\end{aligned}
\end{equation}
Furthermore the CH equation is mass conservative under homogeneous Neumann boundary conditions, defined as:
#+name: eq:boundary-conditions
\begin{equation}
\begin{aligned}
\mathbf{J} \cdot \mathbf{n} &= 0 & \text{on} \, \partial\Omega &\times (0,T),\\
\partial_n\phi &= 0 & \text{on} \, \partial\Omega &\times (0,T),
\end{aligned}
\end{equation}
where \( \mathbf{n}  \) is the outward normal on \( \partial \Omega \).
To show the conservation of mass we analyze the change in total mass in the domain \( \Omega \) over time.
#+name: eq:mass-conservation
\begin{equation}
\begin{aligned}
\frac{d}{dt}\int_{\Omega}\phi \ d \mathbf{x} &=\int_{\Omega}\frac{\partial \phi}{\partial t} \ d\mathbf{x} \\
&= - \int_{\Omega} \nabla \cdot \mathbf{J} \ d\mathbf{x}\\
&=  \int_{\partial\Omega}  \mathbf{J} \cdot \mathbf{n}  \ d\mathbf{s} \\
&= 0 & \forall t\in(0,T)\,,
\end{aligned}
\end{equation}

In order to show thermodynamic consistency of the CH equation, we take the time derivation of the free energy functional [[eqref:eq:energy]].
\begin{align*}
\frac{d}{dt}E^{bulk}[\phi(t)] &= \int_{\Omega} ( \varepsilon^2 \nabla \phi \cdot \nabla \partial_t \phi + W'(\phi) \partial_t \phi) \ d \mathbf{x} \\
&=\int_{\Omega} (\varepsilon^2\nabla\phi + W'(\phi))\partial_t\phi \ d\mathbf{x}\\
&=\int_{\Omega} \mu \partial_t \phi \ d\mathbf{x}\\
&= \int_{\Omega} \mu \cdot \Delta\mu \ d\mathbf{x} \\
&= -\int_{\Omega} \nabla\mu \cdot \nabla\mu \ dx + \int_{\partial\Omega} \mu \nabla\phi_t \cdot \mathbf{n} \ dS \\
&\stackrel{\partial_n\phi = 0}{=} - \int_{ \Omega } |\nabla \mu|^2 \ d \mathbf{x}, & \forall t \in (0,T)
\end{align*}
This a bounded \( L_2 \) norm  on \( \Omega \times (0,T)\) of \( \nabla \mu \).
** initial value-boundary problem
The aim of the CH equation is then to find solutions \( \phi(\vec{x} , t) , \mu(\vec{x} , t): \Omega \times (0,T) \to \mathbb{R} \) such that they satisfy
#+name: eq:initial-value-problem
\begin{equation}
\begin{aligned}
\partial_{t}\phi(x,t) &=  \nabla \cdot(M(\phi)\nabla\mu),\\
\mu &= - \varepsilon^2 \Delta\phi  + W'(\phi), & \text{in} \, \Omega &\times (0,T),\\
-\nabla\mu \cdot \mathbf{n} &= 0\\
\nabla\phi \cdot \mathbf{n} &= 0 & \text{on} \, \partial\Omega &\times (0,T), \\
\phi(x,0) &= \phi^0(x) \,, & \text{in} \, \Omega &
\end{aligned}
\end{equation}
* Discretization of the CH equation
This thesis uses the methods described by [cite:@SHIN20117441] to discretize eqref:eq:initial-value-problem. The method used by them is semi-implicit in time and implicit in space.
** The discretization of functions and derivative operators
As baseline for numerical experiments we use a two-grid method based on the finite difference method defined in [cite:@SHIN20117441].
We discretize our domain \( \Omega \) to be a Cartesian-grid \( \Omega_d \) on a square with side-length \( N\cdot h \), where N is the number of grid-points in one direction, and \( h \) is the distance between grid-points. In all our initial data \( h \) is \( 3\cdot10^{-3}\) and \( N=64 \). However, for stability tests we change \( h \) and \( N \).
 The discrete version or our domain is
\begin{equation}
\Omega_d = \left\{ i,j \mid i,j \in \mathbb{N} \,, i,j \in [2,N+1] \right\},
\end{equation}
as shown in ref:fig:discrete-domain
#+name: fig:discrete-domain
#+begin_src julia-vterm :results file graphics :file domain.svg
using Plots
using LaTeXStrings
pgfplotsx()
Idx = CartesianIndex(1,1)
M = zeros(66,66)
M[2:end-1 , 2:end-1] = ones(64,64)
p= heatmap(M, title=L"\Omega_d" , clim=(0,1),
            gridlinewidth=2 , axis_equal_image=true , extra_kwargs=:subplot , xlims=(1 ,66) , ylims=(1,66))

savefig(p,"images/domain.svg")
#+end_src

#+caption: Discrete Domain used in this Thesis. 1 is inside and 0 outside of the Domain.
#+RESULTS[46038739234db0a64b145e68000e9b1ea9d30425]: fig:discrete-domain
[[file:images/domain.svg]]


We discretize the phase-field ,\( \phi \), and chemical potential ,\( \mu \), into grid-wise functions \(\phi_{ij}, \mu_{ij} \) such that
\begin{equation}
\begin{aligned}
\phi_{ij}^n: \Omega_d \times \left\{ 0, \dots  \right\} &\to \mathbb{R} \,,\\
\mu_{ij}^n: \Omega_d \times \left\{ 0, \dots \right\} &\to \mathbb{R} \,,
\end{aligned}
\end{equation}
Here \( n \) denotes the n^{th} time-step, and \( (i,j) \) are Cartesian indices on the discrete domain \( \Omega_d \).
The authors in [cite:@SHIN20117441] then use the characteristic function \( G \) of the  domain \( \Omega \) to enforce no-flux boundary conditions [[eqref:eq:boundary-conditions]].

\begin{align*}
G(x,y) &=
\begin{cases}
1, & (x,y) \in  \Omega \\
0, & (x,y) \not\in  \Omega
\end{cases}
\end{align*}
We implement the discrete version of \( G \) on \( \Omega_d \) as follows:
\begin{align*}
G_{ij} &=
\begin{cases}
1, & i,j \in [2,N+1]  \\
0, & \text{else}
\end{cases}
\end{align*}
The definition of \( G_{ij} \) with \( i,j \in [2,N+1] \) enables us to evaluate \( G_{ij} \) with non integer values.
#+begin_src julia :tangle src/utils.jl :eval never :exports none
"""
Boundry indicator function

Returns
---------------
1 if index i,j is in bounds(without padding) and 0 else
"""
#+end_src
#+begin_src julia :tangle src/utils.jl :eval never
function G(i, j, len, width)
    if 2 <= i <= len + 1 && 2 <= j <= width + 1
        return 1.0
    else
        return 0.0
    end
end
#+end_src

We then define the discrete derivatives \( D_x\phi_{ij}, \ D_y\phi_{ij} \) using finite differences:
\begin{align}
D_x\phi^{n+1,m}_{i+\frac{1}{2} j} &= \frac{\phi^{n+1,m}_{i+1j} - \phi^{n+1,m}_{ij}}{h} & D_y\phi^{n+1,m}_{ij+\frac{1}{2}} &= \frac{\phi^{n+1,m}_{ij+1} - \phi^{n+1,m}_{ij}}{h}
\end{align}
We define \( D_x\mu_{ij}^{n+\frac{1}{2},m} , D_y\mu_{ij}^{n+\frac{1}{2},m} \) in the same way.
Next we define the discrete gradient \( \nabla_d \phi^{n+1,m}_{ij}\), as well as a modified Laplacian \( \nabla_d \cdot (G_{ij} \nabla_d \phi^{n+1,m}_{ij} )\):



#+name: eq:discretization
\begin{equation}
\begin{aligned}
\nabla_d \phi^{n+1,m}_{ij} &= \left(D_x \phi^{n+1,m}_{i+1j} , \ D_y \phi^{n+1,m}_{ij+1}\right) \,,\\
 \nabla_d \cdot (G_{ij} \nabla_d \phi^{n+1,m}_{ij}) &= \frac{G_{i+\frac{1}{2}j}D_x \phi^{n+1,m}_{i+\frac{1}{2}j} -  G_{i-\frac{1}{2}}D_x \phi^{n+1,m}_{i-\frac{1}{2}j} + D_y \phi^{n+1,m}_{ij+\frac{1}{2}} - D_y \phi^{n+1,m}_{ij-\frac{1}{2}}}{h} \\
  &= \frac{ G_{i+\frac{1}{2}j} \phi^{n + 1,m}_{i+1j} +  G_{i-\frac{1}{2}j} \phi^{n +,m}_{i-1j} + G_{ij+\frac{1}{2}}  \phi^{n +,m}_{ij+1} + G_{ij-\frac{1}{2}} \phi^{n +,m}_{ij-1}    }{h^2}\\
& \, - \frac{\left(   G_{i+\frac{1}{2}j} + G_{i-\frac{1}{2}j} + G_{ij+\frac{1}{2}} + G_{ij-\frac{1}{2}} \right) \cdot \phi_{ij} }{h^2} \,,
\end{aligned}
\end{equation}
The discretization for \(  \nabla_d\mu_{ij}^{n+\frac{1}{2},m} ,  \nabla_d \cdot (G_{ij} \nabla_d \mu^{n+\frac{1}{2},m}_{ij}) \) are done the same as for \( \phi_{ij}^{n+1} \).
 We define \(   \nabla_d \cdot (G_{ij} \nabla_d \phi_{ij} )\) instead of a discrete Laplacian \( \Delta_d \) to ensure a discrete version of boundary conditions [[eqref:eq:boundary-conditions]].
 The authors in [cite:@SHIN20117441] show this to be the case by expanding \( \nabla_d \cdot (G_{ij} \nabla_d\phi_{ij}) \).
Notably, when one point lies outside the domain, e.g. \( G_{i + \frac{1}{2}} = 0 \)  then the corresponding discrete gradient \( \frac{\phi_{i+1}^{n+1} - \phi_i}{h}  \) is weighted by 0. This corresponds the discrete version of \( \partial_n\phi = 0 \) [cite:@SHIN20117441].

To simplify the notation for discretized derivatives we use the following abbreviations:
- \(  \Sigma_G \phi_{ij} = G_{i+\frac{1}{2}j} \phi^{n + 1,m}_{i+1j} +  G_{i-\frac{1}{2}j} \phi^{n +1,m}_{i-1j} + G_{ij+\frac{1}{2}}  \phi^{n +1,m}_{ij+1} + G_{ij-\frac{1}{2}} \phi^{n +1,m}_{ij-1}  \)
- \(  \Sigma_{Gij} = G_{i+\frac{1}{2}j} + G_{i-\frac{1}{2}j} + G_{ij+\frac{1}{2}} + G_{ij-\frac{1}{2}}  \)
The Code for those abreviations is:
#+begin_src julia :tangle src/utils.jl :eval never
function neighbours_in_domain(i, j, G, len, width)
    (
        G(i + 0.5, j, len, width)
        + G(i - 0.5, j, len, width)
        + G(i, j + 0.5, len, width)
        + G(i, j - 0.5, len, width)
    )

end
function discrete_G_weigted_neigbour_sum(i, j, arr, G, len, width)
    (
        G(i + 0.5, j, len, width) * arr[i+1, j]
        + G(i - 0.5, j, len, width) * arr[i-1, j]
        + G(i, j + 0.5, len, width) * arr[i, j+1]
        + G(i, j - 0.5, len, width) * arr[i, j-1]
    )
end
#+end_src

We can then write the modified Laplacian \( \nabla_d \cdot (G \nabla_d\phi_{ij}^{n+1}) \) as:
#+name: eq:modified-laplacian
\begin{align}
\nabla_{d} \cdot(G \nabla_d\phi_{ij}^{n+1}) &= \frac{\Sigma_G\phi_{ij}^{n+1} - \Sigma_{Gij}\cdot \phi_{ij}^{n+1}}{h^2}
\end{align}
We use this modified Laplacian to deal with boundary conditions. Our abbreviations simplify separating implicit and explicit terms in the discretization.
** Initial data
For testing of our numerical solver for eqref:eq:initial-value-problem  we use initial discrete phase-fields defined by the following equations:

\begin{equation}
\begin{aligned}
\phi_{ij}^{0} &=
\begin{cases}
1 &\,, \|(i,j) - (\frac{N}{2} , \frac{N}{2})\|_p < \frac{N}{3}\\
-1 &\,,else
\end{cases}
&
\text{where    }  p \in \{2,\infty\}
\\
\phi_{ij}^0 &=
\begin{cases}
1 &\,,  i < \frac{N}{2} \\
-1 &\,,else
\end{cases}
\\
\phi_{ij}^0 &=
\begin{cases}
1 &\,, \|(i,j) - (\frac{N}{2} , 2)\|_2 < \frac{N}{3} \\
-1 &\,,else
\end{cases}
\\
\phi_{ij}^0 &=
\begin{cases}
1 &\,, \| (i,j) - q_k \|_p < \frac{N}{5}  \\
-1 &\,,else
\end{cases}
& p \in \{1,2, \infty\} , q_k \in Q
\end{aligned}
\end{equation}
where \( q_k \) are random points the given domain. We generate those using the following RNG setup in Julia
#+begin_src julia  :exports both :eval t :results output
using Random
rng = MersenneTwister(42)
gridsize = 64
radius = gridsize /5
blobs = gridsize ÷ 5
rngpoints = rand(rng,1:gridsize, 2, blobs)
#+end_src

#+RESULTS:
: MersenneTwister(42)
: 64
: 12.8
: 12
: 2×12 Matrix{Int64}:
:  48  40  20   1  63  49   8  60  26  58  26  11
:  17  13  56  52  15   9  30  14  40   9  40  25




#+name: fig:testinput
#+begin_src julia-vterm :results file graphics  :file testdata.svg
<<init>>
<<setup-diverse-testgrids>>
gr()
plots =[  heatmap(t[1].phase ,  legend=:none , aspectratio=:equal , grid=false , showaxis=false , size=(600,600))
for t in tests[1:2:end]]
#plots = [heatmap(t[1].phase , size=(600,600), axis=:none , aspect_ratio=:equal) for t in tests]
p = plot(plots... , layout=(1,4) , size=(2400,600))
savefig(p,"images/testdata.svg")
#+end_src

#+caption: Examples of different phase-fields used as the initial condition.
#+RESULTS[96c75eb7f0e23571539c681b9fffaef648de96d5]: fig:testinput
[[file:images/testdata.svg]]

**  Discretization into a linear system
The authors in [cite:@SHIN20117441] then define the discrete CH equation adapted for the domain as:
#+name: eq:discrete-cahn-hilliard
\begin{equation}
\begin{aligned}
\frac{\phi_{ij}^{n+1} - \phi_{ij}^n}{\Delta t}  &=  \nabla _d \cdot (G_{ij} \nabla_d \mu_{ij}^{n+\frac{1}{2}} )  \,, \\
 \mu_{ij}^{n+\frac{1}{2}} &= 2\phi_{ij}^{n+1} - \varepsilon^2  \nabla_d \cdot  (G_{ij} \nabla _d \phi_{ij}^{n+1} ) + W'(\phi_{ij}^n) - 2\phi _{ij}^n \,,
\end{aligned}
\end{equation}
and derive a numerical scheme from this equation.
This method is semi-implicit in time, and consists of a centered difference in space.
The authors in [cite:@SHIN20117441] derive their method by separating [[eqref:eq:discrete-cahn-hilliard]] into implicit and linear terms, and explicit non-linear terms. We write the implicit terms in form of a function \( L: \RR^2 \to \RR^2  \) and the explicit terms in \( (\zeta^n_{ij} , \psi^n_{ij})^T \). We define \( L \) as:
\begin{align*}
L
\begin{pmatrix}
\phi^{n+1}_{ij} \\
\mu^{n+\frac{1}{2}}_{ij}
\end{pmatrix}
&:=
\begin{pmatrix}
\frac{\phi^{n+1}_{ij}}{\Delta t} - \nabla _d \cdot  ( G_{ij} \nabla _d \mu^{n+\frac{1}{2}}_{ij} ) \\
\varepsilon^2 \nabla _d \cdot  (G \nabla_d \phi_{ij}^{n+1}) - 2\phi_{ij}^{n+1} + \mu_{ij}^{n+\frac{1}{2}}
\end{pmatrix}
& \forall i,j&\in\{2, \dots , N+1\}.
\end{align*}
#+begin_src julia :tangle src/multisolver.jl :eval never
function L(solver::multi_solver,i,j , phi , mu)
    xi = solver.phase[i, j] / solver.dt -
         (discrete_G_weigted_neigbour_sum(i, j, solver.potential, G, solver.len, solver.width)
          -
          neighbours_in_domain(i, j, G, solver.len, solver.width) * mu )/solver.h^2
    psi = solver.epsilon^2/solver.h^2 *
          (discrete_G_weigted_neigbour_sum(i, j, solver.phase, G, solver.len, solver.width)
           -
           neighbours_in_domain(i, j, G, solver.len, solver.width) * phi) - 2 * phi + mu
    return [xi, psi]
end
#+end_src
This function follows from [[eqref:eq:discrete-cahn-hilliard]] and is linear in the unknowns \( \left(\phi^{n+1}_{ij} , \mu^{n+\frac{1}{2}}_{ij} \right) \). The non-linear terms of eqref:eq:discrete-cahn-hilliard are aggregated in \( \left(\zeta^n_{ij}, \psi^n_{ij} \right) \), which we define as
\begin{align}
\begin{pmatrix}
\zeta^n_{ij}
 \\
\psi^n_{ij}
\end{pmatrix}
&:=
\begin{pmatrix}
\frac{\phi_{ij}^{n}}{\Delta t}\\
W'(\phi_{ij}^n) - 2\phi_{ij}^n
\end{pmatrix}
& \forall i,j&\in\{2, \dots , N+1\}.
\end{align}
#+begin_src julia :tangle src/utils.jl :eval never
function set_xi_and_psi!(solver::T) where T <: Union{multi_solver , relaxed_multi_solver}
    xi_init(x) = x / solver.dt
    psi_init(x) = solver.W_prime(x) - 2 * x
    solver.xi[2:end-1, 2:end-1] = xi_init.(solver.phase[2:end-1,2:end-1])
    solver.psi[2:end-1, 2:end-1] = psi_init.(solver.phase[2:end-1,2:end-1])
    return nothing
end
#+end_src
The authors [cite:@SHIN20117441] defined a numerical method where all non linear terms are evaluated explicitly. Therefore , we know everything needed to calculate \( (\zeta^n_{ij} , \psi^n_{ij})^T \) at the beginning of each time step. We compute those values once and store them in the solver.
Using \(  \left(\zeta^n_{ij}, \psi^n_{ij} \right)  \) and  \(   L\left(\phi^{n+1}_{ij} , \mu^{n+\frac{1}{2}}_{ij} \right) \) , we can rewrite eqref:eq:discrete-cahn-hilliard as
#+name: eq:LES
\begin{equation}
\begin{aligned}
L
\begin{pmatrix}
\phi^{n+1}_{ij} \\
\mu^{n+\frac{1}{2}}_{ij}
\end{pmatrix}
&=
\begin{pmatrix}
\zeta^n_{ij} \\
\psi^n_{ij}
\end{pmatrix}
.
& \forall i,j \in \{1, \dots , N\}
\end{aligned}
\end{equation}

This Linear system consists of NxN, 2 dimensional linear equations.
Each equation in the linear system  eqref:eq:LES can be rewriten in the form \(\operatorname{\mathbf{DL}}_{ij} \cdot \left( \phi^{n+1}_{ij} , \mu^{n+\frac{1}{2}}_{ij} \right)^T = b_{ij}\):
Where \( \operatorname{\mathbf{DL}}_{ij} \) is
\begin{align*}
 \operatorname{\mathbf{DL}}_{ij} &=
\begin{pmatrix}
\frac{1}{\Delta t} & \frac{1}{h^2}\Sigma_{Gij}  \\
-\frac{\varepsilon^2}{h^2}\Sigma_{Gij} - 2 & 1
\end{pmatrix}
\end{align*}
and where \( \Sigma_{Gij} = G_{i+\frac{1}{2}j} + G_{i-\frac{1}{2}j} + G_{ij+\frac{1}{2}} + G_{ij-\frac{1}{2}} \)
#+begin_src julia :tangle src/multisolver.jl :eval never
function dL(solver::multi_solver , i , j)
    return [ (1/solver.dt) (1/solver.h^2*neighbours_in_domain(i,j,G,solver.len , solver.width));
             (-1*solver.epsilon^2/solver.h^2 * neighbours_in_domain(i,j,G,solver.len , solver.width) - 2) 1]
    end
#+end_src
\( \operatorname{\mathbf{DL}}_{ij} \) is invertible, since its determinant is positive. Therefore the system eqref:eq:LES is solvable
\begin{equation}
\operatorname{det}(\operatorname{\mathbf{DL}}_{ij}) = \frac{1}{\Delta t} + \frac{1}{h^2}\Sigma_{Gij}  \left( + \frac{\varepsilon^2}{h^2}\Sigma_{Gij} +2 \right) > 0
\end{equation}
as \( \Sigma_{Gij} \in \{0,1,2,3,4\} \).

We rewrite eqref:eq:LES in terms of \( \operatorname{\mathbf{DL}}_{ij} \), using the abbreviation for \( \nabla_d \cdot(G_{ij}\nabla_d \mu_{ij}^{n+\frac{1}{2}}) \) introduced in eqref:eq:modified-laplacian.
 #+name: eq:explicit-smooth
 \begin{equation}
\begin{aligned}
&L
\begin{pmatrix}
\phi^{n+1}_{ij} \\
\mu^{n+\frac{1}{2}}_{ij}
\end{pmatrix}
=
\begin{pmatrix}
\zeta^n_{ij} \\
\psi^n_{ij}
\end{pmatrix}
\\
\implies \quad
&\operatorname{\mathbf{DL}}_{ij}\cdot
\begin{pmatrix}
\phi^{n+1}_{ij} \\
\mu^{n+\frac{1}{2}}_{ij}
\end{pmatrix}
+
\begin{pmatrix}
 - \frac{1}{h^2} \Sigma_{Gij}\mu_{ij}^{n+\frac{1}{2}} \\
+ \frac{\varepsilon^2}{h^2} \Sigma_{Gij}\phi_{ij}^{n+1} \\
\end{pmatrix}
=
\begin{pmatrix}
  \zeta_{ij}^n\\
\psi_{ij}^n
\end{pmatrix}
,\\
\implies \quad
&
\operatorname{\mathbf{DL}}_{ij}\cdot
\begin{pmatrix}
\phi^{n+1}_{ij} \\
\mu^{n+\frac{1}{2}}_{ij}
\end{pmatrix}
=
\begin{pmatrix}
  \zeta_{ij}^n\\
\psi_{ij}^n
\end{pmatrix}
-
\begin{pmatrix}
 - \frac{1}{h^2} \Sigma_{Gij}\mu_{ij}^{n+\frac{1}{2}} \\
+ \frac{\varepsilon^2}{h^2} \Sigma_{Gij}\phi_{ij}^{n+1} \\
\end{pmatrix}
\,,
\end{aligned}
\end{equation}
where
- \(  \Sigma_G \phi_{ij}^{n+1} = G_{i+\frac{1}{2}j} \phi^{n + 1,m}_{i+1j} +  G_{i-\frac{1}{2}j} \phi^{n + 1,m}_{i-1j} + G_{ij+\frac{1}{2}}  \phi^{n + 1,m}_{ij+1} + G_{ij-\frac{1}{2}} \phi^{n + 1,m}_{ij-1}  \),
 \(  \Sigma_G \mu_{ij}^{n+\frac{1}{2}} = G_{i+\frac{1}{2}j} \mu^{n + \frac{1}{2},m}_{i+1j} +  G_{i-\frac{1}{2}j} \mu^{n + \frac{1}{2},m}_{i-1j} + G_{ij+\frac{1}{2}}  \mu^{n + \frac{1}{2},m}_{ij+1} + G_{ij-\frac{1}{2}} \mu^{n + \frac{1}{2},m}_{ij-1}  \),

* Numerical solver
The two-grid method, we use, consists of a linear Gauss-Seidel solver and restriction, and prolongation methods, to interpolate between course and fine grids.
** Gauss-Seidel  smoothing
The authors [cite:@SHIN20117441]derived Gauss-Seidel Smoothing from eqref:eq:LES :
 Smoothing denoted as a SMOOTH operator consists of a Gauss-Seidel method, by solving [[eqref:eq:explicit-smooth]] for all \( i,j \) with the initial guess for \( \zeta^n_{ij} , \psi^n_{ij} \).

After having solved equation eqref:eq:LES for \( \left( i-1,j \right) , \left( i , j-1\right)\) we define the Gaus-Seidel iteration in \( s \) for \( \left( i,j \right) \) as follows:
#+name: eq:gauss-seidel
\begin{equation}
\operatorname{\mathbf{DL}}_{ij} \cdot
\begin{pmatrix}
\phi^{n+1 , s+1}_{ij} \\
\mu^{n+\frac{1}{2} , s+1}_{ij}
\end{pmatrix}
=
\begin{pmatrix}
  \zeta_{ij}^n\\
\psi_{ij}^n
\end{pmatrix}
-
\begin{pmatrix}
 - \frac{1}{h^2} \Sigma_{Gij}\mu_{ij}^{n+\frac{1}{2} , s + \frac{1}{2}} \\
+ \frac{\varepsilon^2}{h^2} \Sigma_{Gij}\phi_{ij}^{n+1 , s+\frac{1}{2}} \\
\end{pmatrix}
\,,
\end{equation}
where
- \(  \Sigma_G \phi_{ij}^{n+1  , s+\frac{1}{2}} = G_{i+\frac{1}{2}j} \phi^{n + 1,s}_{i+1j} +  G_{i-\frac{1}{2}j} \phi^{n + 1,s+1}_{i-1j} + G_{ij+\frac{1}{2}}  \phi^{n + 1,s}_{ij+1} + G_{ij-\frac{1}{2}} \phi^{n + 1,s+1}_{ij-1}  \),
- \(  \Sigma_G \mu_{ij}^{n+\frac{1}{2},s+\frac{1}{2}} = G_{i+\frac{1}{2}j} \mu^{n + \frac{1}{2},s}_{i+1j} +  G_{i-\frac{1}{2}j} \mu^{n + \frac{1}{2},s+1}_{i-1j} + G_{ij+\frac{1}{2}}  \mu^{n + \frac{1}{2},s}_{ij+1} + G_{ij-\frac{1}{2}} \mu^{n + \frac{1}{2},s+1}_{ij-1}  \),
This constitutes a Gauss-Seidel method in its element based formula.
#+name: calculate-left-hand-side
#+begin_src julia :eval never :exports none
bordernumber = neighbours_in_domain(i, j, G, solver.len, solver.width)

b = [(
            solver.xi[i, j]
            +
            discrete_G_weigted_neigbour_sum(
                i, j, solver.potential, G, solver.len, solver.width
            ) / solver.h^2
        ), (
            solver.psi[i, j]
            -
            (solver.epsilon^2 / solver.h^2) * discrete_G_weigted_neigbour_sum(
                i, j, solver.phase, G, solver.len, solver.width
            ))]


#+end_src
#+name:SMOOTH
#+begin_src julia :tangle src/multisolver.jl :eval never :noweb no-export
function SMOOTH!(
    solver::T,
    iterations,
    adaptive
) where T <: Union{multi_solver, adapted_multi_solver , gradient_boundary_solver}
    for s = 1:iterations
        # old_phase = copy(solver.phase)
        for I in CartesianIndices(solver.phase)[2:end-1, 2:end-1]
            i, j = I.I

            <<calculate-left-hand-side>>

            res = dL(solver, i,j ) \ b
            solver.phase[i, j] = res[1]
            solver.potential[i, j] = res[2]
        end
    end
end
#+end_src
We denote the approximations for \( \left( \phi_{ij}^{n+1} , \mu^{n+\frac{1}{2}}_{ij}  \right)  \) after smoothing, as  \( \left( \bar{\phi}_{ij}^{n+1} , \bar{\mu}^{n+\frac{1}{2}}_{ij}  \right)  \).
In Fig.[[fig:smoothing-examples]] we show 4 of the 7 initial data after 200 Gauss-Seidel iterations. It is apparent that the sharp interface from the initial Data has been diffused.
#+name: fig:smoothing-examples
#+begin_src julia-vterm :results file graphics  :file smooth.svg
<<input>>
<<setup-diverse-testgrids>>
plots= []
for t in tests
set_xi_and_psi!(t[1])
SMOOTH!(t[1], 200, true);
end
plots =[  heatmap(t[1].phase ,  legend=:none , aspectratio=:equal , grid=false , showaxis=false , size=(600,600))
          for t in tests[1:2:end]]
p = plot(plots... , layout=(1,4) , size=(2400,600))
savefig(p,"images/smooth.svg")

#+end_src

#+caption: Inputs from [[Initial data]] after SMOOTH.
#+RESULTS[fdb9207550b6615253fa672f5417f153b861be3b]: fig:smoothing-examples
[[file:images/smooth.svg]]

** Two-grid method
The numerical method proposed in [cite:@SHIN20117441] consists of repeated sub-iterations of a multi-grid V-cycle.  Specifically we use a two-grid implementation with a fixed number of sub-iterations. Defined as:
#+begin_src julia :eval never :exports code
for j in 1:timesteps

    set_xi_and_psi!(solvers[1])

    for i = 1:subiterations

        v_cycle!(solvers, 1)
    end
end
#+end_src
The approximations for \( \phi_{ij}^{n+1} , \mu_{ij}^{n+\frac{1}{2}} \) after the m^{th} V-cycle sub-iteration are denoted with \(  \phi_{ij}^{n+1,m+1} , \mu_{ij}^{n+\frac{1}{2},m+1}  \) where \( m \) denotes the current sub-iteration Furthermore the V-cycle consists of the following steps:
*** Pre Smoothing
Pre smoothing consists of a fixed number of Gauss-Seidel iterations, in our case *40*,  on the fine grid \( h \), as described in Chapter [[Gauss-Seidel smoothing]]. Afterwards we calculate the residual error \( \left(d_{ij,H}^{n+1,m} , r_{ij,H}^{n+1,m} \right) := L\left( \phi_{ij}^{n+1} , \mu^{n+\frac{1}{2}}_{ij}  \right) - (\zeta^n_{ij} , \psi^n_{ij}  )  \) for the course grid \( H \) correction.
*** Restriction
Restriction from the fine grid to the course grid \(  h \to H  \) for a variable eg. \( \phi_{ij} \) is done as follows:
\begin{equation}
\phi^{H}_{ij} = \frac{1}{\Sigma_{Gij}} \left(G_{2i,2j}\phi^{h}_{2i,2j} + G_{2i-1,2j} \phi^{h}_{2i-1,2j} + G_{2i,2j-1}  \phi^{h}_{2i,2j-1} +G_{2i-1,2j-1} \phi^{h}_{2i-1,2j-1} \right)
\end{equation}
*** Course grid solution
On the course grid we use a Gauss-Seidel iteration to solve \( L(\hat{\phi}_{ij,H}^{n+1,m}, \hat{\mu}_{ij,H}^{n+\frac{1}{2},m})_H = L(\bar{\phi}_{ij,H}^{n+1,m} , \bar{\mu}_{ij,H}^{n+\frac{1}{2},m}) + (d_{ij,H}^{n+1,m} , r_{ij,H}^{n+1,m}) \).
    We solve for \( \left( \hat{\phi}_{ij,H}^{n+1,m}, \hat{\mu}_{ij,H}^{n+\frac{1}{2},m} \right) \) using the same iteration as in Chapter [[Gauss-Seidel smoothing]] however we replace \( (\zeta_{ij}^{n} , \psi_{ij}^n) \) with  \(  L(\bar{\phi}_{ij,H}^{n+1,m} , \bar{\mu}_{ij,H}^{n+\frac{1}{2},m}) + (d_{ij,H}^{n+1,m} , r_{ij,H}^{n+1,m}) \).  In the iteration, where \( \left( \bar{\phi}_{ij,H}^{n+1,m} , \bar{\mu}_{ij,H}^{n+\frac{1}{2},m} \right) \) are the values after the smooth restricted to the coarser grid and \( \left( d_{ij,H}^{n+1,m} , r_{ij,H}^{n+1,m} \right) \) is the residual from the smooth  iteration on the fine grid restricted onto the coarse grid.
*** Prolongation
We prolong the solution from the course grid. Prolongation of a variable eg. \( \phi_{ij} \) from the course grid to the fine grid \( H\to h \) we do by using the nearest neighbour weighed by  \( G \).
\begin{equation}
\begin{pmatrix}
\phi^h_{2i,2j} \\
\phi^h_{2i-1,2j} \\
\phi^h_{2i,2j-1} \\
\phi^h_{2i-1,2j-1}
\end{pmatrix}
=
\begin{pmatrix}
G^{h}_{2i,2j}    \phi_{ij}^{H}       \\
G^{h}_{2i-1,2j}  \phi_{ij}^{H}       \\
G^{h}_{2i,2j-1}  \phi_{ij}^{H}       \\
G^{h}_{2i-1,2j-1}\phi_{ij}^{H}
\end{pmatrix}
\end{equation}
*** Post Smoothing
After prolongation of the course grid solution we perform a post smoothing step using *80* Gauss-Seidel steps. Post smoothing is otherwise identical to pre smoothing

*** additional considerations
We Do Gauss-Seidel smoothing with fixed iterations. As well as a fixed number of sub-iterations.
The V-cycle of a two-grid method using pre- and post-smoothing is then stated by:
#+begin_src julia :eval never :tangle src/mulisolver.jl
function v_cycle!(grid::Array{T}, level) where T <: solver
    finegrid_solver = grid[level]
    #pre SMOOTHing
    SMOOTH!(solver, 40, false)

    d = zeros(size(finegrid_solver.phase))
    r = zeros(size(finegrid_solver.phase))

    # calculate error between L and expected values
    for I in CartesianIndices(finegrid_solver.phase)[2:end-1, 2:end-1]
        d[I], r[I] = [finegrid_solver.xi[I], finegrid_solver.psi[I]]
        .- L(finegrid_solver, I.I..., finegrid_solver.phase[I], finegrid_solver.potential[I])
    end

    restrict_solver!(grid[level], grid[level+1])
    coursegrid_solver = grid[level+1]
    solution = deepcopy(coursegrid_solver)

    d_large = restrict(d, G)
    r_large = restrict(r, G)


    u_large = zeros(size(d_large))
    v_large = zeros(size(d_large))

    for I in CartesianIndices(coursegrid_solver.phase)[2:end-1, 2:end-1]
        coursegrid_solver.xi[I]  , coursegrid_solver.psi[I] = L(coursegrid_solver , I.I... , coursegrid_solver.phase[I] , coursegrid_solver.potential[I] ) .+ [d_large[I],r_large[I]]
    end

    SMOOTH!(coursegrid_solver, 40 , false)

    u_large = coursegrid_solver.phase .- solution.phase
    v_large = coursegrid_solver.potential .- solution.potential

    finegrid_solver = grid[level]
    finegrid_solver.phase .+= prolong(u_large , G)
    finegrid_solver.potential .+= prolong(v_large, G)


    SMOOTH!(finegrid_solver, 80, false)
end
#+end_src


* Numerical experiments
In the previous Chapter we discretized the CH equation based on the two-grid method described by the authors in [cite:@SHIN20117441] and we obtained a numerical scheme for \( \phi , \mu \). In this chapter we analyze the change in mass, change in total energy \( E^{bulk} \), the stability in time, and during sub-iterations.
Since we do not have exact solutions for the initial values tested we evaluate our solvers with a Cauchy criterion. The  initial values we use, if not mentioned otherwise, where:
| Variable: | \varepsilon           | \Delta t     | h         |
|-----------+-------------+---------+-----------|
| Value:    | 8 * 10^{-3} | 10^{-3} | 3*10^{-3} |
** Energy evaluations
Since the continuous total energy [[eqref:eq:energy]] decreases over time, we expect it's discrete counterpart to exhibit the same behaviour. We implement a discrete version of the energy,  and evaluate our solutions on it.
#+name: eq:discrete-energy
\begin{equation}
\begin{aligned}
E^{\text{bulk}}_d(\phi^{n}) &= \sum_{i,j \in \Omega} \frac{\varepsilon^2}{2} |G\nabla_d \phi_{ij} |^2 + W\left(\phi_{ij}\right)  \\
&= \sum_{i,j \in \Omega} \frac{\varepsilon^2}{2} G_{i+\frac{1}{2}j}(D_x\phi_{i+\frac{1}{2}j}) ^2 + G_{ij+\frac{1}{2}}(D_y\phi_{ij+\frac{1}{2}})^2  + W\left(\phi_{ij}\right)  .\\
\end{aligned}
\end{equation}
# [[bulk energy and mass balance]].
In Fig.[[fig:energy-balance]] we observe the discrete total energy going down with increasing number of time-steps, as we expect from a  CH based solver. Visually we observe the energy decrease as reduced surface curvature.
#+name: fig:energy-balance
#+begin_src julia-vterm :results file graphics :file energy_balance.svg
<<init>>
using JLD2
using DataFrames
i0 = 1*64 +1
results = jldopen("experiments/iteration.jld2")["result"]
energy = bulk_energy.(results[i0:i0+63,:].solver)

p1 = plot(1:64 ,
          energy ,
          title=L"Discrete Energy $E_d^{bulk}$",
          xlabel="timesteps" ,
          ylabel="energy"  ,
          label=false)
p2 = heatmap(results.solver[i0].phase ,
             title="initial condition" ,
             legend=:none ,
             aspectratio=:equal ,
             showaxis=false ,
             grid=false)
p3 = heatmap(results.solver[i0+63].phase ,
             title="after 64 time-steps" ,
             aspectratio=:equal ,
             legend=:none ,
             showaxis=false ,
             grid=false)
p = plot(p2,p3,p1 , layout=layout3x1 , size=size3x1  )

savefig(p , "images/energy_balance.svg")
#+end_src

#+caption: Behaviour of energy \( E_{bulk} \) over time for one initial condition \( \phi_0 \).
#+RESULTS[4170efb2b27acf5a7de4ebc5e4ba80cca62e5ac8]: fig:energy-balance
[[file:images/energy_balance.svg]]

** Numerical mass conservation
The analytical CH equation in [[eqref:eq:CH]]  is mass conservative as shown in [[eqref:eq:mass-conservation]].
For numerical experiments we observe the average value of \( \phi_{ij}^{n} \) on \( \Omega_d \).
\begin{align*}
&\frac{\sum_{i,j \in \Omega_d} \phi_{ij}^{n}}{N^2} & n \in& \{0 , \dots , 64\}
\end{align*}
Analytical mass conservation tells us
\begin{equation}
\begin{aligned}
\frac{d}{dt} \int_{\Omega} \phi \ d \operatorname{\mathbf{x}} &= 0 & \text{in} \,&\Omega \times (0,T)
\end{aligned}
\end{equation}
Therefore the average value of \( \phi(\vec{x},t)\,, \vec{x}\in\Omega \) is constant in time.
Hence, the average of our numerical solution should stay constant as well.
In practice we observe slight fluctuations in Figure [[fig:mass-balance]]. Those however are close to machine precision and can therefore be ignored.

#+name: fig:mass-balance
#+begin_src julia-vterm :results file graphics :file mass_balance.svg :output-dir images :noweb no-export :session jl
<<init>>
using JLD2
using DataFrames
using Measures
pgfplotsx()
i0 = 64 * 1 + 1
results = jldopen("experiments/iteration.jld2")["result"]
energy = [ massbal(s.phase) for s in results[i0:i0+63,:].solver]
p1 = plot(1:64 ,
          energy .- energy[1],
          xlabel= "time-steps" ,
          ylabel = "error" ,
          title = "phase change",
          label=false)
p2 = heatmap(results.solver[i0].phase ,
             title="initial condition" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(results.solver[i0+63].phase ,
             title="after 64 time-steps" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false ,
             legend=:none)

p = plot(p2,
         p3 ,
         p1 ,
         layout= layout3x1 ,
         size=size3x1)

savefig(p , "images/mass_balance.svg")
#+end_src

#+caption: Behaviour of phase change over time for one initial condition \( \phi_0 \).
#+RESULTS: fig:mass-balance
[[file:images/mass_balance.svg]]

** Stability of a two-grid sub-iteration
We expect our solver to stay stable when increasing the number of two-grid sub-iterations. To validate this assumption we show convergence with the following Cauchy criterion.
\begin{equation}
\| \phi^{n+1,m-1} - \phi^{n+1,m} \|_{Fr}:= \sqrt{ \sum_{i,j \in \Omega_d} \left|   \phi^{n+1,m-1}_{ij} - \phi^{n+1,m}_{ij} \right|^2 }
\end{equation}
We use similar criteria in the following sub chapters to show convergence for different hyperparameters.
We expect sub-iterations to show Cauchy convergence, which is what we observe in Figure [[fig:convergence]].
#+name: fig:convergence
#+begin_src julia-vterm :results file graphics :file convergence.svg :eval t
<<init>>
<<setup-diverse-testgrids>>
using DataFrames
using JLD2
using LaTeXStrings

pgfplotx()
i0 = 4
df = jldopen("experiments/subiteration.jld2")["result"]
gd = groupby(df , :iteration)
res = combine(gd  , :cycle => ((x)-> [norm(x[i].phase - x[i-1].phase) for i in 2:size(x,1)]))

gres =  groupby(res , :iteration)[1]

p1= res.cycle_function[i0*64:(i0+1)*64-2] |>
    (x)-> plot(x ,
               yscale=:log10 ,
               title="Behaviour" ,
               xlabel="sub-iterations" ,
               ylabel= L" \|\phi^{n+1,m} - \phi^{n+1,m-1}\|_{Fr} " ,
               label= false)
p2 = heatmap(df.cycle[i0].phase ,
             title="initial condition" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(df.cycle[i0].phase .-df.cycle[i0+62].phase ,
             title=L"\phi^{n+1,0} - \phi^{n+1,64}" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false )

p = plot(p2,
         p3 ,
         p1 ,
         layout= layout3x1 ,
         size=(1600 , 1600))
savefig(p , "images/convergence.svg")
#+end_src

        #+caption: Stability of the original CH solver for increasing sub-iterations
#+RESULTS[597a37658d2642d9a31996591fa9771fd0d8f57a]: fig:convergence
[[file:images/convergence.svg]]

During sub-iterations the convergence is exponential , and is reached after about 16 sub-iterations. The bend is only observed in the first time-step, and is likely due to the sharp interface in the initial values which is diffused during the first few sub-iterations. Looking at the difference before, and after one time step, it is apparent , that change is largest in areas with high curvature, which are mainly corners in the interface. Testing showed, that the number of sub-iterations required for convergence is dependant on the number of Gauss-Seidel iterations on each two-grid cycle. Though the general exponential behaviour stayed the same.

#+begin_src julia-vterm :results file graphics html :file subiteration.svg :output-dir images :noweb no-export :session jl :exports none
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
df = jldopen("experiments/subiteration.jld2")["result"]
gd = groupby(df , :iteration)
p1 = heatmap(gd[1].cycle[1].phase , aspectratio=:equal , title= "one subiteration" , showaxis=false  )
p2 = heatmap(gd[1].cycle[64].phase , aspectratio=:equal , title = "64 sub-iterations" , showaxis=false)
p = plot(p1,p2)
savefig(p , "images/subiteration.svg")
#+end_src

#+RESULTS[17fc4df2e4d089d1d12fd7209b2b8dc7cb027c15]:
#+begin_export html
[[file:images/subiteration.svg]]
#+end_export

** Stability in time
We expect our numerical error to decrease when calculating with smaller time steps. To test this, we  successively subdivide the original time interval \( [0,T] \) in finer parts. We fix \( \Delta t \cdot n = T \) for \( T=10^{-2} \) and test different values of \( n \). In Figure [[fig:stability-in-time]], as before, we employ a Cauchy criterion to compare the solution at \( T=10^{-2} \). We employ \( \|\phi^{n,64} - \phi^{n-1,64} \|_{Fr} \) as measure.
#+name: fig:stability-in-time
#+begin_src julia-vterm :results file graphics :file time-stability.svg
<<init>>
using DataFrames
using JLD2
using LaTeXStrings

df = jldopen("experiments/time.jld2")["result"]
gd = groupby(df , :iteration)

sd =  combine(x->(;phase=x[end,:].phase) , gd)
change = [norm(sd[!, "phase"][i] .- sd[! , "phase"][i-1]) for i=2:size(sd , 1)]

p1 = plot(change ,
         xlabel = L"number of time-steps to $t = 10^{-2}s$" ,
         ylabel=L"\|\phi_{ij}^{n+1} - \phi_{ij}^n \|_{Fr}" ,
          label = false,
         title= L"behavior of the original CH solver at $t=10^{-2}s$")
p2 = heatmap(gd[10].phase[end],
             title=L"$t=10^{-2} \,, n=10$" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(gd[end].phase[end],
             title=L"$t=10^{-2} \,, n=64$" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false ,
             legend=:none)

p = plot(p2,
         p3 ,
         p1 ,
         layout= layout3x1 ,
         size=size3x1)
savefig(p , "images/time-stability.svg")
#+end_src

#+Caption: Behavior of the baseline solver while solving the time interval \( T = \left[ 0 , 10^{-2} \right] \) with increasing number of time-steps.
#+RESULTS[0f922b31e3f46dde2af9273d00fa27b1317be553]: fig:stability-in-time
[[file:images/time-stability.svg]]

* Relaxed Cahn-Hilliard equation
In effort to decrease the order of complexity, from fourth order derivative to second order, we propose an elliptical relaxation approach, where the relaxation variable \( c \) is the solution of the following elliptical PDE:
#+name: eq:elliptical-equation
\begin{align}
- \Delta c^\alpha  + \alpha c^a &= \alpha \phi ^\alpha, & \text{in} \quad \Omega
\end{align}
where \( \alpha \) is a relaxation parameter. We expect to approach the original solution of the CH equation [[eqref:eq:CH]] as  \( \alpha \to \infty \).
This results in the following relaxation for the classical CH equation
#+name: eq:relaxed-cahn-hilliard
\begin{equation}
\begin{aligned}
\partial_t \phi^\alpha  &= \Delta \mu \,, & \text{in} \quad \Omega \times (0,T) \\
\mu &= - \varepsilon ^2 \alpha(c^\alpha - \phi^\alpha) + W'(\phi). & \text{in} \quad \Omega \times (0,T)
\end{aligned}
\end{equation}
It requires solving the elliptical PDE for each time-step to calculate \(c\).
** Relaxed initial value-boundary problem
The aim of the relaxed CH equation is then to find solutions \( \phi(\vec{x} , t) , \mu(\vec{x} , t): \Omega \times (0,T) \to \mathbb{R} \) such that they satisfy
#+name: eq:relaxed-initial-value-problem
\begin{equation}
\begin{aligned}
\partial_{t}\phi(x,t) &=  \Delta\mu^{\alpha}\,, \\
\mu^{\alpha} &= - \varepsilon ^2 \alpha(c^\alpha - \phi^\alpha) + W'(\phi) \,, \\
- \Delta c^\alpha  + \alpha c^a &= \alpha \phi ^\alpha, & \text{in} \quad \Omega&\times(0,T\\
-\nabla\mu^{\alpha} \cdot \mathbf{n} &= 0 \,, \\
\nabla\phi^{\alpha} \cdot \mathbf{n} &= 0 \,, \\
\nabla c^{\alpha} \cdot \mathbf{n} &= 0 & \text{on} \, \partial\Omega &\times (0,T)\,, \\
\phi^{\alpha}(\vec{x},0) &= \phi^0(\vec{x}) \,, \\
c^{\alpha}(\vec{x},0) &= \phi^0(\vec{x}) \,, &\text{in} \quad \Omega
\end{aligned}
\end{equation}

** Relaxed energy functional
Since the relaxed CH equation does not trivialy satisfy the same energy decay for eqref:eq:energy, we show the existence of a similar equation for the relaxed problem.
Motivated by the energy functional used in [cite:@CORLI2014773], we let \(\phi_0 \in H^4(\Omega) \) and \(T>0\) be fixed. We assume, there exists a classical solution \(\phi^\alpha , c^\alpha : \Omega \times (0,1) \to \RR  \) of eqref:eq:relaxed-initial-value-problem. Then the energy functional for the relaxed CH equation for \(\forall t \in (0,1)\) can be written as:
\begin{equation}
 \frac{d}{dt} E_{rel}(\phi^{\alpha} , c^\alpha) :=
\frac{d}{dt} \int_{\Omega}  \frac{1}{2}\varepsilon^2 \alpha (c^\alpha - \phi^{\alpha})^2 + W(x) \ d \operatorname{\mathbf{x}}
\end{equation}
Which is derived by a \(L_2\) inner product of eqref:eq:relaxed-cahn-hilliard with \(\mu^\alpha\).
\begin{equation}
\left< \phi_t^{\alpha} , \mu^{\alpha} \right> = \left< \Delta \mu^{\alpha} , \mu^{\alpha} \right>
\end{equation}
it then follows for the left hand side:
\begin{equation}
\begin{aligned}
\left< \phi^{\alpha}_t , \mu^{\alpha} \right> &= \left< \phi^{\alpha}_t , -  \varepsilon^2 \alpha (c^\alpha- \phi^{\alpha}) +  W'(\phi^{\alpha}) \right> \\
&= \int_{\Omega} -  \phi^{\alpha}_t \varepsilon^2 \alpha (c^\alpha - \phi^{\alpha})\ d \operatorname{\mathbf{x}} + \int_{\Omega} \phi^{\alpha}_t W'(\phi^{\alpha}) \ d \operatorname{\mathbf{x}} \\
&= \frac{d}{dt}  \int_{\Omega} \frac{1}{2}\varepsilon^2 \alpha (c^\alpha - \phi^{\alpha})^2\ d \operatorname{\mathbf{x}} + \frac{d}{dt} \int_{\Omega} W'(\phi^{\alpha}) \ d \operatorname{\mathbf{x}} \\
&= \frac{d}{dt} \int_{\Omega}  \frac{1}{2}\varepsilon^2 \alpha (c^\alpha - \phi^{\alpha})^2 + W(x) \ d \operatorname{\mathbf{x}}
= \frac{d}{dt} E_{rel}(\phi^{\alpha}, c^\alpha)
\end{aligned}
\end{equation}
and using the boundary condition \( \left( \nabla\mu \cdot \vec{n} \right) = 0 \) on the right hand side:
\begin{equation}
\begin{aligned}
\left< \Delta \mu^{\alpha} , \mu^{\alpha} \right> &= \int_{\Omega} \mu^{\alpha}\Delta\mu^{\alpha}  d \operatorname{\mathbf{x}} \\
&= - \int_{\Omega} \left|\nabla\mu^{\alpha} \right| d \operatorname{\mathbf{x}} + \int_{\partial\Omega} \mu^{\alpha} ( \nabla\mu^{\alpha} \cdot \vec{n})  d \operatorname{\mathbf{A}} \\
&= - \left\| \nabla \mu^{\alpha} \right\| \leq 0
\end{aligned}
\end{equation}
it therefore holds for a relaxed energy:
\begin{equation}
\frac{d}{dt} E_{rel}(\phi^\alpha , c^\alpha) = \frac{d}{dt} \int_{\Omega}  \frac{1}{2}\varepsilon^2 \alpha (c - \phi)^2 + W(x) \ d \operatorname{\mathbf{x}} \leq 0
\end{equation}
which gives a \(L_2\) bound for \( \Delta c =  \alpha(c-\phi)  \)  and  \(\nabla \mu^\alpha\), similar to the estimate for \( \nabla \mu \) given in the original CH equation.
** Relaxed mass conservation
We use the same aproach as in eqref:eq:mass-conservation to show that the CH equation eqref:eq:relaxed-cahn-hilliard is mass conservative.
\begin{equation}
\int_{\Omega}\partial_t \phi^{\alpha} \ d \operatorname{\mathbf{x}} = \int_{\Omega} \Delta \mu^{\alpha} \ d \operatorname{\mathbf{x}} = \int_{\partial\Omega} \nabla\mu^{\alpha} \cdot n \ d \operatorname{\mathbf{x}} =  0 \qquad \forall t\in (0,T)
\end{equation}
* Discretization of the relaxed problem
As approach for the numerical solver for the CH equation we propose:
#+name: eq:discrete-relaxed-cahn-hilliard
\begin{equation}
\begin{aligned}
\frac{\phi_{ij}^{n+1,\alpha} - \phi_{ij}^{n,\alpha}}{\Delta t}  &=  \nabla _d \cdot (G_{ij} \nabla_d \mu_{ij}^{n+\frac{1}{2},\alpha} )  \,,\\
 \mu_{ij}^{n+\frac{1}{2},\alpha} &= 2\phi_{ij}^{n+1,\alpha} - \varepsilon^2 a(c_{ij}^{n+1,\alpha} - \phi_{ij}^{n+1,\alpha})  + W'(\phi_{ij}^{n,\alpha}) - 2\phi _{ij}^{n,\alpha} \,. & i,j &\in \{2, \dots , N+1\}
\end{aligned}
\end{equation}
This approach is inspired by [[eqref:eq:discrete-cahn-hilliard]] and adapted to the relaxed CH equation [[eqref:eq:discrete-relaxed-cahn-hilliard]].
We then apply the multi-grid method proposed in [[Two-grid method]] to the relaxed problem by replacing the differential operators with their discrete counterparts, as defined in [[eqref:eq:discretization]],
and expand them.
To solve the additional elliptical system, we propose a simple implicit finite difference scheme similar to what we use for the baseline solver.
\begin{align*}
- \nabla_d \cdot  (G_{ij} \nabla_d c_{ij}^{n+1,\alpha}) + \alpha  c_{ij}^{n+1,\alpha} &= \alpha \phi_{ij}^{n+1,\alpha} \,, & i,j &\in \{2, \dots , N+1\}
\end{align*}
** Elliptical PDE
We then use the finite differences defined in eqref:eq:discretization to derive the corresponding linear system.
\begin{align*}
- \frac{1}{h^2} ( G_{i+\frac{1}{2}j}(c_{i+1j}^{n+1,\alpha} - c_{ij}^{n+1,\alpha}) & \\
+G_{ij+\frac{1}{2}}(c_{ij+1}^{n+1,\alpha} - c_{ij}^{n+1,\alpha}) & \\
+G_{i-\frac{1}{2}j}(c_{i-1j}^{n+1,\alpha} - c_{ij}^{n+1,\alpha})& \\
+G_{ij-\frac{1}{2}}(c_{ij-1}^{n+1,\alpha} - c_{ij}^{n+1,\alpha})) + \alpha  c_{ij}^{n+1\alpha} &=\alpha  \phi_{ij}^{n+1,\alpha} \,, & i,j \in \{2, \dots , N+1\}
\end{align*}

We abbreviate \(  \Sigma_G c^{n+1,\alpha}_{ij} = G_{i+\frac{1}{2}j} c^{n+1,\alpha}_{i+1j} +  G_{i-\frac{1}{2}j} c^{n+1,\alpha}_{i-1j} + G_{ij+\frac{1}{2}}  c^{n+1,\alpha}_{ij+1} + G_{ij-\frac{1}{2}} c^{n+1,\alpha}_{ij-1}  \) and \(  \Sigma_{Gij} = G_{i+\frac{1}{2}j} + G_{i-\frac{1}{2}j} + G_{ij+\frac{1}{2}} + G_{ij-\frac{1}{2}}  \). Then the discrete elliptical PDE can be stated as:
#+name: eq:discrete_elyps
\begin{align}
-\frac{ \Sigma_G c^{n+1,\alpha}_{ij}}{h^2} + \frac{\Sigma_G}{h^2} c^{n+1,\alpha}_{ij} + \alpha c^{n+1,\alpha}_{ij} &= \alpha\phi^{n+1,\alpha}_{ij} \,. & i,j&\in \{2, \dots , N+1\} , \alpha \in \RR^{+}
\end{align}
this constitutes a linear system with \( N\times N \) equations
** Relaxed system
#+end_src
We use the same discretization approach, as for the baseline system.
We reformulate the discretization [[eqref:eq:discrete-relaxed-cahn-hilliard]] in terms of the relaxed function \(L_r\) as follows:
\begin{align*}
L_r
\begin{pmatrix}
\phi ^{n+1,\alpha}_{ij} \\
\mu^{n+\frac{1}{2},\alpha}_{ij}
\end{pmatrix}
&=
\begin{pmatrix}
\frac{\phi^{n+1,m,\alpha}_{ij}}{\Delta t} - \nabla _d \cdot (G_{ji} \nabla _d \mu^{n + \frac{1}{2},m,\alpha}_{ji}) \\
\varepsilon ^2 \alpha (c^\alpha_{ij} - \phi^{n+1,m,\alpha}_{ij}) - 2\phi ^{n+1,m,\alpha}_{ij} -\mu^{n + \frac{1}{2},m,\alpha}_{ji}
\end{pmatrix}
& \forall i,j &\in\{2, \dots , N+1\}
\end{align*}
#+begin_src julia :tangle src/multi_relaxed.jl :eval never
function L(solver::relaxed_multi_solver,i,j , phi , mu)
    xi = solver.phase[i, j] / solver.dt -
         (discrete_G_weigted_neigbour_sum(i, j, solver.potential, G, solver.len, solver.width)
          -
          neighbours_in_domain(i, j, G, solver.len, solver.width) * mu )/solver.h^2
    psi = solver.epsilon^2 * solver.alpha*(solver.c[i,j] - phi) - solver.potential[i,j] - 2 * solver.phase[i,j]
    return [xi, psi]
end
#+end_src

and its Jacobian:
\begin{align*}
DL_r\begin{pmatrix}
\phi^{n+1,\alpha, m}_{ij} \\
\mu^{n+\frac{1}{2},m,\alpha}_{ij}
\end{pmatrix} &= \begin{pmatrix}
\frac{1}{\Delta t} & \frac{1}{h^2}\Sigma_{G}  \\
- \varepsilon^2 \alpha  - 2 & 1
\end{pmatrix}
& \forall i,j &\in\{2, \dots , N+1\}
\end{align*}
#+begin_src julia :tangle src/multi_relaxed.jl :eval never
function dL(solver::relaxed_multi_solver , i , j)
    return [ (1/solver.dt) (1/solver.h^2*neighbours_in_domain(i,j,G,solver.len , solver.width));
             (-1*solver.epsilon^2 * solver.alpha  - 2) 1]
    end
#+end_src
Much like in the original solver in eqref:eq:explicit-smooth now write eqref:eq:discrete-relaxed-smooth in terms of the function above.
\begin{align}
L_r
\begin{pmatrix}
\phi ^{n+1,\alpha}_{ij} \\
\mu^{n+\frac{1}{2},\alpha}_{ij}
\end{pmatrix}
&=
\begin{pmatrix}
\zeta^n_{ij}
 \\
\psi^n_{ij}
\end{pmatrix},
& \forall i,j &\in\{2, \dots , N+1\}
\end{align}
where \( \left( \zeta_{ij}^n  , \psi_{ij}^n \right) \) are the same in the original and relaxed solvers.
Since the relaxed CH equation is no longer second order in both directions the resulting LES is simpler. To take advantage of this, we resolve the system algebraically for each grid-point \( \left( i.j \right) \in \{2, \dots , N+1\} \).
#+name: eq:discrete-relaxed-smooth
\begin{align}
  -\frac{\Sigma_{Gij}}{h^2}\mu^{n + \frac{1}{2},m,\alpha}_{ji} &= \frac{\phi ^{n+1,m,\alpha}_{ij}}{\Delta t} - \zeta^{n,\alpha}_{ij} - \frac{\Sigma_G\mu_{ij}}{h^2} \,,\\
\label{discrete-relaxed-smooth2}
 \varepsilon ^2 \alpha \phi ^{n+1,m,\alpha}_{ij} + 2 \phi ^{n+1,m,\alpha}_{ij} &= \varepsilon ^2 \alpha c^{n,\alpha}_{ij}  -\mu^{n + \frac{1}{2},m,\alpha}_{ji}  - \psi_{ij}^{n,\alpha} \,,
\end{align}
where
- \(  \Sigma_G \mu_{ij} = G_{i+\frac{1}{2}j} \mu^{n + \frac{1}{2},m}_{i+1j} +  G_{i-\frac{1}{2}j} \mu^{n + \frac{1}{2},m}_{i-1j} + G_{ij+\frac{1}{2}}  \mu^{n + \frac{1}{2},m}_{ij+1} + G_{ij-\frac{1}{2}} \mu^{n + \frac{1}{2},m}_{ij-1}  \),
We simplify eqref:eq:discrete-relaxed-smooth by substituting \( \mu_{ij}^{n+1,\alpha} \) from the first line into the second.
\begin{align*}
\varepsilon^2 \alpha(\phi_{ij}^{n+1,m,\alpha}) + 2\phi_{ij}^{n+1,m,\alpha} &= \varepsilon^2 \alpha c^\alpha - \frac{h^2}{\Sigma_G} (\frac{\phi_{ij}^{n+1,m,\alpha}}{\Delta t} - \zeta^n_{ij} - \frac{1}{h^2} \Sigma_G \mu_{ij}) - \psi_{ij}
\end{align*}
We solve this system for \( \phi_{ij}^{n+1,m,\alpha} \). This results in the following system
#+name: eq:relaxed-les
\begin{equation}
\begin{aligned}
 \phi_{ij}^{n+1,m,\alpha} &= \left(\varepsilon^2 \alpha c^\alpha - \frac{h^2}{\Sigma_G}(- \zeta^n_{ij} - \frac{\Sigma_G \mu_{ij}}{h^2} ) -\psi_{ij}\right)\left(\varepsilon^2 \alpha + 2 + \frac{h^2}{\Sigma_G \Delta t}\right)^{-1} \\
\mu_{ij}^{n+\frac{1}{2} ,m , \alpha} &= \frac{h^2}{\Sigma_G} (\frac{\phi_{ij}^{n+1,m,\alpha}}{\Delta t} - \zeta^n_{ij} - \frac{1}{h^2} \Sigma_G \mu_{ij})
& \forall i,j &\in\{2, \dots , N+1\}
\end{aligned}
\end{equation}
* Relaxed numerical solver
** Gauss Seidel solver for the elliptical system
To solve the elliptical system, we introduce a Gauss-Seidel solver similar to the Gauss-Seidel Solver used for the smoothing step in the two-grid method.
We define this iteration in terms of \( s \).
For the Gauss-Seidel Iterative solver, we define the abbreviations
 \[  \Sigma_G c^{n+1,\alpha , s+\frac{1}{2}}_{ij} = G_{i+\frac{1}{2}j} c^{n+1,\alpha,s}_{i+1j} +  G_{i-\frac{1}{2}j} c^{n+1,\alpha,s+1}_{i-1j} + G_{ij+\frac{1}{2}}  c^{n+1,\alpha, s}_{ij+1} + G_{ij-\frac{1}{2}} c^{n+1,\alpha ,s+1}_{ij-1}  \]
We then define the Gaus-Seidel iteration by the following, and solve algebraically for \( c_{ij}^{n+1,\alpha,s+1} \)
\begin{align*}
\left( \frac{\Sigma_{Gij}}{h^2} + \alpha \right)c_{ij}^{n+1,\alpha,s+1} = \alpha\phi^{n+1,\alpha}_{ij} + \frac{\Sigma_G c_{ij}^{n+1,\alpha,s+\frac{1}{2}}}{h^2}\\
c_{ij}^{n+1,\alpha,s+1} = \frac{\alpha\phi^{n+1,\alpha}_{ij} + \frac{\Sigma_G c_{ij}^{n+1,\alpha, s+\frac{1}{2}}}{h^2}}{\frac{\Sigma_{G}}{h^2} + \alpha}\\
c_{ij}^{n+1,\alpha, s+1} = \frac{\alpha h^2 \phi^{n+1,\alpha}_{ij}}{\Sigma_{Gij} + \alpha h^2} + \frac{\Sigma_G c_{ij}^{n+1,\alpha , s+\frac{1}{2}}}{\Sigma_{Gij} + \alpha h^{2}}
\end{align*}
We the Gaus-Seidel solver for *1000* iterations to ensure convergence.
Furthermore we denote the solution of the iterative solver with \( c_{ij}^{n+1,\alpha} \). We implement the corresponding iteration as follows:
#+begin_src julia :eval never :tangle src/elypssolver.jl :exports none
using ProgressBars

"""
    elyps_solver(c,
    phase,
    len,
        width,
    alpha,
    h,
    n
)

TBW
"""
#+end_src
#+name: elyps_solver
#+begin_src julia :eval never :tangle src/elypssolver.jl
function elyps_solver!(solver::T, n) where T  <: Union{relaxed_multi_solver , adapted_relaxed_multi_solver}
    for k in 1:n
        for i = 2:(solver.len+1)
            for j = 2:(solver.width+1)
                bordernumber = neighbours_in_domain(i, j,G, solver.len, solver.width)
                solver.c[i, j] =
                    (
                        solver.alpha * solver.phase[i, j] +
                        discrete_G_weigted_neigbour_sum(i, j, solver.c, G, solver.len, solver.width) / solver.h^2
                    ) / (bordernumber / solver.h^2 + solver.alpha)

            end
        end
    end
end
** Relaxed Gauss-Seidel iteration
Similar to eqref:eq:gauss-seidel, we derive a Gauss-Seidel iteration for the relaxed problem from eqref:eq:relaxed-les.
\begin{equation}
\begin{aligned}
 \phi_{ij}^{n+1,\alpha,s+1} &= \left(\varepsilon^2 \alpha c^\alpha - \frac{h^2}{\Sigma_G}(- \zeta^n_{ij} - \frac{\Sigma_G \mu_{ij}^{n+\frac{1}{2},\alpha,s+\frac{1}{2}}}{h^2} ) -\psi_{ij}^n\right)\left(\varepsilon^2 \alpha + 2 + \frac{h^2}{\Sigma_G \Delta t}\right)^{-1} \\
\mu_{ij}^{n+\frac{1}{2} , \alpha , s+1} &= \frac{h^2}{\Sigma_G} (\frac{\phi_{ij}^{n+1,\alpha , s+1}}{\Delta t} - \zeta^n_{ij} - \frac{1}{h^2} \Sigma_G \mu_{ij}^{n+\frac{1}{2},\alpha,s+\frac{1}{2}})
\end{aligned}
\end{equation}
where
- \(  \Sigma_G \mu_{ij}^{n+\frac{1}{2},\alpha,s+\frac{1}{2}} = G_{i+\frac{1}{2}j} \mu^{n + \frac{1}{2},s}_{i+1j} +  G_{i-\frac{1}{2}j} \mu^{n + \frac{1}{2},s+1}_{i-1j} + G_{ij+\frac{1}{2}}  \mu^{n + \frac{1}{2},s}_{ij+1} + G_{ij-\frac{1}{2}} \mu^{n + \frac{1}{2},s+1}_{ij-1}  \),
Contrary to the Gauss-Seidel iteration in the baseline solver, this iteration is significantly cheaper to calculate, since it no longer requires solving a 2x2 LES for each grid-point.
#+name: solve-for-phi
#+begin_src julia :eval never :exports none
bordernumber = neighbours_in_domain(i, j, G, solver.len, solver.width)

solver.phase[I] = (solver.epsilon^2 * solver.alpha * solver.c[I] - solver.h^2 / bordernumber * ( -solver.xi[I]  - discrete_G_weigted_neigbour_sum(i,j,solver.potential , G , solver.len , solver.width) / solver.h^2 ) - solver.psi[I]) / (solver.epsilon^2 * solver.alpha  + 2 + solver.h^2 / (bordernumber*solver.dt))
#+end_src
#+name: update-the-potential
#+begin_src julia :eval never :exports none
            solver.potential[I] = (solver.phase[I]/solver.dt - solver.xi[I] - discrete_G_weigted_neigbour_sum(i,j, solver.potential , G , solver.len , solver.width)/solver.h^2) * (-solver.h^2/bordernumber)
#+end_src
We implement the iteration as:
#+name: SMOOTH_relaxed
#+begin_src julia :eval never :tangle src/multi_relaxed.jl :noweb no-export
function SMOOTH!(
    solver::T,
    iterations,
    adaptive
) where T <: Union{relaxed_multi_solver , adapted_relaxed_multi_solver}
    for k = 1:iterations
        # old_phase = copy(solver.phase)
        for I in CartesianIndices(solver.phase)[2:end-1, 2:end-1]
            i, j = I.I
            <<solve-for-phi>>
            <<update-potential>>
        end

        #if adaptive && LinearAlgebra.norm(old_phase - solver.phase) < 1e-10
            ##println("SMOOTH terminated at $(k) succesfully")
            #break
        #end
    end
end
#+end_src

** The relaxed two-grid method
As the difference between both methods is abstracted away in the operators, the relaxed V-cycle replaces the original operators with their relaxed counterparts. Due to julias multiple dispatch feature this changes nothing in the implementation Therefore we reuse the original V-cycle as defined in the [[Two-grid method]].
In the executions for each time step, we add the elliptic solver inside the sub-iteration. The iterative solver is then defined as:
#+begin_src julia :eval never :exports code
for j in 1:timesteps

    set_xi_and_psi!(solvers[1])

    for i = 1:subiterations

        elyps_solver!(solvers[1] , 1000)
        v_cycle!(solvers, 1)
    end
end
#+end_src

* Discrete mass conservation
Since both the CH equation [[eqref:eq:CH]] and the baseline solver from Fig.[[fig:mass-balance]] are mass conservative, the relaxed solver should be as well.
Mass conservation for the CH equation is given as
\begin{equation}
\int_{\Omega} \partial_t\phi = 0
\end{equation}
We show a discrete analogue for both the baseline and the relaxed approach
\begin{equation}
\sum_{i,j \in \Omega_d} \frac{1}{\Delta t} (\phi_{ij}^{n+1} - \phi_{ij}^{n}) = 0.
\end{equation}
We show this for a square domain \( \Omega_d \) using the first line of  eqref:eq:discrete-cahn-hilliard and eqref:eq:discrete-relaxed-cahn-hilliard respectively.
\begin{equation}
\begin{aligned}
\sum_{i=2}^{N+1} \sum_{j=2}^{N+1} \frac{1}{\Delta t} \left( \phi_{ij}^{n+1} - \phi_{ij}^{n} \right) &= \sum_{i=2}^{N+1}\sum_{j=2}^{N+1} \nabla_d \cdot (G_{ij} \nabla_d \mu_{ij}^{n+\frac{1}{2}}) \\
\end{aligned}
\end{equation}
We split the right double sum into three parts. We consider them separately. The first part consists of the inner sum, where \( G_{i+\frac{1}{2}j} = G_{i+\frac{1}{2}j} = G_{ij+\frac{1}{2}} = G_{ij-\frac{1}{2}} = 0 \). The inner sum can therefore be written as such:
\begin{equation}
\begin{aligned}
&= \sum_{i=3}^{N}\sum_{j=3}^{N} \frac{1}{h^2} \left( \mu^{n+\frac{1}{2}}_{i+1j} + \mu^{n+\frac{1}{2}}_{i-1j} + \mu^{n+\frac{1}{2}}_{ij+1} + \mu^{n+\frac{1}{2}}_{ij-1} - 4 \mu^{n+\frac{1}{2}}_{ij} \right) \\
\end{aligned}
\end{equation}
The second part consists of the sums over the edges excluding the corners.
\begin{equation}
\begin{aligned}
&+ \sum_{i=3}^{N} \frac{\Sigma_G\mu_{i2}^{n+\frac{1}{2}} - \Sigma_{Gi2}\cdot \mu_{i2}^{n+\frac{1}{2}}}{h^2} \\
&+ \sum_{i=3}^{N} \frac{\Sigma_G\mu_{iN+1}^{n+\frac{1}{2}} - \Sigma_{GiN+1}\cdot \mu_{iN+1}^{n+\frac{1}{2}}}{h^2} \\
&+ \sum_{j=3}^{N} \frac{\Sigma_G\mu_{i2}^{n+\frac{1}{2}} - \Sigma_{Gi2}\cdot \mu_{i2}^{n+\frac{1}{2}}}{h^2} \\
&+ \sum_{j=3}^{N} \frac{\Sigma_G\mu_{N+1j}^{n+\frac{1}{2}} - \Sigma_{GN+1j}\cdot \mu_{N+1j}^{n+\frac{1}{2}}}{h^2}\\
\end{aligned}
\end{equation}
And the third part consists of the corners.
\begin{equation}
\begin{aligned}
&+ \frac{\Sigma_G\mu_{N+1N+1}^{n+\frac{1}{2}} - \Sigma_{GN+1,N+1}\cdot \mu_{N+1,N+1}^{n+\frac{1}{2}}}{h^2}\\
&+  \frac{\Sigma_G\mu_{N+1,2}^{n+\frac{1}{2}} - \Sigma_{GN+1,2}\cdot \mu_{N+1,2}^{n+\frac{1}{2}}}{h^2}\\
&+  \frac{\Sigma_G\mu_{2,N+1}^{n+\frac{1}{2}} - \Sigma_{G2,N+1}\cdot \mu_{2,N+1}^{n+\frac{1}{2}}}{h^2}\\
&+  \frac{\Sigma_G\mu_{2,2}^{n+\frac{1}{2}} - \Sigma_{G 2,2}\cdot \mu_{2,2}^{n+\frac{1}{2}}}{h^2}
\end{aligned}
\end{equation}
The first double sum is a telescopic sum, and contracts to the following:
\begin{equation}
\begin{aligned}
\sum_{i=3}^{N}\sum_{j=3}^{N} \frac{1}{h^2} \left( \mu^{n+\frac{1}{2}}_{i+1j} + \mu^{n+\frac{1}{2}}_{i-1j} + \mu^{n+\frac{1}{2}}_{ij+1} + \mu^{n+\frac{1}{2}}_{ij-1} - 4 \mu^{n+\frac{1}{2}}_{ij} \right) &=
 \sum_{i=3}^{N} \mu_{i2}^{n+\frac{1}{2}} - \mu_{i3}^{n+\frac{1}{2}}\\
& + \sum_{i=3}^{N} \mu_{iN+1}^{n+\frac{1}{2}} - \mu_{iN}^{n+\frac{1}{2}}\\
& + \sum_{j=3}^{N} \mu_{2j}^{n+\frac{1}{2}} - \mu_{3j}^{n+\frac{1}{2}}\\
& + \sum_{j=3}^{N} \mu_{N+1j}^{n+\frac{1}{2}} - \mu_{Nj}^{n+\frac{1}{2}}
\end{aligned}
\end{equation}
Additionally, we simplify each of the sums in the second part, since the values of \( G \) are known on the boundary. On the right boundary ,for \( 2 < i < N+1 \),  \( G_{iN+\frac{3}{2}} = 0 \) and \( G_{iN+\frac{1}{2}}  = G_{i+\frac{1}{2}N+1}= 1 \)  it therefore follows:
\begin{equation}
\begin{aligned}
\sum_{i=3}^{N} \frac{\Sigma_G\mu_{iN+1}^{n+\frac{1}{2}} - \Sigma_{GiN+1}\cdot \mu_{iN+1}^{n+\frac{1}{2}}}{h^2} &=
\frac{1}{h^2}\sum_{i=3}^{N} G_{i+\frac{1}{2}N+1} \mu^{n + \frac{1}{2}}_{i+1N+1} +  G_{i-\frac{1}{2}N+1} \mu^{n + \frac{1}{2}}_{i-1N+1} \\
& \qquad+ G_{iN+\frac{3}{2}}  \mu^{n + \frac{1}{2}}_{iN+2} + G_{iN-\frac{3}{2}} \mu^{n + \frac{1}{2}}_{iN} \\
&\qquad- (G_{iN+\frac{3}{2}}+G_{iN+\frac{1}{2}}+G_{i+\frac{1}{2}N+1}+G_{i-\frac{1}{2}N+1}) \mu^{n+\frac{1}{2}}_{iN+1} \\
&= \frac{1}{h^2} \sum_{i=3}^{N} \mu^{n+\frac{1}{2}}_{i+1N+1} +  \mu^{n+\frac{1}{2}}_{i-1N+1} + \mu^{n+\frac{1}{2}}_{iN} - 3\mu^{n+\frac{1}{2}}_{iN+1}
\end{aligned}
\end{equation}
this sum, as it it is telescopic simplify further to
\begin{equation}
\begin{aligned}
\sum_{i=3}^{N} \frac{\Sigma_G\mu_{iN+1}^{n+\frac{1}{2}} - \Sigma_{GiN+1}\cdot \mu_{iN+1}^{n+\frac{1}{2}}}{h^2} &=  (\mu_{NN+1}+\mu_{3N+1})-(\mu_{N+1N+1}+\mu_{2N+1}) \\
&-  \sum_{i=3}^{N} \mu^{n+\frac{1}{2}}_{i1N} - \mu^{n+\frac{1}{2}}_{iN+1}
\end{aligned}
\end{equation}
similar the other three sums on the boundary simplify to
\begin{equation}
\begin{aligned}
\sum_{i=3}^{N} \frac{\Sigma_G\mu_{i2}^{n+\frac{1}{2}} - \Sigma_{Gi2}\cdot \mu_{i2}^{n+\frac{1}{2}}}{h^2} &= (\mu_{N,2}+\mu_{3,2}) - (\mu_{N+1,2}+\mu_{2,2}) - \sum_{i=3}^{N} \mu^{n+\frac{1}{2}}_{i1N} - \mu^{n+\frac{1}{2}}_{iN+1}  \\
\sum_{j=3}^{N} \frac{\Sigma_G \mu_{2j}^{n+\frac{1}{2},\alpha} - \Sigma_{G2j}\cdot \mu_{2j}^{n+\frac{1}{2}}}{h^2} &= (\mu_{2,3}+\mu_{2,N}) - (\mu_{2,N+1}+ \mu_{2,2}) - \sum_{j=3}^{N} \mu^{n+\frac{1}{2}}_{2j} - \mu^{n+\frac{1}{2}}_{3j} \\
\sum_{j=3}^{N} \frac{\Sigma_G\mu_{N+1j}^{n+\frac{1}{2}} - \Sigma_{GN+1j}\cdot \mu_{N+1j}^{n+\frac{1}{2}}}{h^2} &= (\mu_{N+1,N}+\mu_{N+1,3}) - (\mu_{N+1,N+1}+\mu_{N+1,2}) -  \sum_{j=3}^{N} \mu^{n+\frac{1}{2}}_{N+1,j} - \mu^{n+\frac{1}{2}}_{N,j}
\end{aligned}
\end{equation}
we observe that the resulting sums are equal and opposite to the result from the first sum. They therefore cancel each other and we are left with the corner terms. Those terms sum up to
\begin{equation}
\mu_{N+1,N}+\mu_{N+1,3} - 2\mu_{N+1,N+1} ++\mu_{2,3}+\mu_{2,N} - 2\mu_{2,2} + \mu_{N,2}+\mu_{N+1,3} - 2\mu_{2,N+1} + \mu_{N,N+1}+\mu_{3,N+1} - 2\mu_{N+1,2}
\end{equation}
The third sum, on the corners, can be simplified the same way as the two others.
\begin{equation}
\begin{aligned}
\frac{\Sigma_G\mu_{N+1N+1}^{n+\frac{1}{2}} - \Sigma_{GN+1,N+1}\cdot \mu_{N+1,N+1}^{n+\frac{1}{2}}}{h^2} &= \frac{1}{h^2} (\mu^{n+\frac{1}{2}}_{NN+1} + \mu^{n+\frac{1}{2}}_{N+1N} - 2 \mu^{n+\frac{1}{2}}_{N+1N})\\
\frac{\Sigma_G\mu_{2,2}^{n+\frac{1}{2}} - \Sigma_{G2,2}\cdot \mu_{2,2}^{n+\frac{1}{2}}}{h^2} &= \frac{1}{h^2} (\mu^{n+\frac{1}{2}}_{3,2} + \mu^{n+\frac{1}{2}}_{2,3} - 2 \mu^{n+\frac{1}{2}}_{2,2})\\
\frac{\Sigma_G\mu_{2N+1}^{n+\frac{1}{2}} - \Sigma_{G2,N+1}\cdot \mu_{2,N+1}^{n+\frac{1}{2}}}{h^2} &= \frac{1}{h^2} (\mu^{n+\frac{1}{2}}_{3N+1} + \mu^{n+\frac{1}{2}}_{2N} - 2 \mu^{n+\frac{1}{2}}_{2N+1})\\
\frac{\Sigma_G\mu_{N+1,2}^{n+\frac{1}{2}} - \Sigma_{GN+1,2}\cdot \mu_{N+1,2}^{n+\frac{1}{2}}}{h^2} &= \frac{1}{h^2} (\mu^{n+\frac{1}{2}}_{N+1,3} + \mu^{n+\frac{1}{2}}_{N,2} - 2 \mu^{n+\frac{1}{2}}_{N+1,2})
\end{aligned}
\end{equation}
Those terms cancel out with what remains in the second sum. We therefore conclude
#+name: eq:discrete-mass-conservation
\begin{equation}
\sum_{i=2}^{N+1} \sum_{j=2}^{N+1} \frac{1}{\Delta t} \left( \phi_{ij}^{n+1} - \phi_{ij}^{n}\right) = 0
\end{equation}
 Therefore, in eqref:eq:discrete-mass-conservation, the discretizations for both, the baseline and the relaxed methods, have a discrete equivalent of mass conservation.
* Numerical experiments for the relaxed system
We expect the relaxed solver to behave the same as the baseline method for all test cases that we have introduced in Chapter [[Numerical experiments]]. Therefore we run the same experiments for our relaxed solver. If not mentioned otherwise, we use the following hyper-parameters
| Variable: | \varepsilon           | \Delta t     | h         |
| Value:    | 8 * 10^{-3} | 10^{-3} | 3*10^{-3} |
** explicit and implicit solution of the elliptical problem
Initially we experimented with solving the elliptical problem explicitly at the beginning of each time-step. This resulted in inconsistent behavior. We show the extend of this in correlation to \( \alpha \) and \(\varepsilon\) in [[fig:relaxed-smooth-eval]] where we run 100 elliptical Gauss-Seidel iterations  followed by 1000 Gauss-Seidel iterations for the relaxed problem. We observe difficulty in developing the diffuse interface of the CH equation.Furthermore, comparing the results for \( \alpha = 10000 , \varepsilon \in \{0.05 , 0.025\} \) with the result of the original solver for \( \varepsilon=0.01 \) might suggest to use a lower value of \( \varepsilon \) to mitigate this. However this resulted in unpredictable interface with in further time steps.

#+name: fig:relaxed-smooth-eval
#+begin_src julia-vterm :results file graphics :file explicit-elips-smooth.svg :eval t
using DataFrames
using JLD2
using ProgressMeter
using LaTeXStrings
<<init>>
<<setup-diverse-testgrids>>
default(fontfamily="computer modern" , titlefontsize= 6 , guidefontsize=2 , tickfontsize = 2 , legendfontsize=2)
gr()
n = 4
m = 64
plots = []

for ε in [5e-2 , 2.5e-2, 1e-2]
g = testgrid(multi_solver, M , 2 , epsilon = ε)
set_xi_and_psi!(g[1])
for i = 1:10
   SMOOTH!(g[1], 100 , true )
end
po = heatmap(g[1].phase , title=L"original , $\varepsilon = %$ε$ " , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
push!(plots , po)


for α in [1e3 , 1e4 , 1e5 ,1e6]
    g = testgrid(relaxed_multi_solver, M , 2 , alpha = α , epsilon = ε)
    set_xi_and_psi!(g[1])
    elyps_solver!(g[1] , 100)
    for i = 1:10
        SMOOTH!(g[1], 100 , true )
    end
px = heatmap(g[1].phase , title=L"\alpha=%$α , \varepsilon=%$ε" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
    push!(plots , px)
    end
    end
p = plot(plots... , layout=grid(3,5))
savefig(p ,"images/explicit-elips-smooth.svg")

#+end_src

#+caption: Effect of the relaxed SMOOTH operator for different values of \(\alpha\) and \(\varepsilon\)
#+RESULTS: fig:relaxed-smooth-eval
[[file:images/explicit-elips-smooth.svg]]

Since explicit solving  eqref:eq:discrete_elyps at the beginning of a time-step did not yield a predictable solver we experiment with solving eqref:eq:discrete_elyps and eqref:eq:discrete-relaxed-smooth in an alternating manner. For ref:fig:alternating-elips-smooth we run 100 elliptical then 100 relaxed iterations and repeat both 10 times. For this experiment we observe the relaxed solver to be significantly better in developing the same interface as the baseline solver for a fixed value of \( \varepsilon \).

#+name: fig:alternating-elips-smooth
#+begin_src julia-vterm :results file graphics :file alternating-elips-smooth.svg :eval t
using DataFrames
using JLD2
using ProgressMeter
using LaTeXStrings
<<init>>
<<setup-diverse-testgrids>>
default(fontfamily="computer modern" , titlefontsize= 6 , guidefontsize=2 , tickfontsize = 2 , legendfontsize=2)
gr()
n = 4
m = 64
plots = []

for ε in [5e-2 , 2.5e-2, 1e-2]
g = testgrid(multi_solver, M , 2 , epsilon = ε)
set_xi_and_psi!(g[1])
for i = 1:10
   SMOOTH!(g[1], 100 , true )
end
po = heatmap(g[1].phase , title=L"original , $\varepsilon = %$ε$ " , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
push!(plots , po)


for α in [1e3 , 1e4 , 1e5 ,1e6]
    g = testgrid(relaxed_multi_solver, M , 2 , alpha = α , epsilon = ε)
    set_xi_and_psi!(g[1])
    for i = 1:10
    elyps_solver!(g[1] , 100)
        SMOOTH!(g[1], 100 , true )
    end
px = heatmap(g[1].phase , title=L"\alpha=%$α , \varepsilon=%$ε" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
    push!(plots , px)
    end
    end
p = plot(plots... , layout=grid(3,5))
savefig(p ,"images/alternating-elips-smooth.svg")

#+end_src

#+caption: Effect of the relaxed SMOOTH operator, and additional solving of the elliptical problem, for different values of \(\alpha\) and \(\varepsilon\)
#+RESULTS: fig:alternating-elips-smooth
[[file:images/alternating-elips-smooth.svg]]

The experimentation shows that  \( \alpha \) has an effect similar to \( \varepsilon \), where it changes the boundary thickness in the phase-field \( \phi \). Therefore \( \varepsilon \) and \( \alpha \) might be correlated. To test this hypothesis we use a simple Monte Carlo optimizer for \( \alpha,\varepsilon \).
** optimizer for alpha
The Monte Carlo optimizer runs both solvers for one time-step and compares their phase-fields \( \phi \) afterwards. It runs the relaxed solver with random values for \( \varepsilon , \alpha \) in a normal distribution around the current optimum. This results in a optimal \( \varepsilon \) found that is very close to the \( \varepsilon \) used in the baseline (9e-3 compared to 8e-3). This gives us confidence that the relaxed method solves the same problem, with the same \( \varepsilon \), as the baseline. Optimal values for \( \alpha \) varied , however stayed fairly large around \( 10^5 \to 10^{6} \).

#+begin_src julia :tangle src/optim.jl :noweb yes :noweb no-export
using Distributions
using DataFrames
using JLD2
<<init>>

function test_values(alpha_distribution::Distribution , epsilon_distribution::Distribution , M)
    alpha = rand(alpha_distribution)
    eps = max(rand(epsilon_distribution)  ,1e-10)
    relaxed_solver = testgrid(relaxed_multi_solver, M, 2; alpha=alpha, epsilon=eps)
    set_xi_and_psi!(relaxed_solver[1])
    #SMOOTH!(relaxed_solver[1], 100, false)
    for j=1:64
    elyps_solver!(relaxed_solver[1], 2000)
    v_cycle!(relaxed_solver , 1)
    end
    error = norm(relaxed_solver[1].phase .- original_solver[1].phase) / *(size(relaxed_solver[1].phase)...)
    return (;alpha=alpha , epsilon=eps , error=error)
end

original_solver = testgrid(multi_solver, M, 2)
set_xi_and_psi!(original_solver[1])
for j=1:64
v_cycle!(original_solver , 1)
end
#SMOOTH!(original_solver[1], 100, false);
eps = 3e-3
#M = testdata(64, div(64,3), 64/5 , 2)
alpha0 = 10000
epsilon0 = 1e-2
best_alpha = alpha0 / 10
best_epsilon = epsilon0 / 10
best_error  = Inf
results = DataFrame()
for n=1:1000
    searchradius = 1
    alpha_distribution = Normal(best_alpha , searchradius * alpha0)
    epsilon_distribution = Normal(best_epsilon , searchradius * epsilon0)
    result = test_values(alpha_distribution , epsilon_distribution , M)
    if result.error < best_error
        global best_error = result.error
        global best_alpha = result.alpha
        global best_epsilon = result.epsilon
        println(result)
    end
push!(results , result)
end
jldsave("experiments/alpha-epsilon.jld2"; result=results)
println("Best alpha: $best_alpha , Best epsilon: $best_epsilon")
#+end_src
** effect of \( \alpha  \) on the Gauss-Seidel iteration
To see the impact of \( \alpha \) with a fixed value of \( \varepsilon=8*10^{-3} \) on our solver, we evaluate both solvers after one time-step , and then calculate the difference between \( \phi_{ij}^{n+1} \) and \( \phi_{ij}^{n+1,\alpha} \), for various values of \( \alpha \).
Since the solution of the relaxed solver should approach the original solver, we expect
\begin{equation}
||\phi^{n+1} - \phi^{n+1,\alpha}||_{Fr} \to 0.
\end{equation}
In Fig.[[fig:alpha-error]] we observe the following behaviour where in all cases the difference between the relaxed solver and the original solver is apparent. Furthermore we observe a optimal value of \( \alpha \) at approximately \( 7.5 * 10^5 \). We explain this with our observations done for the Smoothing operator, where for small and large values of \( \alpha \) the relaxed solver results in restricted behaviour, which we also expect. On the other hand, for large values of \( \alpha \) the elliptical equation approaches \( \phi \), however it does not converge to \( \phi \) for small values of \( \alpha \).
#+name: fig:alpha-error
#+begin_src julia-vterm :results graphics file :file alpha-error.svg
using JLD2
using DataFrames
using Measures
<<init>>

pgfplotsx()
results = jldopen("experiments/alpha.jld2")["result"]
p=plot(results.alpha , results.error ./64^2, label=false , xlabel=L"\alpha" , ylabel="difference" )

savefig(p, "images/alpha-error.svg")
#+end_src

#+caption: Difference between the original solver \( \phi^1_{ij} \) and the relaxed solver \( \phi^{1,\alpha}_{ij} \) for different values of  \( \alpha \)
#+RESULTS[f474fd86cf30b6b4f9d6b13a527d99b42b609d04]: fig:alpha-error
[[file:images/alpha-error.svg]]

** Direct comparison of the baseline solver with the relaxed solver
Then we show a comparison of both solvers by plotting the phase-fields after 64 time-steps, and the difference \(\|\phi^{n+1} - \phi^{n+1,\alpha}\|_{Fr}\) over the time-steps \( n \in \{0 , \dots , 63 \}\) .
#+begin_src julia-vterm :results file graphics html :file relaxed-comp.gif
<<init>>
using JLD2
using DataFrames
using Measures
using LaTeXStrings

gr()

results = jldopen("experiments/iteration.jld2")["result"]
results1 = jldopen("experiments/relaxed-iteration.jld2")["result"]
results2 = jldopen("experiments/relaxed-iteration-nophi.jld2")["result"]
results3 = jldopen("experiments/relaxed-iteration-nosubiter.jld2")["result"]
titles =  ["original" , "subiter elliptical" , L"without $2\phi$" , L"without $2\phi$ and subiter"]

anim = @animate for iter in zip(results.solver,results1.solver ,results2.solver , results3.solver)
    plots = []
    for (phase , title) in zip(iter ,titles)
        push!(plots , heatmap(phase.phase , title=title , legend=:none , aspectratio=:equal , grid=false , showaxis=false))
        plot(plots...)
        end
    end
gif(anim , "images/relaxed-comp.gif", fps=10)
#+end_src
We can observe slight differences between the original solver and the relaxed solver. To quantify those, we run the relaxed solver for a fixed value of \( \alpha=7700 \) , as it is in the interval where \( \alpha \) is minimal in Fig.[[fig:alpha-error]]. We show the numerical difference between \( \phi_{ij}^n \) and \( \phi_{ij}^{n,\alpha} \) in Fig.[[fig:relaxed-original-comparison]]. The observed difference is mainly in areas with high curvature and inclusions of small segments of one phase in the other.
#+name: relaxed-comparison
#+begin_src julia-vterm :results file graphics html :file relaxed-comparison.gif
<<init>>
using JLD2
using DataFrames
using Measures
using LaTeXStrings

i = 0*64 +1
gr()

original_results = jldopen("experiments/alt-iteration.jld2")["result"]
relaxed_results = jldopen("experiments/alt-relaxed-iteration.jld2")["result"]

difference = [norm(original.phase./2 .- relaxed.phase./2) /64^2 for (original, relaxed) in zip(original_results.solver[i:i+63],relaxed_results.solver[i:i+63])
]
anim = @animate for (original, relaxed) in zip(original_results.solver[i:i+63],relaxed_results.solver[i:i+63])
        p1 = plot(1:size(difference,1) , difference , xlabel= "time-steps" , ylabel = "error"  , title="diffrence" , label=false)
        p2 = heatmap(original.phase , title="original" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
        p3 = heatmap(relaxed.phase , title="relaxed" , aspectratio=:equal , grid=false , showaxis=false , legend=:none)
        plot(p1,p2,p3 , layout=(1,3) , size=(2000 ,700) , bottom_margin=20Plots.mm , left_margin=20Plots.mm)
        end
gif(anim , "images/relaxed-comparison.gif", fps=10)
#+end_src

#+RESULTS[76efe71ab5265c6ad1da811a6f84242b09d84c91]: relaxed-comparison
#+begin_export html
[[file:images/relaxed-comparison.gif]]
#+end_export


#+name: fig:relaxed-original-comparison
#+begin_src julia-vterm :results file graphics :file relaxed-comparison.svg
<<init>>
using JLD2
using DataFrames
using Measures
using LaTeXStrings

i = 0*64 +1
pgfplotsx()
original_results = jldopen("experiments/alt-iteration.jld2")["result"]
relaxed_results = jldopen("experiments/alt-relaxed-iteration.jld2")["result"]

difference = [norm(original.phase .- relaxed.phase) /64^2 for (original, relaxed) in zip(original_results.solver[i:i+63],relaxed_results.solver[i:i+63])
]
original, relaxed =   original_results.solver[i+63],relaxed_results.solver[i+63]

p1 = plot(1:size(difference,
                 1) ,
          difference ,
          xlabel= "time-steps" ,
          ylabel = "error"  ,
          title="diffrence" ,
          label=false)

p2 = heatmap(original.phase ,
             title=L"original at $n=64$" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(relaxed.phase ,
             title=L"relaxed at $n=64$" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false ,
             legend=:none)
p=plot(p2,
       p3,
       p1 ,
       layout=layout3x1 ,
       size=size3x1 )
savefig(p , "images/relaxed-comparison.svg")
#+end_src

#+caption: Comparison between the original and the relaxed CH solvers.
#+RESULTS[cca6cd5858468e58cbdf037ea297f65a5a9bf61b]: fig:relaxed-original-comparison
[[file:images/relaxed-comparison.svg]]

** Relaxed energy evaluations
We do evaluate our relaxed method using the discrete energy defined in [[eqref:eq:discrete-energy]]. On the same initial data, and with the same values for \( \varepsilon , h , dt \) as in the Chapter.[[Energy evaluations]]. Since we expect the relaxed approach to solve the same initial value problem  eqref:eq:initial-value-problem, we expect both solvers to behave the same. In Figure.[[fig:relaxed-energy-balance]] we then observe the energy decay we expected. Our relaxed approach closely follows the baseline, although it consistently decays slightly faster. Both solvers decay in a similar manner and both solvers show a slight bend after a few iterations. However the bend in the relaxed solver is noticeably more pronounced. Additionally, in later iterations the relaxed solver shows faster energy decay than the original. We explain these differences with the observations taken in later experiments, where we observe mass-loss and slower convergence. We suspect the relaxed solver to therefore be more aggressive when minimizing energy.
#+name: fig:relaxed-energy-balance
#+begin_src julia-vterm :results file graphics :file relaxed-energy-balance.svg
<<init>>
using JLD2
using DataFrames
i0 = 1*64 +1
original_results = jldopen("experiments/alt-iteration.jld2")["result"]
relaxed_results = jldopen("experiments/alt-relaxed-iteration.jld2")["result"]
original_energy = bulk_energy.(original_results[i0:i0+63,:].solver)
relaxed_energy = bulk_energy.(relaxed_results[i0:i0+63,:].solver)
p1 = plot(1:64 ,
          original_energy ,
          title=L"Discrete Energy $E_d^{bulk}$",
          xlabel="timesteps" ,
          ylabel="energy"  ,
          label="original")
p1 = plot!(p1,
           1:64 ,
           relaxed_energy ,
           title=L"Discrete Energy $E_d^{bulk}$",
           xlabel="timesteps" ,
           ylabel="energy"  ,
           label="relaxed")
p2 = heatmap(relaxed_results.solver[i0].phase ,
             title="initial condition" ,
             legend=:none ,
             aspectratio=:equal ,
             showaxis=false ,
             grid=false)
p3 = heatmap(relaxed_results.solver[i0+63].phase ,
             title="after 64 time-steps" ,
             aspectratio=:equal ,
             legend=:none ,
             showaxis=false ,
             grid=false)

p = plot(p2,p3,p1 , layout=layout3x1 , size=(1600 ,1600))
savefig(p , "images/relaxed-energy-balance.svg")
#+end_src

#+caption: Energy decay of the relaxed solver compared to the original solver.
#+RESULTS[ddc6809b1b7f3918596757e2d1ab6e0f9ac4c92f]: fig:relaxed-energy-balance
[[file:images/relaxed-energy-balance.svg]]

** Stability of a relaxed two-grid sub-iteration
We use the same Cauchy criterion we used for the baseline solver.
Furthermore, we compare the sub-iteration behaviour of the relaxed solver to the original we therefore plot \( \|\phi_{ij}^{n+1,m} - \phi_{ij}^{n+2,m-1} \|_{Fr} \) against \( \| \phi_{ij}^{n+1,m,\alpha} - \phi_{ij}^{n+1,m-1,\alpha} \| \) for \( m \in \{2, \dots , 64\} \). The  sub-iterations in Fig.[[fig:relaxed-convergence]] are stable. However, the relaxed solver shows significantly slower convergence compared to the baseline solver. Which is why without the log-log scale employed, the behavior of both solvers would not be visible in the same plot. This behaviour suggests that the relaxed solver does not converge towards the solution of eqref:eq:relaxed-les. Further experiments on mass conservation confirm this suspicion. During further experiments with different initial conditions and number of Gauss-Seidel steps, we where not able to change the slow convergence behaviour.
#+name: fig:relaxed-convergence
#+begin_src julia-vterm :results file graphics :file relaxed-convergence.svg :eval t
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
n=1024

i0 = 1

df = jldopen("experiments/subiteration.jld2")["result"]
df = groupby(df , :experiment)[i0]
gd = groupby(df , :iteration)
original_res = combine(gd  , :cycle => ((x)-> [norm(x[i].phase - x[i-1].phase) for i in 2:size(x,1)]))

original_res =  groupby(original_res , :iteration)[1].cycle_function

df = jldopen("experiments/alt-relaxed-subiteration.jld2")["result"]
df = groupby(df , :experiment)[i0]
gd = groupby(df , :iteration)
relaxed_res = combine(gd  , :cycle => ((x)-> [norm(x[i].phase - x[i-1].phase) for i in 2:size(x,1)]))

relaxed_res =  groupby(relaxed_res , :iteration)[1].cycle_function
p=plot([original_res.+1e-14, relaxed_res.+1e-14],label= ["original"  "relaxed"] , ylabel="difference" , xlabel="sub-iteration" , yaxis=:log10 , xaxis = :log2 ) savefig(p , "images/relaxed-convergence.svg")
#+end_src

#+caption: Cauchy convergence of the basline and relaxed solver, during sub-iteration V-cycles.
#+RESULTS[eae421fa29f236ef3b08615ec0e7bca1501530e6]: fig:relaxed-convergence
        [[file:images/relaxed-convergence.svg]]
** Relaxed numerical mass balance
As already mentioned the relaxed solver is not mass conservative.
Our relaxed solver shows significant mass loss as seen in Fig.[[fig:relaxed-mass-balance]], especially when compared to the original solver in Fig.[[fig:mass-balance]]. Both the original approach and the relaxed one exhibit a discrete equivalent of mass conservation, therefore their difference has to be explained by the numerical solver. Which is consistent with the observations made with sub-iteration convergence. We therefore conclude that our relaxed solver does not properly converge. This, most likely, is due to our choice of alternating the solution of \( c \) and \( \phi \). because we intend to solve them both implicitly. Coupling the elliptical and relaxed CH equation might alleviate this, however the resulting system would be of similar complexity to eqref:eq:LES, which is what we intend to prevent with the relaxation approach. Alternatively, solving \( c \) explicitly leads to an unstable solver with the initial conditions used by us. We did not test different initial conditions due to a lack of computational resources and time.
#+name: fig:relaxed-mass-balance
#+begin_src julia-vterm :results file graphics :file relaxed-mass-balance.svg :eval t
<<init>>
using JLD2
using DataFrames
using Measures
gr()
i0 = 64 * 1+1
results = jldopen("experiments/alt-relaxed-iteration.jld2")["result"]
energy = [ massbal(s.phase) .- massbal(results.solver[i0].phase) for s in results[i0:i0+63,:].solver]
p1 = plot(1:64 , energy, xlabel= "time-steps" , ylabel = "error"  , label =false)
p2 = heatmap(results.solver[i0].phase , title="initial condition" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
p3 = heatmap(results.solver[i0+63].phase , title="after 64 time-steps" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
p = plot(p2,p3 , p1 , layout=layout3x1 , size=(1600 ,1600))
savefig(p , "images/relaxed-mass-balance.svg")
#+end_src

#+caption: Mass loss in the relaxed solver
#+RESULTS[2791ce53917102c842085e5a07bbd977aa2123dd]: fig:relaxed-mass-balance
[[file:images/relaxed-mass-balance.svg]]

** Relaxed stability in time
We test the behaviour under refinement in time by successive subdividing of the original time interval \( [0,T] \) into finer parts. Then, in ref:fig:relaxed-stability-in-time, we observe the stability of the relaxed solver in time, wich is similar  to the original solver. We compare  \( \| \phi^{n+1,\alpha}_{\Delta t = \frac{10^{-2}}{n+1}} - \phi^{n,\alpha}_{\Delta t = \frac{10^{-2}}{n}} \|_{Fr} \) at \( t=10^{-2} \), for wich the relaxed solver is consistently lower than the original solver. This might suggest a more consistent method over time. More likely however, this is due to the more aggressive energy decay, and non mass conservation.
#+name: fig:relaxed-stability-in-time
#+begin_src julia-vterm :results file graphics :file relaxed-time-stability.svg
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
df = jldopen("experiments/relaxed-time.jld2")["result"]
dfo = jldopen("experiments/time.jld2")["result"]
gdo = groupby(dfo,:iteration)
dfo = DataFrame([ last(x) for x in gdo])
change = [norm(df[!, "phase"][i] .- df[! , "phase"][i-1]) for i=2:size(df , 1)]
change0 = [norm(dfo[!, "phase"][i] .- dfo[! , "phase"][i-1]) for i=2:size(dfo , 1)]
p = plot(change , ylabel = "difference" , xlabel = "number of timesteps" , label="relaxed" )
p = plot(p , change0 , ylabel = L"\| \phi^{n+1,\alpha}_{\Delta t = \frac{10^{-2}}{n+1}} - \phi^{n,\alpha}_{\Delta t = \frac{10^{-2}}{n}} \|_{Fr} " , xlabel = "number of timesteps" , label="original")
savefig(p , "images/relaxed-time-stability.svg")
#+end_src

#+Caption: Behavior of the relaxed and baseline solvers while solving the time interval \( t \in \left[ 0 , 10^{-2} \right] \) with increasing number of time-steps.

** additional considerations
The approach for the relaxed solver was in our tests significantly faster than the baseline implementation. The run-time in practice is highly dependant on the amount of V-cycle iterations, for the baseline and relaxed solver respectively, as well as the number of Gauss-Seidel iterations for the elliptical problem.  When using similar number of V-cycles and sufficiently small number of elliptical iterations, the relaxed solver is several time faster. When we let both solvers iterate until convergence, the relaxed solver still is slightly faster. However, as we will show in our experiments, the relaxed solver has problems with convergence and never converges to the solution of the baseline. Because of this, and due to potential improvements in our implementation, the previous statements are to be taken as a qualitative guide, and not as a quantitative statement, which would require further research.
* Conclusion
In this thesis we have presented a simple introduction to the CH equation, following the authors in[cite:@SHIN20117441]  we have shown how to discretize it, and have provided a numerical solver for it.
Additionally, we introduced a elliptical relaxation approach for the CH equation that itself is mass conservative.
We have presented a solver for the CH equation as a baseline method, and have shown how to derive a Gauss-Seidel based two-grid method from the authors [cite:@SHIN20117441] initial approach.
For the relaxation approach, we introduced a finite difference discretization using methods that we used for the baseline. Building on this, we derive a Gauss-Seidel based two-grid method, that closely resembles  the method we used for our baseline.
In hindsight, we wouldn't have used a two-grid method for either solver, because the benefit of a multi-grid method, which is faster convergence, did not contribute to the goal of our Thesis. On the contrary, the added complexity made it harder to distinguish between errors in our numerical implementation, errors in the two-grid implementation, or systematic errors in the relaxation. Since the main goal of our thesis, was to compare the relaxation to our baseline, a Gauss-Seidel Method would have been sufficient for our experiments.
For our experiments, we have introduced measures to evaluate the stability of both solvers in regard to time and mass conservation as well as their sub-iteration behaviour.
We have observed the baseline to be mass conservative, in a numerical sense, and we have shown it to be stable in all tested measures.
On the relaxed solver, we have experimentally tested the effect of the relaxation parameter \( \alpha \). We came to the conclusion, that the solver does not work for to small or to large values of \( \alpha \). We investigated the correlation between \( \alpha \) and the interface parameter \( \varepsilon \) and found no direct correlation after switching to an implicit strategy for the elliptical parameter \( c \). The explicit test was inconclusive. We ran an optimizer for \( \alpha \) and \( \varepsilon \) that strongly suggested that \( \varepsilon \) is independent of the relaxation. We came to this conclusion since the \( \varepsilon \), that best approximated the baseline results was close to the \( \varepsilon \) in the baseline. However, verifying this hypothesis would require, either numerical or statistical analysis, which we leave open for further research.
We show that both discretizations satisfy a discrete version of conservation of mass. However, since the discretization is implicit, this does not equate to a mass conservative solver.
While the basline solver is mass conservative, the relaxed solver is not, which we have shown experimentally. On all other measures, the relaxed solver behaved similar to the baseline, and we explained the small discrepancies through the numerical error that causes mass loss.
We intentionally didn't evaluate run-time since numerical experiments have shown both solvers to be dependant on the amount of sub-iterations, hyperparameters such as \( \varepsilon \) as well as the number off smoothing iterations.
It would therefore be unfair to evaluate one solver on a set of parameters tweaked for the other.
As example for this dilemma we recall runs where the relaxed solver was around 10x faster than the baseline with the same parameters.
The baseline solver was able to run with 10x less smoothing iterations than the relaxed one.
A fair comparison would hence require to find the optimal number of smoothing for each solver.

** Outlook
This thesis leaves a lot of room for further research. We have already mentioned runtime evaluations, which require more optimizations, and additional experiments to test the number of smoothing iterations. Here it would be beneficial if both solvers are made adaptive, to ensure fair evaluations.
Furthermore, we initially considered a machine learning approach to replace the elliptical system. We didn't follow this idea mostly due to time constraints, as we had already collected trainings data during our numerical experiments. Our choice of programming language would have been of benefit here, as it would enable more advanced techniques, such as integrating the numerical solver in the trainings loop since julia offers automatic differentiation of arbitrary functions, and therefore enables back-propagation (gradient descent) through the entire solver. It would also have been interesting to try different discretizations of the relaxed CH equation, and a different method for solving it, such as a finite volume or finite element method. Those bring the challenge of beeing harder to compare to our baseline.
* Appendix :noexport:
** Additional Experiments
*** Stability in space
We expect our methods to be stable under different grid-sizes \( h \) and grid-points \( N \). Therefore we expect the difference after one time-step between eg. a \( 512 \times 512 \) grid and a \( 1024 \times 1024 \) grid to be smaller than the difference between a \( 64 \times 64 \) grid and a \( 128 \times 128 \) grid. In order to keep the problem the same , we fix \( Nh = 10^{-3} \cdot 1024 \) and test for \( N \in \left\{ 1024 , 512 , 256 , 128 , 64 , 32 \right\} \)
In Fig.[[fig:stability-in-space]] we observe the differences to fluctuate between \(10^{-3}\) and \(10^{-4}\). Indicating that the solver is somewhat stable.
#+name: fig:stability-in-space
#+begin_src julia-vterm :results file graphics :file space-stability.svg
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
gr()
df = jldopen("experiments/space_refinement.jld2")["result"]
gd = groupby(df , :iteration)
n = 4
change = [norm(gd[n].phase[i] - restrict(gd[n].phase[i-1] , G))/ *(size(gd[n].phase[i])...) for i= 2:size(gd[n].phase , 1) ]


p1 = plot([L"1024^2 \to 512^2" , L"512^2 \to 256^2" , L"256^2\to128^2" , L"128^2\to64^2" , L"64^2 \to32^2"],
         change ,
         ylabel = "difference" ,
         yscale=:log10,
         xlabel = "change in number of gridpoints" ,
         label=L"\Delta \phi" ,
         xscale=:log2 ,
         seriestype=:scatter ,
         xaxis=:flip ,
         legend=:topright)

p2 = heatmap(gd[4].phase[1],
             title=L"1024 \times 1024" ,
             legend=:none ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false)
p3 = heatmap(gd[4].phase[4],
             title=L"128 \times 128" ,
             aspectratio=:equal ,
             grid=false ,
             showaxis=false ,
             legend=:none)

p = plot(p2,
         p3 ,
         p1 ,
         layout= layout3x1 ,
         size=size3x1)
savefig(p , "images/space-stability.svg")
#+end_src

#+Caption: Behavior of the baseline solver while solving on successively finer grids
#+RESULTS[86f3e5e3d57f940d637b74a1a467c1862bbcb3aa]: fig:stability-in-space
[[file:images/space-stability.svg]]


*** Relaxed stability in space
For the relaxed solver we do the same evaluation for space, that we did for the baseline solver. We \(\Delta\phi\) going down exponentially with increasing grid sizes. This behaviour is as expected. However the jump from    \(128^2 \to 64^2\) to \(256^2 \to 128^2\)leads us to believe, that the solver is not yet stable for courser grids.

#+name: fig:relaxed-stability-in-space
#+begin_src julia-vterm :results file graphics :file relaxed-space-stability.svg :exports none
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
default(fontfamily="computer modern" , titlefontsize=22 , guidefontsize=18 , tickfontsize = 12 , legendfontsize=12)
gr()
odf = jldopen("experiments/space_refinement.jld2")["result"]
df = jldopen("experiments/relaxed_space_refinement.jld2")["result"]
gd = groupby(df , :iteration)
ogd = groupby(odf , :iteration)
n = 4
change = [norm(gd[n].phase[i] - restrict(gd[n].phase[i-1] , G))/ *(size(gd[n].phase[i])...) for i= 2:size(gd[n].phase , 1) ]
ochange = [norm(ogd[n].phase[i] - restrict(ogd[n].phase[i-1] , G))/ *(size(ogd[n].phase[i])...) for i= 2:size(ogd[n].phase , 1) ]
p = plot([L"1024^2 \to 512^2" , L"512^2 \to 256^2" , L"256^2\to128^2" , L"128^2\to64^2" , L"64^2 \to32^2"],
[ochange ,change] ,
ylabel = "difference" ,
yaxis = :log10,
xlabel = "change in number of gridpoints" ,
labels=[L"original $\Delta \phi$"  L"relaxed $\Delta\phi$"  ],
seriestype=:scatter ,
xaxis=:flip ,
legend=:topright,
right_margin = 10 * Plots.mm,
left_margin = 2 * Plots.mm,
bottom_margin = 2* Plots.mm)

savefig(p , "images/relaxed-space-stability.svg")
#+end_src

#+RESULTS[ac21efe8cb7262c2f4dc5fe974e6ca94c77da05b]: fig:relaxed-stability-in-space
[[file:images/relaxed-space-stability.svg]]

** rng generation
for random point generation we use the folowing Function and seed.

#+RESULTS:
: 2×12 Matrix{Int64}:
:  48  40  20   1  63  49   8  60  26  58  26  11
:  17  13  56  52  15   9  30  14  40   9  40  25


the random testdata is then generated as follows
#+name: testdata
#+begin_src julia :eval never :tangle src/utils.jl :exports none
using Random
function testdata(gridsize , blobs , radius ,norm;rng=MersenneTwister(42))
rngpoints = rand(rng,1:gridsize, 2, blobs)
M = zeros(gridsize,gridsize) .- 1
for p in axes(rngpoints , 2)
    point = rngpoints[:, p]
    for I in CartesianIndices(M)
        if (LinearAlgebra.norm(point .- I.I  , norm) < radius)
            M[I] = 1
        end
    end
end
M
end
#+end_src
** Experiments :noexport:
*** iteration
#+begin_src julia :results output  :noweb yes :eval never :tangle experiments/src/iteration.jl
using JLD2
using DataFrames
using Random
<<init>>
<<setup-diverse-testgrids>>
function iter(g::Vector{T} , n) where T<: solver
    out = []
    for j in 1:64
    set_xi_and_psi!(g[1])
    for i = 1:64
        v_cycle!(g, 1)
    end
    push!(out, (solver=deepcopy(g[1]), iteration=j , experiment=n))
    end
    return out
end


tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/iteration.jld2"; result)
#+end_src

#+RESULTS:

#+name: fig:behaviour
#+begin_src julia-vterm :results graphics file html :file behaviour.gif :chache :session jl :noweb no-export :output-dir images :exports none :noweb no-export
<<init>>
using JLD2
using DataFrames
results = jldopen("experiments/iteration.jld2")["result"]
n  = size(results.solver , 1)
pbar = ProgressBar(total = 10 * n)
energy = zeros(0)
massbalance = zeros(0)

anim = @animate for res in eachrow(results)
    push!(energy , bulk_energy(res.solver))
    push!(massbalance , massbal(res.solver.phase))

    p0 = heatmap(res.solver.phase , clim =(-1,1) , framestyle=:none , legend=true, lims=(1, size(res.solver.phase , 1)) , aspect_ratio=:equal, title  = "phasefield" )
   p1 = heatmap(res.solver.potential , framestyle=:none , legend=true, lims=(1,size(res.solver.phase , 1)), aspect_ratio=:equal, title  = "potential" )

    current_range = (res.experiment -1)*64 +1

    p3 = plot( 1:res.iteration, (massbalance .-massbalance[current_range])[current_range:current_range+res.iteration-1] , xlim=(1,64),  title = "Mass change")
    p2 = plot(1:res.iteration , energy[current_range:current_range+res.iteration-1], xlim=(1,64),  title = "Bulk energy")
    plot(p0,p1,p2,p3)
end
gif(anim , "images/behaviour.gif" , fps = 10)
#+end_src

#+caption: Behaviour of bulk energy \( E_{bulk} \) and amount of fluid changing phase, for different initial conditions

*** subiteration
#+begin_src julia :results output :noweb yes :tangle experiments/src/subiteration.jl
using DataFrames
using JLD2
using ProgressMeter
<<init>>
<<setup-diverse-testgrids>>
n = 4
m = 64

function iter(g::Vector{T} , n , k , prg::Progress) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        v_cycle!(g, 1)
        push!(out, (cycle=deepcopy(g[1]), iteration=j , subiteration=i , experiment=k))
        next!(prg)
    end
    end
    return out
end


tasks = []
prg=Progress(size(tests ,1)*n*m , showspeed=true , )
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i] , n , i , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/subiteration.jld2"; result)
#+end_src
*** time
#+begin_src julia :results output :noweb yes :exports none :tangle experiments/src/time.jl
using DataFrames
using JLD2
<<init>>
SIZE  =64
M = testdata(SIZE, SIZE ÷ 5, SIZE /5 , 2)
tests = [testgrid(multi_solver , M , 2 , dt = t ) for t in 1e-2./(1:64)]

function iter(g::Vector{T} , n) where T<: solver
    out = []
    for i = 1:n
    set_xi_and_psi!(g[1])
    for i = 1:64
        v_cycle!(g, 1)
    end
    push!(out, (phase=copy(g[1].phase), iteration=n))
    end
    return out
end


tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/time.jld2"; result)
#+end_src
*** space
#+begin_src julia :results output :noweb yes :exports none :tangle experiments/src/space.jl
using DataFrames
using JLD2
using ProgressMeter
<<init>>

M = testdata(2^10 , 2^5 , 2^7 , 2 )
grids = testgrid(multi_solver  , M , 7 , h0 = 3e-3*64 / 1024)
# inits
for i=2:size(grids,1)
    restrict_solver!(grids[i-1] , grids[i])
end
tests = [[grids[i-1] , grids[i]] for i=2:size(grids,1)]
n = 4
m = 64

function iter(g::Vector{T} , n , prg::Progress) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        v_cycle!(g, 1)
        next!(prg)
    end
    push!(out, (phase=copy(g[1].phase), iteration=j))
    end
    return out
end


tasks = []
prg=Progress(size(tests ,1)*n*m , showspeed=true , )
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], n , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/space_refinement.jld2"; result)
#+end_src
** Relaxed experiments :noexport:
*** Iteration
#+begin_src julia    :noweb no-export :tangle experiments/src/relaxed-iteration.jl :async
using JLD2
using DataFrames
using ProgressMeter
using Random
<<init>>
<<setup-diverse-testgrids>>

#tests = [testgrid(relaxed_multi_solver, M , 2;alpha=82000 , epsilon=0.009) for M in initial_data]
tests = [testgrid(relaxed_multi_solver, M , 2) for M in initial_data]

n = 64
m = 64


function iter(g::Vector{relaxed_multi_solver} , n , prg::Progress)
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        elyps_solver!(g[1] , 1000)
        v_cycle!(g, 1)
        next!(prg)
    end
    push!(out, (solver=deepcopy(g[1]), iteration=j , experiment=n))
    end
    return out
end

prg=Progress(size(tests ,1)*n*m , showspeed=true , )
tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/relaxed-iteration.jld2"; result)
#+end_src

*** Subiteration
#+begin_src julia :tangle experiments/src/relaxed-subiteration.jl :noweb yes
using DataFrames
using JLD2
using ProgressMeter
<<init>>
<<setup-diverse-testgrids>>
#tests = [testgrid(relaxed_multi_solver, M , 2;alpha=32428.2 , epsilon=0.163398) for M in initial_data]
tests = [testgrid(relaxed_multi_solver, M , 2) for M in initial_data]
n = 4
m = 1024

function iter(g::Vector{T} , n ,k , prg::Progress) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        elyps_solver!(g[1] , 1000)
        v_cycle!(g, 1)
        push!(out, (cycle=deepcopy(g[1]), iteration=j , subiteration=i , experiment=k))
        next!(prg)
    end
    end
    return out
end


tasks = []
prg=Progress(size(tests ,1)*n*m , showspeed=true , )
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i] , n , i , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/relaxed-subiteration.jld2"; result)
#+end_src

*** Time
#+begin_src julia :results output :noweb yes :exports none :tangle experiments/src/relaxed-tiem.jl
using DataFrames
using JLD2
<<init>>
tests = [testgrid(relaxed_multi_solver , M , 2 , dt = t ) for t in 1e-2./(1:64)]

function iter(g::Vector{T} , n) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:64
        elyps_solver!(g[1] , 1000)
        v_cycle!(g, 1)
    end
    end
    push!(out, (phase=copy(g[1].phase), iteration=n))
    return out
end


tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/relaxed-time.jld2"; result)
#+end_src
*** Space
#+begin_src julia :results output :noweb yes :exports none :tangle experiments/src/relaxed-space.jl
using DataFrames
using JLD2
using ProgressMeter
<<init>>

M = testdata(2^10 , 2^5 , 2^7 , 2 )
grids = testgrid(relaxed_multi_solver  , M , 7 , h0=3e-3 * 64 /1024)
# inits
for i=2:size(grids,1)
    restrict_solver!(grids[i-1] , grids[i])
end
tests = [[grids[i-1] , grids[i]] for i=2:size(grids,1)]

n = 4
m = 1024

function iter(g::Vector{T} , n , prg::Progress) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    elyps_solver!(g[1] , 1000)
    for i = 1:m
        v_cycle!(g, 1)
        next!(prg)
    end
    push!(out, (phase=copy(g[1].phase), iteration=j))
    end
    return out
end


prg=Progress(size(tests ,1)*n*m , showspeed=true , )
tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], n , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/relaxed_space_refinement.jld2"; result)
#+end_src
*** alpha

#+begin_src julia :noweb no-export :eval never :tangle experiments/src/alpha.jl :exports results
<<init>>
using JLD2
using Distributed
using ProgressBars
using DataFrames

original_grid = testgrid(multi_solver, M, 2)
alphas = 0:1e4:2e6

function alpha_error(alpha::Number , solution::Array )
    test_solver  = testgrid(relaxed_multi_solver, M, 2, alpha=alpha)
    set_xi_and_psi!(test_solver[1])
    for j in 1:64
        elyps_solver!(test_solver[1], 1000)
        v_cycle!(test_solver , 1)
    end
return [(;alpha=alpha , error=norm(test_solver[1].phase - solution))]
end
set_xi_and_psi!(original_grid[1])
for j in 1:64
    v_cycle!(original_grid, 1)
end
print("finished original v_cycle")
tasks = []
for alpha in alphas
    t = Threads.@spawn alpha_error(alpha , original_grid[1].phase)
    push!(tasks , (alpha=alpha , task = t))
end
result = DataFrame()
for task in ProgressBar(tasks)
    append!(result , fetch(task.task) )
    end
jldsave("experiments/alpha.jld2"; result)
#+end_src

** alternative experiments
*** iteration
#+begin_src julia :results output  :noweb yes :eval never :tangle experiments/src/alt-iteration.jl
using JLD2
using DataFrames
using Random
using ProgressMeter
<<init>>
<<setup-diverse-testgrids>>
n = 64
m = 16
function iter(g::Vector{T} , experiment , prg::Progress) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        v_cycle!(g, 1)
        next!(prg)
    end
    push!(out, (solver=deepcopy(g[1]), iteration=j , experiment=experiment))
    end
    return out
end


prg=Progress(size(tests ,1)*n*m , showspeed=true , )
tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/alt-iteration.jld2"; result)
#+end_src
#+begin_src julia    :noweb no-export :tangle experiments/src/test-relaxed-iteration.jl :eval never
using JLD2
using DataFrames
using ProgressMeter
using Random
<<init>>
<<setup-diverse-testgrids>>

#tests = [testgrid(relaxed_multi_solver, M , 2;alpha=82000 , epsilon=0.009) for M in initial_data]
tests = [testgrid(relaxed_multi_solver, M , 2 , h0=1.5e-3) for M in initial_data]

n = 64
m = 1024


function iter(g::Vector{relaxed_multi_solver} , experiment , prg::Progress)
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        elyps_solver!(g[1] , 100)
        SMOOTH!(g[1] , 1 ,true)
        next!(prg)
    end
    push!(out, (solver=deepcopy(g[1]), iteration=j , experiment=experiment))
    end
    return out
end

prg=Progress(size(tests ,1)*n*m , showspeed=true , )
tasks = []
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i], i , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/test-nomulti-iteration.jld2"; result)
#+end_src

*** subiteration

#+begin_src julia :tangle experiments/src/alt-relaxed-subiteration.jl :noweb yes
using DataFrames
using JLD2
using ProgressMeter
<<init>>
<<setup-diverse-testgrids>>
#tests = [testgrid(relaxed_multi_solver, M , 2;alpha=32428.2 , epsilon=0.163398) for M in initial_data]
tests = [testgrid(relaxed_multi_solver, M , 2) for M in initial_data]
n = 4
m = 1024

function iter(g::Vector{T} , n ,experiment , prg::Progress) where T<: solver
    out = []
    for j in 1:n
    set_xi_and_psi!(g[1])
    for i = 1:m
        elyps_solver!(g[1] , 1000)
        v_cycle!(g, 1)
        push!(out, (cycle=deepcopy(g[1]), iteration=j , subiteration=i , experiment=experiment))
        next!(prg)
    end
    end
    return out
end


tasks = []
prg=Progress(size(tests ,1)*n*m , showspeed=true , )
for i in eachindex(tests)
    t = Threads.@spawn iter(tests[i] , n , i , prg)
    push!(tasks , (iteration = 1 , task = t))
    end
result = DataFrame()
for task in tasks
    append!(result , fetch(task.task) )
    end
jldsave("experiments/alt-relaxed-subiteration.jld2"; result)
#+end_src
** alternative results
*** iteration
#+begin_src julia-vterm :results graphics file html :file alt-behaviour.gif :session jl :noweb no-export :output-dir images :exports none :noweb no-export :cache no
<<init>>
using JLD2
using DataFrames
gr()
results = jldopen("experiments/alt-iteration.jld2")["result"]
n  = size(results.solver , 1)
pbar = ProgressBar(total = 10 * n)
energy = zeros(0)
massbalance = zeros(0)

anim = @animate for res in eachrow(results)
    heatmap(res.solver.phase , clim =(-1,1) , framestyle=:none , legend=true, lims=(1, size(res.solver.phase , 1)) , aspect_ratio=:equal, title  = "phasefield" )

end

gif(anim , "images/alt-behaviour.gif" , fps = 10)
#+end_src

#+RESULTS[5be40c097a9ead7bf13cef459456889ddedb2fd2]:
#+begin_export html
[[file:images/alt-behaviour.gif]]
#+end_export


#+begin_src julia-vterm :results graphics file html :file test-relaxed-behaviour.gif :session jl :noweb no-export :output-dir images :exports none :noweb no-export :cache no
<<init>>
using JLD2
using DataFrames
gr()
results = jldopen("experiments/test-relaxed-iteration.jld2")["result"]
n  = size(results.solver , 1)
pbar = ProgressBar(total = 10 * n)
energy = zeros(0)
massbalance = zeros(0)

anim = @animate for res in eachrow(results)
    heatmap(res.solver.phase , clim =(-1,1) , framestyle=:none , legend=true, lims=(1, size(res.solver.phase , 1)) , aspect_ratio=:equal, title  = "phasefield" )

end

gif(anim , "images/test-relaxed-behaviour.gif" , fps = 10)
#+end_src

#+RESULTS:
#+begin_export html
[[file:images/test-relaxed-behaviour.gif]]
#+end_export

*** subiteration
#+begin_src julia-vterm :results file graphics :file alt-relaxed-convergence.svg
<<init>>
using DataFrames
using JLD2
using LaTeXStrings
gr()
n=1024

i0 = 1
df = jldopen("experiments/subiteration.jld2")["result"]
df = groupby(df , :experiment)[i0]
gd = groupby(df , :iteration)
original_res = combine(gd  , :cycle => ((x)-> [norm(x[i].phase - x[i-1].phase) for i in 2:size(x,1)]))

original_res =  groupby(original_res , :iteration)[1].cycle_function


df = jldopen("experiments/alt-relaxed-subiteration.jld2")["result"]
df = groupby(df , :experiment)[i0]
gd = groupby(df , :iteration)
relaxed_res = combine(gd  , :cycle => ((x)-> [norm(x[i].phase - x[i-1].phase) for i in 2:size(x,1)]))

relaxed_res =  groupby(relaxed_res , :iteration)[1].cycle_function
p=plot([original_res, relaxed_res],label= ["original"  "alternative relaxed"] , ylabel="difference" , xlabel="sub-iteration" )
savefig(p , "images/alt-relaxed-convergence.svg")
#+end_src

#+RESULTS[f6f4ee058d52005f0de0c501ab03bc21e22f21df]:
[[file:images/alt-relaxed-convergence.svg]]

*** mass
#+begin_src julia-vterm :results file graphics :file test-nomulti-mass-balance.svg :eval t :cache nil
<<init>>
using JLD2
using DataFrames
using Measures
gr()
i0 = 64 * 0+1
results = jldopen("experiments/test-nomulti-iteration.jld2")["result"]
energy = [ massbal(s.phase) .- massbal(results.solver[i0].phase) for s in results[i0:i0+63,:].solver]
p1 = plot(1:64 , energy, xlabel= "time-steps" , ylabel = "error"  , label =false)
p2 = heatmap(results.solver[i0].phase , title="initial condition" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
p3 = heatmap(results.solver[i0+63].phase , title="after 64 time-steps" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
p = plot(p2,p3 , p1 , layout=layout3x1 , size=(1600 ,1600))

savefig(p , "images/test-nomulti-mass-balance.svg")
#+end_src

#+ATTR_ORG: :width 900
#+RESULTS[b2c7d914038a15bb0a7f1b596edfb1a1377c4fbd]:
[[file:images/test-nomulti-mass-balance.svg]]

#+begin_src julia-vterm :results file graphics :file alt-mass-balance.svg
<<init>>
using JLD2
using DataFrames
using Measures

i0 = 64 * 0+1
results = jldopen("experiments/alt-iteration.jld2")["result"]
energy = [ massbal(s.phase) .- massbal(results.solver[i0].phase) for s in results[i0:i0+63,:].solver]
p1 = plot(1:64 , energy, xlabel= "time-steps" , ylabel = "error"  , label =false)
p2 = heatmap(results.solver[i0].phase , title="initial condition" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
p3 = heatmap(results.solver[i0+63].phase , title="after 64 time-steps" , legend=:none , aspectratio=:equal , grid=false , showaxis=false)
p = plot(p2,p3 , p1 , layout=layout3x1 , size=(1600 ,1600))
savefig(p , "images/alt-mass-balance.svg")
#+end_src

#+RESULTS[ae674c0585cf06a81f1a5f3ff0f6f1881ee869ee]:
[[file:images/alt-mass-balance.svg]]

*** vid
#+name: fig:relaxed-anim
#+begin_src julia-vterm :results file graphics html :file relaxed-anim.gif
<<init>>
using JLD2
using DataFrames
using Measures

gr()

results = jldopen("experiments/relaxed-iteration4.jld2")["result"]
anim = @animate for s in results.solver
    heatmap(s.phase)
    end
gif(anim , "images/relaxed-anim.gif", fps=10)
#+end_src

#+RESULTS: fig:relaxed-anim
[[file:images/relaxed-anim.gif]]
** bulk energy and mass balance
#+begin_src julia :tangle src/utils.jl :eval never
function bulk_energy(solver::T) where T <: Union{multi_solver , relaxed_multi_solver}
    energy = 0
    dx = CartesianIndex(1,0)
    dy = CartesianIndex(0,1)
    W(x) = 1/4 * (1-x^2)^2
    for I in CartesianIndices(solver.phase)[2:end-1,2:end-1]
        i,j = I.I
        energy += solver.epsilon^2 / 2 * G(i+ 0.5,j ,solver.len, solver.width) * (solver.phase[I+dx] - solver.phase[I])^2 + G(i,j+0.5,solver.len ,solver.width) * (solver.phase[I+dy] - solver.phase[I])^2 + W(solver.phase[I])
        end
   return energy
end
#+end_src

#+begin_src julia :tangle src/utils.jl
function massbal(arr)
    num_cells= *((size(arr).-2)...)
    return sum(arr[2:end-1, 2:end-1])/num_cells
    end
#+end_src

* Utility functions :noexport:
#+name: imports
#+begin_src julia :session jl :results silent :exports none
using Plots
using LinearAlgebra
#+end_src


#+begin_src julia :tangle src/utils.jl :eval never
###############################################################################
#                  Common Utility Functions For Multi Solvers                 #
###############################################################################
"""
restricts an array on the small grid to an array in the large grid asserts size arr=2^n + 2 and returns ret=2^(n-1) + 2

Returns
---------------------------
large grid array + padding
"""
function restrict(arr, G)
    shape = (size(arr) .- 2) .÷ 2
    ret = zeros(shape .+ 2)
    for I in CartesianIndices(ret)[2:end-1, 2:end-1]
        i, j = I.I
        g = [
            G(2 * i - 1, 2 * j - 1, (size(arr) .- 2)...),
            G(2 * i - 1, 2 * j, (size(arr) .- 2)...),
            G(2 * i, 2 * j - 1, (size(arr) .- 2)...),
            G(2 * i, 2 * j, (size(arr) .- 2)...)
        ]
        if sum(g) == 0
            ret[I] = 0
        else
            ret[I] = (
                1 / sum(g)
                ,*
                dot(g,
                    [
                        arr[2*i-1, 2*j-1],
                        arr[2*i-1, 2*j],
                        arr[2*i, 2*j-1],
                        arr[2*i, 2*j]
                    ]
                )
            )
        end
    end
    return ret
end

"""
    prolong(arr , G)

interpolates int a smaller grid by a factor of 2

"""
function prolong(arr, G)
    inner_shape = (size(arr) .- 2) .* 2
    ret = zeros(inner_shape .+ 2)
    ONE = oneunit(CartesianIndices(arr)[1])
    for I in CartesianIndices(arr)[2:end-1, 2:end-1]
        Ind = 2 * (I - ONE) + ONE
        for J in (Ind-ONE):Ind
            ret[J] = G(J.I..., inner_shape...) * arr[I]
        end
    end
    return ret
end
"""
    restrict!(smallgrid_solver::multi_solver , largegrid_solver::multi_solver)::multi_solver

------------
Requires
----------
smallgrid solver and largegid solvers to be multiple of 2 from each other bar padding eg. (66x66)->(34x34)

------------
Returns
------------
    nothing. mutatest largegid in place to represent the smallgrid

"""
function restrict_solver!(smallgrid_solver::T, largegrid_solver::T) where {T<:solver}
    copy!(largegrid_solver.phase, restrict(smallgrid_solver.phase, G))
    copy!(largegrid_solver.potential, restrict(smallgrid_solver.potential, G))
    return nothing
end
#+end_src
#+begin_src julia :tangle src/solvers.jl :eval never
abstract type solver end
struct multi_solver <: solver
    phase::Matrix{Float64}
    potential::Matrix{Float64}
    xi::Matrix{Float64}
    psi::Matrix{Float64}
    epsilon::Float64
    h::Float64
    dt::Float64
    W_prime::Function
    len::Int
    width::Int

end
struct relaxed_multi_solver <: solver
    phase::Matrix{Float64}
    potential::Matrix{Float64}
    xi::Matrix{Float64}
    psi::Matrix{Float64}
    c::Matrix{Float64}
    epsilon::Float64
    h::Float64
    dt::Float64
    W_prime::Function
    len::Int
    width::Int
    alpha::Float64

end
#+end_src
#+begin_src julia :tangle src/testgrids.jl :eval never
function W_prime(x)
    return -x * (1 - x^2)
end
function testgrid(::Type{multi_solver},M, len; dt = 1e-3 ,  epsilon=8e-3 , h0=3e-3)
    grid = Array{multi_solver}(undef, len)
    phase = zeros(size(M) .+ 2)
    phase[2:end-1, 2:end-1] = M


    for i = 1:len
        dims = size(M) .÷ 2^(i-1) .+ 2
        grid[i] = multi_solver(zeros(dims),
            zeros(dims),
            zeros(dims),
            zeros(dims),
            epsilon, h0 * 2^i, dt,
            W_prime,
            (dims .- 2)...)

    end
    copyto!(grid[1].phase, phase)
    return grid

end

function testgrid(::Type{relaxed_multi_solver},M, len ; alpha=1.5e6 , dt=1e-3, epsilon=8e-3 , h0=3e-3)
    grid = Array{relaxed_multi_solver}(undef, len)
    phase = zeros(size(M) .+ 2)
    phase[2:end-1, 2:end-1] = M

    for i = 1:len
        dims = size(M) .÷ 2^(i-1) .+ 2
        grid[i] = relaxed_multi_solver(zeros(dims),
            zeros(dims),
            zeros(dims),
            zeros(dims),
            zeros(dims),
            epsilon, h0 * 2^i, dt,
            W_prime,
            (dims .- 2)... ,
            alpha)

    end
    copyto!(grid[1].phase, phase)
    return grid
end


#+end_src

#+name: init
#+begin_src julia :eval never
include(pwd() * "/src/solvers.jl")
include(pwd() * "/src/adapted_solvers.jl")
include(pwd() * "/src/utils.jl")
include(pwd() * "/src/multisolver.jl")
include(pwd() * "/src/multi_relaxed.jl")
include(pwd() * "/src/testgrids.jl")
include(pwd() * "/src/elypssolver.jl")
using Plots
using LaTeXStrings
using LinearAlgebra
using Printf
using ProgressBars
default(fontfamily="computer modern" , titlefontsize=32 , guidefontsize=22 , tickfontsize = 22 , legendfontsize=22)
pgfplotsx()
layout2x2 = grid(2,2)
layout3x1 = @layout [ b  c ; a]
size3x1 = (1600,1600)
SIZE = 64
M = testdata(SIZE, SIZE ÷ 5, SIZE /5 , 2)

#+end_src

#+name: setup-grid
#+begin_src julia :eval never :noweb yes
<<init>>
testgrd = testgrid(multi_solver,M, 2)
test_solver = testgrd[1]
#+end_src


#+name: setup-relaxed-grid
#+begin_src julia :eval never :noweb yes
<<init>>
testgrd = testgrid(relaxed_multi_solver,M, 2)
println("Hi")
solver = testgrd[1]
#+end_src

#+name: setup-comparison
#+begin_src julia :noweb yes
<<init>>
using Plots
using LinearAlgebra
using ProgressBars
using JLD2
M = jldopen("data/test-phasefield.jld2")["M"]

relaxed_grid1 = testgrid(relaxed_multi_solver, M, 2 ,alpha=1e3)
relaxed_grid2 = testgrid(relaxed_multi_solver, M, 2 , alpha=1e4)
relaxed_grid3 = testgrid(relaxed_multi_solver, M, 2 , alpha=1e5)
original_grid = testgrid(multi_solver, M, 2)

#+end_src

#+name: setup-diverse-testgrids
#+begin_src julia :noweb yes
incirc(M) = filter(x -> norm(x.I .- (size(M, 1) / 2, size(M, 2) / 2)) < min(size(M)...) / 3, CartesianIndices(M))
insquare(M) = filter(x -> norm(x.I .- (size(M, 1) / 2, size(M, 2) / 2), Inf) < min(size(M)...) / 4, CartesianIndices(M))
side(M) = filter(x -> x.I[2] < size(M, 2) ÷ 2, CartesianIndices(M))
halfcirc(M) = filter(x -> norm(x.I .- (1, size(M, 2) / 2), 2) < min(size(M)...) / 3, CartesianIndices(M))

function get_special_input(fn, size)
    M = fill(-1, size , size )
    M[fn(M)] .= 1
    return M
end
SIZE  =64
t1= [testdata(SIZE, SIZE ÷ 5, SIZE /5 , j) for j in [1,2, Inf]]
t2 = [get_special_input(fn,SIZE) for  fn in [halfcirc , incirc, side , insquare]]
initial_data = [t1 ; t2]
tests = [testgrid(multi_solver, M , 2) for M in initial_data]

#+end_src


* References :ignore:
#+PRINT_BIBLIOGRAPHY:
#  LocalWords:  Discretization

* Footnotes
[fn:1] This solver uses a two dimensional version with 2 second order terms instead of the full fourth order  equation.

[fn:2] Julia provides iteration utilities over n dimensional matricies. Therefore it would technically be possible to write dimension agnostic algorithms. While we used some of this functionality, we did not implement a full n-dimensional algorithm, and only provide a 2D implementation
# Local Variables:
# mode: org
# org-export-allow-bind-keywords: t
# End:
